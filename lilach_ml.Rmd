---
title: "Final Project"
author: "Lilach Herzog & Leslie Cohen"
date: "6/10/2022"
output:
  pdf_document:
    fig_caption: yes
  word_document:
    toc: yes
    toc_depth: 2
editor_options:
  chunk_output_Type: inline
fig_width: 5 
fig_height: 3
fig_caption: true
bookdown::pdf_book:
  includes:
    in_header: preamble.tex
---
```{r, message=FALSE, warning=FALSE, echo=FALSE, results='hide'}
knitr::opts_chunk$set(fig.width=5, fig.height=3,collapse = TRUE) 
```
## Upload data and libraries

```{r, message=FALSE, warning=FALSE, echo=FALSE, results='hide'}
library(tidymodels) # for the sampling and splitting the data
library(class)

library("Hmisc")
library(corrplot) # for eda
library(hopkins)

library(tidyverse)    # data manipulation and visualization
library(ggplot2)
library(ggfortify) #autoplot
library(gridExtra) # grid.arrange
library(grid) # textGrob
library(gtable) # gtable_add_rows+gtable_add_grob
library(kableExtra)

library(gmodels) # CrossTable

library(caret) # Classification And REgression Training: contains functions to streamline the model training process for complex regression and classification problems.
library(pROC) # roc

library(kernlab)      # SVM: ksvm
library(e1071)        # SVM methodology

library(C50) # DT
library(modeldata)  # for the cells data
library(vip)        # for variable importance plots
library(ranger)

```


```{r message=FALSE, warning=FALSE}

glaucoma <- read.csv("GlaucomaM.csv")# read data
```

We have data from 196 patients, with 63 features.
Since the whole table is already shuffled according to the 'Class' column, we don't need to shuffle the whole table again
In order to get consistent results we initialized a pseudo random number generator. We also made sure to shuffle our data.
```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
set.seed(123)
glaucoma <- glaucoma[sample(nrow(glaucoma)),]
```


# preproccessing

 We want to classify our data according to the 'Class' column, which contains the information about whether all the information about a person (represented in a row) belongs to a healthy (normal) or sick (glaucoma) patient, divided neatly in half. Before starting on teaching the algorithm, we converted the 'Class' column to a factor.
```{r, message=FALSE, warning=FALSE, results='hide'}
glaucoma$Class<-as.factor(glaucoma$Class)
glaucoma_num_class<-as.numeric(glaucoma$Class)
```

## EDA analysis

Results obtained with corrplot: Positive correlations are displayed in a
blue scale while negative correlations are displayed in a red scale. we
can observe 8 clusters of features that highly correlate. Results obtained with heatmap: we
can observe one big cluster that could be divided in 2 and 2 others
clusters of features.

We would like to observe if we can  cluster patients healthy or with glaucoma with all the features.
We performed first a PCA on all features and values.

Preform PCA on our data
```{r, message=FALSE, warning=FALSE, results='hide'}
default_pca <- prcomp(glaucoma[,1:62], center = TRUE,  scale. = TRUE)
summary(default_pca)
default_pca_plot <- autoplot(default_pca, data = glaucoma, colour = 'Class')
fit_pca <- lm(Class ~ .,
            cbind(Class = glaucoma[, "Class"], default_pca$x[, 1:2]) %>% as.data.frame())
```
```{r, message=FALSE, warning=FALSE, echo=FALSE, fig.height=2, fig.cap= 'default PCA plot', results='hide', fig.show='hide'}
grid.arrange(default_pca_plot +
               labs(title = paste0("hopkins=",round(hopkins(glaucoma[,1:62], m=nrow(default_pca$x)-1),3)),
                    subtitle =paste0("r-sqr= ", round(summary(fit_pca)$adj.r.squared, 3))))
```

In the PCA, we can observe 2 clusters of patients: glaucoma patients and normal
patients. Unfortunately, some of them were not clustered correctly (mixed clouds of blue and red points) in the same area. 

## correlation
This mixture could be, probably to the fact that many features correlate, so our next course of action was to check for any correlation between features to determine which feature are correlated by using pearson correlation


```{r, message=FALSE, warning=FALSE, results='hide', fig.show='hide', echo=FALSE}
gl.cor = cor(glaucoma[,c(1:62)], method = c("pearson"))
corrplot(gl.cor)
palette = colorRampPalette(c("green", "white", "red")) (20)
heatmap(x = gl.cor, col = palette, symm = TRUE)
```

```{r, message=FALSE, warning=FALSE, results='hide', fig.show='hide',echo=FALSE}
gl.rcorr = rcorr(as.matrix(glaucoma[,c(1:62)]))
gl.rcorr
gl.coeff = gl.rcorr$r
cor_matrix_rm <- gl.cor                  # Modify correlation matrix
cor_matrix_rm[upper.tri(cor_matrix_rm)] <- 0
diag(cor_matrix_rm) <- 0
data<-glaucoma[,c(1:62)]
```
```{r, message=FALSE, warning=FALSE, results='hide', fig.show='hide'}
# Remove highly correlated variables
data_no_corr <- data[ , !apply(cor_matrix_rm,  2,function(x) any(x > 0.8))]
```
```{r, message=FALSE, warning=FALSE, results='hide', fig.show='hide',echo=FALSE}
head(data_no_corr)
colSums(glaucoma[,c(1:62)] != 0)
```
```{r, message=FALSE, warning=FALSE, results='hide', fig.show='hide'}
gl.cor2 = cor(data_no_corr, method = c("pearson"))
```

```{r, message=FALSE, warning=FALSE, results='hide', fig.show='hide',echo=FALSE}
corrplot(gl.cor2)
heatmap(x = gl.cor2, col = palette, symm = TRUE)
```
Preform PCA on our new data
```{r, message=FALSE, warning=FALSE, results='hide', fig.show='hide',echo=FALSE}
corr_pca <- prcomp(data_no_corr, center = TRUE, scale. = TRUE)#play with options
summary(corr_pca)
data_no_corr$Class<-glaucoma$Class
corr_pca_plot <- autoplot(corr_pca, data = data_no_corr, colour = 'Class')
```
```{r, message=FALSE, warning=FALSE, results='hide', fig.show='hide'}
fit_corr_pca  <- lm(Class ~ ., cbind(Class = glaucoma$Class, corr_pca$x[, 1:2])
                    %>% as.data.frame())
```

## importance
We then used the varImp to estimate the variable importance, which is printed and plotted, using the logistic regression method to train.
We will start learning about training and ML algorithms starting next practice.
```{r,  message=FALSE, warning=FALSE, results='hide', fig.show='hide'}
control <- trainControl(method="repeatedcv", number=10, repeats=3)
model <- train(Class~., data=glaucoma, method="multinom", preProcess="scale", trControl=control)
importance <- varImp(model, scale=FALSE) # estimate variable importance
print(importance)
plot(importance)
important_feat<-glaucoma[,rownames(importance$importance)[importance$importance$Overall>0.5]]
important_feat.pca <- prcomp(important_feat, center = TRUE,  scale. = TRUE)
summary(important_feat.pca)
important_feat[,"Class"]<-glaucoma[,"Class"]
important_feat_plot <- autoplot(important_feat.pca, data = important_feat, colour = 'Class')
fit_important <- lm(Class ~ ., cbind(Class = glaucoma$Class, important_feat.pca$x[, 1:2]) %>% as.data.frame())
```

We compared the results of all the PCA's we performed:

```{r, message=FALSE, warning=FALSE, echo=FALSE, fig.width=8, fig.height=3}
grid.arrange(default_pca_plot +
               labs(title = "all data" ,subtitle =
                                 paste(paste0("r-sqr= ", round(summary(fit_pca)$adj.r.squared, 3)),
                                 paste0(";hopkins=",round(hopkins(glaucoma[,-63], m=nrow(default_pca$x)-1),4)))) +
               theme(legend.position = "none"),
             corr_pca_plot + labs(title = "remove correlates",subtitle =
                                 paste(paste0("r-sqr= ", round(summary(fit_corr_pca )$adj.r.squared, 3)),
                                 paste0(";hopkins=",round(hopkins(data_no_corr[,-21], m=nrow(corr_pca$x)-1),3)))) +
               theme(legend.position = "none"),
             important_feat_plot + labs(title = "keep importatnt features",subtitle =
                                 paste(paste0("r-sqr= ", round(summary(fit_important)$adj.r.squared, 3)),
                                 paste0(";hopkins=",
                                        round(hopkins(important_feat[,-31], m=nrow(important_feat.pca$x)-1),4)))) +
               theme(legend.position = "right"),
             ncol=3,nrow=1, widths=c(4,4,5)) 
```
```{r, message=FALSE, warning=FALSE, echo=FALSE, results='hide'}
pca_stats <- data.frame(features = c("All features", "Without correlated features", "Only important features"),
                        r_squared = c(round(summary(fit_pca)$adj.r.squared, 3),
                                      round(summary(fit_corr_pca)$adj.r.squared, 3),
                                      round(summary(fit_important)$adj.r.squared, 3)),
                        hopkins = c(round(hopkins(glaucoma[,-63], m=nrow(default_pca$x)-1),4),
                                    round(hopkins(data_no_corr[,-21], m=nrow(corr_pca$x)-1),4),
                                    round(hopkins(important_feat[,-31], m=nrow(important_feat.pca$x)-1),4)))
print.data.frame(pca_stats)                      
```
It looks like there is no big difference this way or that. The best result we got from the data without the correlated features. We will sometimes try to run the algorithms on the smaller dataset without the correlated, but seeing as they are all fairly similar, we'll mainly work on the whole dataset.


## Split into training and test sets

We split the whole data set into 80% training set (teach the algorithm by giving it data with the expected results) and 20% test set (predicting on data where the algorithm doesn't know the result, but since it is in our dataset we do know it and can evaluate how well the algorithm succeeded in predicting).

Since we know that there are about a half of each Class in the data set, we wanted to be sure that the distribution is as we defined (about 1/2 of each Class in both training and test sets should be 'normal' and 1/2 'glaucoma'), so we split the data it 2 ways.

#### Split using an index
After shuffling, use the first 80% for training, the last 20% for test

```{r splitting, message=FALSE, warning=FALSE}
set.seed(123)
index <- sample(nrow(glaucoma),round(nrow(glaucoma)*.8),replace = FALSE) 
train_set <- glaucoma[index,]
test_set<- glaucoma[-index,]
train_div<-prop.table(table(train_set$Class))
testdiv<-prop.table(table(test_set$Class))
```
```{r, message=FALSE, warning=FALSE, echo=FALSE}

print(paste0('The glaucoma/normal distribution was: train set = ',
       round(train_div[1],2),"/",round(testdiv[1],2), ', and test set = ',
       round(train_div[2],2),"/",round(testdiv[2],2)))
```

#### Split using the function 'initial_split'
We then used the 'initial_split' function, with the built in 'training' and 'testing' functions
```{r better splitting, message=FALSE, warning=FALSE}
patient_split <- initial_split(glaucoma, strata = Class )
data_train <- training(patient_split) # we used the default parameters of splitting size
data_test <- testing(patient_split)
train_div<-prop.table(table(data_train$Class))
testdiv<-prop.table(table(data_test$Class))
```
```{r echo=FALSE}
print(paste0('The glaucoma/normal distribution was: train set = ',
       round(train_div[1],2),"/",round(testdiv[1],2), ', and test set = ',
       round(train_div[2],2),"/",round(testdiv[2],2)))
```

Using this method we got exactly 0.5 in both sets. We will use this division, so we saved the true "Classes" of the training and test sets. We also created a training and test set that include only the desired features.
```{r warning=FALSE, message=FALSE, echo=FALSE, results='hide'}
train_set_labels <-data_train["Class"] #save true "Classes" of the training set
test_set_labels <-data_test["Class"] #save true "Classes" of the test set
data_train_new<-data_train[,colnames(data_no_corr)]
data_test_new<-data_test[,colnames(data_no_corr)]
```
Since in most algorithms it is important for the data to be scaled, we decided to start by normalizing it.

#### normalization:
We decided to normalize the data, thinking that the range of values of each parameter is similar in order to compare the distances of different features with different scales.
```{r normalization, results='hide', message=FALSE, warning=FALSE}
normalize <- function(x) {return ((x - min(x)) / (max(x) - min(x)))}
```
```{r results='hide', message=FALSE, warning=FALSE, echo=FALSE}
normalize(c(1, 2, 3, 4, 5)) # test our function 1
normalize(c(10, 20, 30, 40, 50)) # test our function 2
```
```{r results='hide', message=FALSE, warning=FALSE}
train_set_norm <- cbind(as.data.frame(lapply(data_train[,-63], normalize)), Class=data_train$Class)
test_set_norm <- cbind(as.data.frame(lapply(data_test[,-63], normalize)), Class=data_test$Class)
summary(train_set_norm$ag) # test our dataset
```
```{r results='hide', message=FALSE, warning=FALSE, echo=FALSE}
train_set_norm_new<-train_set_norm[,colnames(data_no_corr)]
test_set_norm_new<-test_set_norm[,colnames(data_no_corr)]
```

#### z-score standardization: 
We also tried with z score scaling instead of normalization.
```{r z-score standardization, message=FALSE, warning=FALSE, results='hide'}
train_set_z <- cbind(as.data.frame(lapply(data_train[,-63], scale)), Class=data_train$Class)
test_set_z <- cbind(as.data.frame(lapply(data_test[,-63], scale)), Class=data_test$Class)
```
```{r results='hide', message=FALSE, warning=FALSE, echo=FALSE}
train_set_z_new<-train_set_z[,colnames(data_no_corr)]
test_set_z_new<-test_set_z[,colnames(data_no_corr)]
```

## algorithms evaluation
In order to decide which algorithm was best we calculated the accuracy, sensitivity, precision and specificity. We also wanted one general number that includes all the calculations, so we did:

- Sensitivity (true predicted glaucoma/have glaucoma) is the percentage of true positives -predicted to have glaucoma- out of all the people that actually have glaucoma. In other words- it refers to the test's ability to correctly detect ill patients who do have the condition. Since we don't want to miss a sick person that then won't be treated ( type II error), it is more important that the sensitivity  will be high. We decided to give it a weight of 4.

- Precision (true predicted glaucoma/predicted glaucoma), or positive predictive value (PPV) is the percentage of true positives out of all the predicted positives. We don't want to scare a person that isn't sick for nothing (type I error), but of course (in case of glaucoma) that error is preferable to type II error.  We decided to give it a weight of 2.

- Accuracy (true prediction/all predictions) is the proportion of correct predictions (both true positives and true negatives) among the total number of cases examined. It's a test to generally know how well we predicted, without segregating the positives from the negatives.  We decided to give it a weight of 2.

- Specificity (true predicted healthy/ healthy) relates to the test's ability to correctly reject healthy patients without a condition. It's also important, but less so.  We decided to give it a weight of 1.

```{r calc_score, results='hide', message=FALSE, warning=FALSE}
calc_score <- function(x) {return (round( ((2*x$Accuracy + 4*x$Sensitivity +
                                       2*x$Precision + x$Specificity) / 9 ),3)) }
```


# SVM (Support Vector Machines)

Support vectors are the data points that define the position and the margin of the hyper-plane. They are called “support” vectors, because these are the representative data points of the classes. If we move one of them, the position and/or the margin will change.
Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that assigns new examples into that same space and predicts which category they belong to based on which side of the gap they fall. The SVM algorithm maps training examples to points in space so as to maximize the width of the gap between the two categories.
The effectiveness of SVM depends a lot on the selection of kernel. We tried using 4 different kernels and compared their results.

### default 
```{r SVM default, message=FALSE, warning=FALSE, results='hide'}
Class_classifier_svm <-  ksvm(x=Class ~., data=data_train)
Class_predictions_svm <- predict(Class_classifier_svm, data_test)
svm_linear_norm <- confusionMatrix(Class_predictions_svm,data_test[,63], dnn = c('actual', 'predicted'))
svm_stats_default <- data.frame(
                           Accuracy=svm_linear_norm[["overall"]][["Accuracy"]],
                           Sensitivity=round(svm_linear_norm[["byClass"]] [["Sensitivity"]],3), 
                           Precision=round(svm_linear_norm[["byClass"]] [["Precision"]],3), 
                           Specificity=round(svm_linear_norm[["byClass"]] [["Specificity"]],3))
svm_stats_default$General <- round(calc_score(svm_stats_default),3)

```
We ran the algorithm on the datatset with the selected features
```{r  message=FALSE, warning=FALSE, results='hide', echo=FALSE}
Class_classifier_svm_new <-  ksvm(x=Class ~., data=data_train_new)
Class_predictions_svm_new <- predict(Class_classifier_svm_new, data_test_new)
svm_linear_norm_new <- confusionMatrix(Class_predictions_svm_new,data_test[,63], dnn = c('actual', 'predicted'))
svm_stats_default_new <- data.frame( 
                           Accuracy=svm_linear_norm_new[["overall"]][["Accuracy"]],
                           Sensitivity=round(svm_linear_norm_new[["byClass"]] [["Sensitivity"]],3), 
                           Precision=round(svm_linear_norm_new[["byClass"]] [["Precision"]],3), 
                           Specificity=round(svm_linear_norm_new[["byClass"]] [["Specificity"]],3))
svm_stats_default_new$General <- round(calc_score(svm_stats_default_new),3)
```
```{r  message=FALSE, warning=FALSE, echo=FALSE}
print(paste(paste0("the whole normalized dataset's score = ", svm_stats_default$General),
            paste0("> the abridged dataset's score = ", svm_stats_default_new$General)))
svm_stats<-svm_stats_default
rownames(svm_stats)<-"defalut"
```
Interestingly, we got a worse result using the selected featured than the whole dataset.
## different kernels

Since it is important when using SVM to work only on numeric and scaled data, we made sure our data was numeric (all columns originally numeric, and the ‘Class’ column we factorizes), randomized (we used the ‘sample’ function beforehand) and scaled. 
The package we used (kernlab) let us scale the data when running the algorithm itself, so we'll start by scaling with the algorithm. We built the model:
```{r linear SVM train, message=FALSE, warning=FALSE, results='hide'}
train_svm_linear<-train(Class ~ ., data_train,
                       method = "svmLinear",
                       trControl = trainControl(method="repeatedcv", number = 10, repeats = 10),
                       metric = "Kappa",  
                       preProcess = c("center","scale"))

svm_predictions_linear<-predict(train_svm_linear, data_test)
Class_classifier_linear <-  ksvm(x=Class ~., data=data_train,kernel = "vanilladot")
```

We saw that the model used 49 support vectors, and that the training error was 0.054795. SVM can be a bit of a black box so we don't know much else.
After building the model on our training set (80% of the data) we ran the model on the test set:
```{r Class_predictions_linear, message=FALSE, warning=FALSE}
Class_predictions_linear <- predict(Class_classifier_linear, data_test[,-63])
```
We used the table() function to compare the predicted Class to the true Class in the testing:
```{r svm_linear confusionMatrix, message=FALSE, warning=FALSE,results='hide'}
table(Class_predictions_linear, data_test$Class)
svm_linear <- confusionMatrix(Class_predictions_linear,data_test[,63])
```

Now we wanted to see how well our classifier performed according to the different ways of measuring success, and compared to using only the selected features.

```{r svm_linear_stats, message=FALSE, warning=FALSE, echo=FALSE}
df_linear<-data.frame(predictions=c(as.numeric(Class_predictions_linear)),labels = c(data_test$Class))

svm_linear_stats <- data.frame(  
                           Accuracy=svm_linear[["overall"]][["Accuracy"]],
                           Sensitivity=round(svm_linear[["byClass"]] [["Sensitivity"]],3), 
                           Precision=round(svm_linear[["byClass"]] [["Precision"]],3), 
                           Specificity=round(svm_linear[["byClass"]] [["Specificity"]],3))
svm_linear_stats$General <- round(calc_score(svm_linear_stats),3)

```
```{r message=FALSE, warning=FALSE, echo=FALSE,results='hide'}
train_svm_linear_new<-train(Class ~ ., data_train_new,
                       method = "svmLinear",
                       trControl = trainControl(method="repeatedcv", number = 10, repeats = 10),
                       metric = "Kappa",  
                       preProcess = c("center","scale"))

svm_predictions_linear_new<-predict(train_svm_linear_new, data_test_new)
Class_classifier_linear_new <-  ksvm(x=Class ~., data=data_train_new,kernel = "vanilladot")
Class_predictions_linear_new <- predict(Class_classifier_linear_new, data_test_new[,-63])
table(Class_predictions_linear_new, data_test$Class)
svm_linear_new <- confusionMatrix(Class_predictions_linear_new,data_test[,63])
svm_linear_new_stats <- data.frame(  
                           Accuracy=svm_linear_new[["overall"]][["Accuracy"]],
                           Sensitivity=round(svm_linear_new[["byClass"]] [["Sensitivity"]], 3),
                           Precision=round(svm_linear_new[["byClass"]] [["Precision"]],3), 
                           Specificity=round(svm_linear_new[["byClass"]] [["Specificity"]],3))
svm_linear_new_stats$General <- round(calc_score(svm_linear_new_stats),3)
```
```{r message=FALSE, warning=FALSE, echo=FALSE,}
print(paste("linear: the whole dataset's score = ", svm_linear_stats$General,"> the abridged dataset's score = ", svm_linear_new_stats$General))
svm_stats["linear",]<-svm_linear_stats
```

Again the whole dataset yielded better results.
A standard Gaussian Radial basis function (RBF) is the second kernel we tried.

```{r svm_rbf, message=FALSE, warning=FALSE}
Class_classifier_rbf <-  ksvm(x=Class ~., data=data_train,kernel = "rbfdot")
```
```{r svm rbf, message=FALSE, warning=FALSE, echo=FALSE}
Class_predictions_rbf <- predict(Class_classifier_rbf, data_test[,-63])
svm_rbf <- confusionMatrix(Class_predictions_rbf,data_test[,63])


# calculate performance
df_rbf<-data.frame(predictions=c(as.numeric(Class_predictions_rbf)),labels = c(data_test$Class))
svm_rbf_stats <- data.frame(  
                           Accuracy=svm_rbf[["overall"]][["Accuracy"]],
                           Sensitivity=round(svm_rbf[["byClass"]] [["Sensitivity"]],3), 
                           Precision=round(svm_rbf[["byClass"]] [["Precision"]],3), 
                           Specificity=round(svm_rbf[["byClass"]][["Specificity"]],3))
svm_rbf_stats$General <- round(calc_score(svm_rbf_stats),3)

Class_classifier_rbf_new <-  ksvm(x=Class ~., data=data_train_new,kernel = "rbfdot")
Class_predictions_rbf_new <- predict(Class_classifier_rbf_new, data_test_new[,-63])
svm_rbf_new <- confusionMatrix(Class_predictions_rbf_new,data_test[,63])
df_rbf_new<-data.frame(predictions=c(as.numeric(Class_predictions_rbf_new)),labels = c(data_test$Class))
pROC_svm_rbf_new <- roc(df_rbf_new$labels,df_rbf_new$predictions)
svm_rbf_stats_new <- data.frame(  
                           Accuracy=svm_rbf_new[["overall"]][["Accuracy"]],
                           Sensitivity=round(svm_rbf_new[["byClass"]] [["Sensitivity"]],3), 
                           Precision=round(svm_rbf_new[["byClass"]] [["Precision"]],3), 
                           Specificity=round(svm_rbf_new[["byClass"]][["Specificity"]],3))
svm_rbf_stats_new$General <- round(calc_score(svm_rbf_stats_new),3)
```
```{r message=FALSE, warning=FALSE, echo=FALSE,}
print(paste("RBF: the whole dataset's score = ", svm_rbf_stats$General,"> the abridged dataset's score = ", svm_rbf_stats_new$General))
svm_stats["RBF",]<-svm_rbf_stats
```
```{r svm_poly, message=FALSE, warning=FALSE}
Class_classifier_poly <-  ksvm(x=Class ~., data=data_train,kernel = "polydot")
```
```{r SVM Polynomial, message=FALSE, warning=FALSE, echo=FALSE}
Class_predictions_poly <- predict(Class_classifier_poly, data_test[,-63])
svm_poly <- confusionMatrix(Class_predictions_poly,data_test[,63])
# calculate performance
df_poly<-data.frame(predictions=c(as.numeric(Class_predictions_poly)),labels = c(data_test$Class))
pROC_svm_poly <- roc(df_poly$labels,df_poly$predictions)
svm_poly_stats <- data.frame(
                           Accuracy=svm_poly[["overall"]][["Accuracy"]],
                           Sensitivity=round(svm_poly[["byClass"]] [["Sensitivity"]],3), 
                           Precision=round(svm_poly[["byClass"]] [["Precision"]],3), 
                           Specificity=round(svm_poly[["byClass"]][["Specificity"]],3))
svm_poly_stats$General<-round(calc_score(svm_poly_stats),3)
Class_classifier_poly_new <-  ksvm(x=Class ~., data=data_train_new,kernel = "polydot")
Class_predictions_poly_new <- predict(Class_classifier_poly_new, data_test_new[,-63])
svm_poly_new <- confusionMatrix(Class_predictions_poly_new,data_test[,63])
df_poly_new<-data.frame(predictions=c(as.numeric(Class_predictions_poly_new)),labels = c(data_test$Class))
pROC_svm_pol_newy <- roc(df_poly_new$labels,df_poly_new$predictions)
svm_poly_stats_new <- data.frame( 
                           Accuracy=svm_poly_new[["overall"]][["Accuracy"]],
                           Sensitivity=round(svm_poly_new[["byClass"]] [["Sensitivity"]], 3),
                           Precision=round(svm_poly_new[["byClass"]] [["Precision"]],3), 
                           Specificity=round(svm_poly_new[["byClass"]][["Specificity"]],3))
svm_poly_stats_new$General <- round(calc_score(svm_poly_stats_new),3)
```
```{r message=FALSE, warning=FALSE, echo=FALSE,}
print(paste(paste0("polynomial: the whole dataset's score = ", svm_poly_stats$General),
            paste0("> the abridged dataset's score = ", svm_poly_stats_new$General)))
svm_stats["Polynomial",]=svm_poly_stats 
```
```{r SVM Hyperbolic tangent, message=FALSE, warning=FALSE, results='hide'}
Class_classifier_tan <- ksvm(x=Class ~., data=data_train,kernel = "tanhdot")
```

```{r SVM tan, message=FALSE, warning=FALSE, echo=FALSE, results='hide'}
Class_predictions_tan <- predict(Class_classifier_tan, data_test[,-63])
svm_tan <- confusionMatrix(Class_predictions_tan,data_test[,63])

# calculate performance
df_tan<-data.frame(predictions=c(as.numeric(Class_predictions_tan)),labels = c(data_test$Class))
pROC_svm_tan <- roc(df_tan$labels,df_tan$predictions)
svm_tan_stats <- data.frame(Accuracy=svm_tan[["overall"]][["Accuracy"]],
                           Sensitivity=round(svm_tan[["byClass"]] [["Sensitivity"]], 3),
                           Precision=round(svm_tan[["byClass"]] [["Precision"]],3), 
                           Specificity=round(svm_tan[["byClass"]][["Specificity"]],3))

#new data hyperbolic
Class_classifier_tan_new <- ksvm(x=Class ~., data=data_train_new,kernel = "tanhdot")
Class_predictions_tan_new <- predict(Class_classifier_tan_new, data_test_new[,-63])
svm_tan_new <- confusionMatrix(Class_predictions_tan_new,data_test[,63])
svm_tan_stats_new <- data.frame(Accuracy=svm_tan_new[["overall"]][["Accuracy"]],
                           Sensitivity=round(svm_tan_new[["byClass"]] [["Sensitivity"]], 3),
                           Precision=round(svm_tan_new[["byClass"]] [["Precision"]],3), 
                           Specificity=round(svm_tan_new[["byClass"]][["Specificity"]],3))
svm_tan_stats$General <-round(calc_score(svm_tan_stats),3)
svm_tan_stats_new$General <-round(calc_score(svm_tan_stats_new),3)


svm_stats["Hyperbolic",]=svm_tan_stats_new

```

```{r message=FALSE, warning=FALSE, echo=FALSE}
print(paste("Hyperbolic: the whole dataset's score = ", svm_tan_stats$General,"> the abridged dataset's score = ", svm_tan_stats_new$General))
```

```{r echo=FALSE, out.width='80%', results='hide'}
print.data.frame(data.frame(linear=c(svm_stats["linear",]$General, svm_linear_new_stats$General),
                            RBF=c(svm_stats["RBF",]$General, svm_rbf_stats_new$General),
                            Hyperbolic=c(svm_stats["Hyperbolic",]$General, svm_tan_stats_new$General), row.names = c("all features","selected featurs")))
```
We saw that again using only the selected features gave no advantage. The best result was using the RBF kernel, followed by the linear kernel:
```{r echo=FALSE, out.width='80%'}
print.data.frame(svm_stats[2:3,])
```
```{r message=FALSE, warning=FALSE, echo=FALSE, results='hide', fig.show='hide'}
t_svm_stats <- tableGrob(svm_stats[2:3,])
gtitle_svm_stats <- textGrob("SVM statistics")
gtable_svm_stats <- gtable_add_rows(t_svm_stats, heights = grobHeight(gtitle_svm_stats)+ unit(5,"mm"), pos = 0)
gtable_svm_stats <- gtable_add_grob(gtable_svm_stats, gtitle_svm_stats, 1, 1, 1, ncol(gtable_svm_stats))
grid.arrange(gtable_svm_stats,ncol=1)
```

We got the same results when running the RBF as the default, and that is the best result.
The linear, polynomial and hyperbolic kernels also yielded the same results, with a slight decrease in accuracy (from 86% to 82%), sensitivity (from 84% to 76%) and precision (from 87.5% to 86.4%). In both cases the specificity is 88% which is good.
The polynomial kernel yielded the exact same results as the linear kernel, so we didn't continue working with it.

Other kernels might prove even better, or the cost of constraints parameter C could be varied to modify the width of the decision boundary.

```{r svm_best_stats_1, message=FALSE, warning=FALSE, echo=FALSE}
#We saved the best scores
svm_best_stats<-svm_stats["RBF",]
rownames(svm_best_stats)<-c("RBF")
```

## pre-scaled data

### normalized data

We tried using the data that we normalized and compared the 2. While using the 'ksvm' function we chose "scaled=FALSE"

```{r svm normalized linear, message=FALSE, warning=FALSE, results='hide'}
Class_classifier_linear_norm <-  ksvm(x=Class ~., data=train_set_norm,
                                      kernel = "vanilladot", scaled=FALSE)
```

```{r message=FALSE, warning=FALSE, results='hide', echo=FALSE}
Class_predictions_linear_norm <- predict(Class_classifier_linear_norm, test_set_norm)
svm_linear_norm <- confusionMatrix(Class_predictions_linear_norm,data_test[,63])
pROC_svm_linear_norm <- roc(c(data_test$Class), c(as.numeric(Class_predictions_linear_norm)))
svm_linear_norm_stats <- data.frame( 
                           Accuracy=svm_linear_norm[["overall"]][["Accuracy"]],
                           Sensitivity=round(svm_linear_norm[["byClass"]][["Sensitivity"]], 3),
                           Precision=round(svm_linear_norm[["byClass"]][["Precision"]],3), 
                           Specificity=round(svm_linear_norm[["byClass"]][["Specificity"]],3))

Class_classifier_linear_norm_new <-  ksvm(x=Class ~., data=train_set_norm_new,
                                          kernel = "vanilladot", scaled=FALSE)
Class_predictions_linear_norm_new <- predict(Class_classifier_linear_norm_new, test_set_norm_new)
svm_linear_norm_new <- confusionMatrix(Class_predictions_linear_norm_new,data_test[,63])
pROC_svm_linear_norm_new <- roc(c(data_test$Class), c(as.numeric(Class_predictions_linear_norm_new)))
svm_linear_norm_stats_new <- data.frame( 
                           Accuracy=svm_linear_norm_new[["overall"]][["Accuracy"]],
                           Sensitivity=round(svm_linear_norm_new[["byClass"]][["Sensitivity"]], 3),
                           Precision=round(svm_linear_norm_new[["byClass"]][["Precision"]],3), 
                           Specificity=round(svm_linear_norm_new[["byClass"]][["Specificity"]],3))

svm_linear_norm_stats$General <- round(calc_score(svm_linear_norm_stats),3)
svm_linear_norm_stats_new$General <- round(calc_score(svm_linear_norm_stats_new),3)
```

As before we ran the same kernels (except for the polynomial which yielded the same result as the linear) on both the whole dataset and the selected part, but show here the process of the linear kernel only (the rest can be seen in our code).

```{r  message=FALSE, warning=FALSE, echo=FALSE}
print(paste("linear: the whole normalized dataset's score = ", svm_linear_norm_stats$General,"> the abridged dataset's score = ", svm_linear_norm_stats_new$General))

svm_norm_stats<-svm_linear_norm_stats
rownames(svm_norm_stats)<-"linear"
```

```{r svm normalized data, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}


# rbf
Class_classifier_rbf_norm <-  ksvm(x=Class ~., data=train_set_norm,kernel = "rbfdot", scaled=FALSE)
Class_predictions_rbf_norm <- predict(Class_classifier_rbf_norm, test_set_norm)
svm_rbf_norm <- confusionMatrix(Class_predictions_rbf_norm,data_test[,63])
pROC_svm_rbf_norm <- roc(c(data_test$Class), c(as.numeric(Class_predictions_rbf_norm)))
svm_rbf_norm_stats <- data.frame( 
                           Accuracy=svm_rbf_norm[["overall"]][["Accuracy"]],
                           Sensitivity=round(svm_rbf_norm[["byClass"]] [["Sensitivity"]], 3),
                           Precision=round(svm_rbf_norm[["byClass"]][["Precision"]],3), 
                        Specificity=round(svm_rbf_norm[["byClass"]][["Specificity"]],3))
svm_rbf_norm_stats$General<-round(calc_score(svm_rbf_norm_stats),3)

Class_classifier_rbf_norm_new <-  ksvm(x=Class ~., data=train_set_norm_new,kernel = "rbfdot", scaled=FALSE)
Class_predictions_rbf_norm_new <- predict(Class_classifier_rbf_norm_new, test_set_norm_new)
svm_rbf_norm_new <- confusionMatrix(Class_predictions_rbf_norm_new,data_test[,63])
pROC_svm_rbf_norm_new <- roc(c(data_test$Class), c(as.numeric(Class_predictions_rbf_norm_new)))
svm_rbf_norm_stats_new <- data.frame(
                           Accuracy=svm_rbf_norm_new[["overall"]][["Accuracy"]],
                           Sensitivity=round(svm_rbf_norm_new[["byClass"]] [["Sensitivity"]],3), 
                           Precision=round(svm_rbf_norm_new[["byClass"]][["Precision"]],3), 
                        Specificity=round(svm_rbf_norm_new[["byClass"]][["Specificity"]],3))

svm_rbf_norm_stats_new$General <- round(calc_score(svm_rbf_norm_stats_new),3)
```

```{r  message=FALSE, warning=FALSE, echo=FALSE}
print(paste("rbf: the whole normalized dataset's score = ", svm_rbf_norm_stats$General,"> the abridged dataset's score = ", svm_rbf_norm_stats_new$General))

svm_norm_stats["rbf",]<-svm_rbf_norm_stats
```


```{r  message=FALSE, warning=FALSE, echo=FALSE, results='hide'}
# Hyperbolic tangent
Class_classifier_tan_norm <-  ksvm(x=Class ~., data=train_set_norm,kernel = "tanhdot", scaled=FALSE)
Class_predictions_tan_norm <- predict(Class_classifier_tan_norm, test_set_norm)
svm_tan_norm <- confusionMatrix(Class_predictions_tan_norm,data_test[,63])
pROC_svm_tan_norm <- roc(c(data_test$Class), c(as.numeric(Class_predictions_tan_norm)))
svm_tan_norm_stats <- data.frame(
                           Accuracy=svm_tan_norm[["overall"]][["Accuracy"]],
                           Sensitivity=round(svm_tan_norm[["byClass"]] [["Sensitivity"]],3), 
                           Precision=round(svm_tan_norm[["byClass"]] [["Precision"]],3), 
                           Specificity=round(svm_tan_norm[["byClass"]][["Specificity"]],3))
svm_tan_norm_stats$General <- round(calc_score(svm_tan_norm_stats),3)

Class_classifier_tan_norm_new <-  ksvm(x=Class ~., data=train_set_norm_new,kernel = "tanhdot", scaled=FALSE)
Class_predictions_tan_norm_new <- predict(Class_classifier_tan_norm_new, test_set_norm_new)
svm_tan_norm_new <- confusionMatrix(Class_predictions_tan_norm_new,data_test[,63])
pROC_svm_tan_norm_new <- roc(c(data_test$Class), c(as.numeric(Class_predictions_tan_norm_new)))
svm_tan_norm_stats_new <- data.frame(
                           Accuracy=svm_tan_norm_new[["overall"]][["Accuracy"]],
                           Sensitivity=round(svm_tan_norm_new[["byClass"]] [["Sensitivity"]], 3),
                           Precision=round(svm_tan_norm_new[["byClass"]] [["Precision"]],3), 
                           Specificity=round(svm_tan_norm_new[["byClass"]][["Specificity"]],3))

svm_tan_norm_stats_new$General <- round(calc_score(svm_tan_norm_stats_new),3)


```



```{r message=FALSE, warning=FALSE, echo=FALSE, fig.height=3, fig.width=7}
print(paste("tan: the whole normalized dataset's score = ", svm_tan_norm_stats$General,"< the abridged dataset's score = ", svm_tan_norm_stats_new$General))

svm_norm_stats["tan, no corr",]<-svm_tan_norm_stats_new
```


We saw that again using only the selected features gave no advantage. The 'General' scores were:
```{r echo=FALSE, out.width='80%', results='hide'}
print.data.frame(data.frame(linear=c(svm_norm_stats["linear",]$General, svm_linear_norm_stats_new$General),
                            RBF=c(svm_norm_stats["rbf",]$General, svm_rbf_norm_stats_new$General),
                            Hyperbolic=c(svm_norm_stats["tan, no corr",]$General, svm_tan_norm_stats_new$General), row.names = c("all features","selected featurs")))
```

The best result was using the RBF kernel, followed by the linear kernel:
Using the linear, RBF and Polynomial kernels we managed to got to specificity and precision of 1: all those predicted healthy were actually healthy. But it came at the price of sensitivity: the linear and polynomial yielded a sensitivity of 64%, and the RBF of 68%. 

```{r message=FALSE, warning=FALSE, echo=FALSE, fig.show='hide', results='hide'}
knitr::kable(rbind(svm_norm_stats[-3,], "tan, all data"=svm_tan_norm_stats),
             caption = 'SVM results using our normalization function')%>%
  kable_paper("striped", full_width = F) %>%
  column_spec(6, bold = T) %>%
  row_spec(2, bold = T, color = "black", background = "yellow")

svm_best_stats["RBF norm",]<-svm_norm_stats["rbf",]
```
##### SVM results using our normalization function:
```{r echo=FALSE}
print.data.frame(svm_norm_stats)
```
Interestingly, the hyperbolic kernel on the whole dataset yielded the worst results by far, with sensitivity and precision of 0! the same kernel on the smaller dataset yielded a bit higher results, but still very bad ones (our general score jumped from 24% to 60.2%, but the other kernels are at 82% or 84%)

### z-score scaled

We tried scaling with z-score as well. The rest of the code is exactly as before so we did not print it in the report.

```{r  message=FALSE, warning=FALSE, results='hide'}
Class_classifier_linear_z <-  ksvm(x=Class ~., data=train_set_z,kernel = "vanilladot", scaled=FALSE)
```
```{r svm scaled data linear, message=FALSE, warning=FALSE, results='hide',echo=FALSE}
Class_predictions_linear_z <- predict(Class_classifier_linear_z, test_set_z)
svm_linear_z <- confusionMatrix(Class_predictions_linear_z,data_test[,63])
pROC_svm_linear_z <- roc(c(data_test$Class), c(as.numeric(Class_predictions_linear_z)))
svm_linear_z_stats <- data.frame(
                           Accuracy=svm_linear_z[["overall"]][["Accuracy"]],
                           Sensitivity=round(svm_linear_z[["byClass"]] [["Sensitivity"]], 3),
                           Precision=round(svm_linear_z[["byClass"]][["Precision"]],3), 
                           Specificity=round(svm_linear_z[["byClass"]][["Specificity"]],3))

Class_classifier_linear_z_new <-  ksvm(x=Class ~., data=train_set_z_new,kernel = "vanilladot", scaled=FALSE)
Class_predictions_linear_z_new <- predict(Class_classifier_linear_z_new, test_set_z_new)
svm_linear_z_new <- confusionMatrix(Class_predictions_linear_z,data_test[,63])
pROC_svm_linear_z_new <- roc(c(data_test$Class), c(as.numeric(Class_predictions_linear_z_new)))
svm_linear_z_new_stats <- data.frame(
                           Accuracy=svm_linear_z_new[["overall"]][["Accuracy"]],
                           Sensitivity=round(svm_linear_z_new[["byClass"]] [["Sensitivity"]], 3),
                           Precision=round(round(svm_linear_z_new[["byClass"]][["Precision"]],3), 3),
                           Specificity=round(svm_linear_z_new[["byClass"]][["Specificity"]],3)) 
svm_linear_z_stats$General <- round(calc_score(svm_linear_z_stats),3)
svm_linear_z_new_stats$General <- round(calc_score(svm_linear_z_new_stats),3)

print(paste(paste0("the whole z-scaled dataset's score (linear) is ", svm_linear_z_stats$General),
            paste0("; the abridged dataset's score = ", svm_linear_z_new_stats$General)))
```

```{r svm scaled data, message=FALSE, warning=FALSE, echo=FALSE, results='hide'}
svm_z_stats<-svm_linear_z_stats
rownames(svm_z_stats)<-c("linear")

# rbf
Class_classifier_rbf_z <-  ksvm(x=Class ~., data=train_set_z,kernel = "rbfdot", scaled=FALSE)
Class_predictions_rbf_z <- predict(Class_classifier_rbf_z, test_set_z)
svm_rbf_z <- confusionMatrix(Class_predictions_rbf_z,data_test[,63])
pROC_svm_rbf_z <- roc(c(data_test$Class), c(as.numeric(Class_predictions_rbf_z)))
svm_rbf_z_stats <- data.frame( 
                           Accuracy=svm_rbf_z[["overall"]][["Accuracy"]],
                           Sensitivity=round(svm_rbf_z[["byClass"]] [["Sensitivity"]],3), 
                           Precision=round(svm_rbf_z[["byClass"]] [["Precision"]],3), 
                           Specificity=round(svm_rbf_z[["byClass"]] [["Specificity"]],3)) 

Class_classifier_rbf_z_new <-  ksvm(x=Class ~., data=train_set_z_new,kernel = "rbfdot", scaled=FALSE)
Class_predictions_rbf_z_new <- predict(Class_classifier_rbf_z_new, test_set_z_new)
svm_rbf_z_new <- confusionMatrix(Class_predictions_rbf_z_new,data_test[,63])
pROC_svm_rbf_z_new <- roc(c(data_test$Class), c(as.numeric(Class_predictions_rbf_z_new)))
svm_rbf_z_stats_new <- data.frame(
                           Accuracy=svm_rbf_z_new[["overall"]][["Accuracy"]],
                           Sensitivity=round(svm_rbf_z_new[["byClass"]] [["Sensitivity"]],3), 
                           Precision=round(svm_rbf_z_new[["byClass"]] [["Precision"]],3), 
                           Specificity=round(svm_rbf_z_new[["byClass"]] [["Specificity"]],3)) 

svm_rbf_z_stats$General <- round(calc_score(svm_rbf_z_stats),3)
svm_rbf_z_stats_new$General <- round(calc_score(svm_rbf_z_stats_new),3)

print(paste(paste0("the whole z-scaled dataset's score (RBF) is ", svm_rbf_z_stats$General),
            paste0("; the abridged dataset's score = ", svm_rbf_z_stats_new$General)))

svm_z_stats["rbf",]<-svm_rbf_z_stats


# Hyperbolic tangent
Class_classifier_tan_z <-  ksvm(x=Class ~., data=train_set_z,kernel = "tanhdot", scaled=FALSE)
Class_predictions_tan_z <- predict(Class_classifier_tan_z, test_set_z)
svm_tan_z <- confusionMatrix(Class_predictions_tan_z,data_test[,63])
pROC_svm_tan_z <- roc(c(data_test$Class), c(as.numeric(Class_predictions_tan_z)))
svm_tan_z_stats <- data.frame(  
                           Accuracy=svm_tan_z[["overall"]][["Accuracy"]],
                           Sensitivity=round(svm_tan_z[["byClass"]] [["Sensitivity"]], 3),
                           Precision=round(svm_tan_z[["byClass"]] [["Precision"]],3), 
                           Specificity=round(svm_tan_z[["byClass"]][["Specificity"]],3)) 

Class_classifier_tan_z_new <-  ksvm(x=Class ~., data=train_set_z_new,kernel = "tanhdot", scaled=FALSE)
Class_predictions_tan_z_new <- predict(Class_classifier_tan_z_new, test_set_z_new)
svm_tan_z_new <- confusionMatrix(Class_predictions_tan_z_new,data_test[,63])
pROC_svm_tan_z_new <- roc(c(data_test$Class), c(as.numeric(Class_predictions_tan_z_new)))
svm_tan_z_stats_new <- data.frame(  
                           Accuracy=svm_tan_z_new[["overall"]][["Accuracy"]],
                           Sensitivity=round(svm_tan_z_new[["byClass"]] [["Sensitivity"]], 3),
                           Precision=round(svm_tan_z_new[["byClass"]] [["Precision"]],3), 
                           Specificity=round(svm_tan_z_new[["byClass"]][["Specificity"]],3)) 
svm_tan_z_stats$General <- calc_score(svm_tan_z_stats)
svm_tan_z_stats_new$General <- calc_score(svm_tan_z_stats_new)


print(paste(paste0("the whole z-scaled dataset's score (hyperbolic) is ", svm_tan_z_stats$General),
            paste0("; the abridged dataset's score = ", svm_tan_z_stats_new$General)))
svm_z_stats["tan, no corr",]<-svm_tan_z_stats_new
```
```{r fig.width=7,echo=FALSE, results='hide', fig.show='hide'}
knitr::kable(svm_z_stats, caption = 'SVM results on scaled data')%>%
  kable_paper("striped", full_width = F) %>%
  column_spec(6, bold = T) %>%
  row_spec(2, bold = T, color = "black", background = "yellow")

svm_best_stats["RBF z-scaled",]<-svm_z_stats["rbf",]
```

##### SVM results on scaled data:
```{r echo=FALSE}
print.data.frame(svm_z_stats)
```
Again, we got the best results while using the RBF kernel, though this time it was better in all aspects.

## SVM conclusions

We saw that out of all the kernels, the standard Gaussian Radial basis function (RBF) gave the best result. We also saw that using the smaller dataset that included only the selected features did not yield any better results. One last thing to note is that normalization yielded the same accuracy (86%), worse sensitivity (72% as opposed to 84%), but managed to get to 100% in both precision and specificity. 

```{r message=FALSE, warning=FALSE, echo=FALSE, fig.height=3, fig.width=7}


knitr::kable(svm_best_stats, caption = 'best SVM results')%>% kable_paper("striped", full_width = F) %>% column_spec(6, bold = T) %>%  row_spec(1, bold = T, color = "black", background = "yellow")
```

Using the normalized data, all  25 of the healthy were predicted as healthy, whereas using the default scaling 3 were told they were sick when they were actually healthy.
On the other hand, out of 25 people with glaucoma, when we used the default scaling with the RBF kernel it predicted correctly  21  with glaucoma (so 4 sick people were told they are healthy), as opposed to only 18  that the normalized data yielded, (so 7 sick people think they are healthy..).
The 'General' score the way we decided to calculate it gave the best result to the RBF kernel on the scaled/default data (85.7% as opposed to 84.4%), even though there was a 100% success rate in both precision and specificity, but if we calculate it differently (for instance less weight on sensitivity) we might decide that scaling might do a better job with normalizing. In this case, we decided it's preferable that 3 people think they are sick and later on learn they are healthy if 3 less people are diagnosed correctly when they are sick, so we will choose the default as the best SVM result.

#### confusion matrix of the best SVM model:
```{r message=FALSE, warning=FALSE, echo=FALSE}
grid.table(svm_rbf[["table"]])
best_stats<-cbind(svm_best_stats["RBF",], notes="RBF")
rownames(best_stats)<-"SVM"
```

We saved the SVM results of our normalized data with the RBF kernel to compare with the rest of the algorithms.

# K-Nearest Neighbors (KNN) algorithm

The KNN algorithm is a non-parametric supervised learning method, used for classification and regression (we used it for classification). It is based on the assumption that similar objects exist in close proximity (are near to each other). K is a positive integer (a user-defined constant) that represents the number of training samples. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class which is most frequent among the k training samples nearest to that query point.

## normalized data
### choose K's manually
We started by choosing different K's manually. We chose and tried k=3,9,15,21, but we show here the whole process just for k=3
```{r  message=FALSE, warning=FALSE, results='hide'}
pred_knn_3 <- knn(train = train_set_norm[,-63], test = test_set_norm[,-63], cl = data_train[,63], k=3)
knn_table_3<-CrossTable(x = data_test[,63], y = pred_knn_3, prop.chisq=FALSE)[["t"]]
knn_seperate_cv_stats <- data.frame(k=3, Accuracy=accuracy(knn_table_3)$.estimate,
                           Sensitivity=round(sensitivity(knn_table_3),3), 
                           Precision=round(precision(knn_table_3),3), 
                           Specificity=round(specificity(knn_table_3),3))
```
```{r manual k choice, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}

pred_knn_9 <- knn(train = train_set_norm[,-63], test = test_set_norm[,-63], cl = data_train[,63], k=9)
knn_table_9<-CrossTable(x = data_test[,63], y = pred_knn_9, prop.chisq=FALSE)[["t"]]

pred_knn_15 <- knn(train = train_set_norm[,-63], test = test_set_norm[,-63], cl = data_train[,63], k=15)
knn_table_15<-CrossTable(x = data_test[,63], y = pred_knn_15, prop.chisq=FALSE)[["t"]]

pred_knn_21 <- knn(train = train_set_norm[,-63], test = test_set_norm[,-63], cl = data_train[,63], k=21)
knn_table_21<-CrossTable(x = data_test[,63], y = pred_knn_21, prop.chisq=FALSE)[["t"]]

knn_seperate_cv_stats <- data.frame(
                           Accuracy=c(accuracy(knn_table_3)$.estimate,
                                    accuracy(knn_table_9)$.estimate,
                                    accuracy(knn_table_15)$.estimate,
                                    accuracy(knn_table_21)$.estimate),
                           Sensitivity=c(round(sensitivity(knn_table_3),3),
                                       round(sensitivity(knn_table_9),3),
                                       round(sensitivity(knn_table_15),3),
                                       round(sensitivity(knn_table_21),3)), 
                           Precision=c(round(precision(knn_table_3),3),
                                       round(precision(knn_table_9),3),
                                       round(precision(knn_table_15),3),
                                       round(precision(knn_table_21),3)), 
                           Specificity=c(round(specificity(knn_table_3),3),
                                       round(specificity(knn_table_9),3),
                                       round(specificity(knn_table_15),3),
                                       round(specificity(knn_table_21),3)),
                           other=c("k=3","k=9","k=15","k=21"))
knn_seperate_cv_stats$General <- round(calc_score(knn_seperate_cv_stats),3)
knn_seperate_cv_stats
best_knn_seperate_cv_stats<-knn_seperate_cv_stats[1,]
```
The best sensitivity result (86.4%) was for k=9, but the accuracy was 82%, the precision was 76% and specificity of 78.6%.
Using our calculation of the best score, the best results we got was with k=3, with accuracy, sensitivity, precision and specificity all equal to 84%. 
We then ran the again algorithm with k=3, but time on the selected features only.
```{r message=FALSE, warning=FALSE, echo=FALSE,results='hide'}
pred_knn_3_new <- knn(train = train_set_norm_new[,-21], test = test_set_norm_new[,-21], cl = data_train[,63], k=3)
knn_table_3_new<-CrossTable(x = data_test[,63], y = pred_knn_3_new, prop.chisq=FALSE)[["t"]]
knn_seperate_cv_stats_new <- data.frame(Accuracy=accuracy(knn_table_3_new)$.estimate,
                           Sensitivity=round(sensitivity(knn_table_3_new),3), 
                           Precision=round(precision(knn_table_3_new),3), 
                           Specificity=round(specificity(knn_table_3_new),3),
                           "k=3")
knn_seperate_cv_stats_new$General <- round(calc_score(knn_seperate_cv_stats_new),3)
```
```{r message=FALSE, warning=FALSE, echo=FALSE, fig.width=6, fig.height=3}
print(paste(paste0("k=3: the whole dataset's score is ", best_knn_seperate_cv_stats$General),
            paste0("> the abridged dataset's score > ", knn_seperate_cv_stats_new$General)))
best_knn_stats<-best_knn_seperate_cv_stats
rownames(best_knn_stats)<-c("manually chosen K's")

```
We got much better results with the whole dataset.

### choose K's with Caret's "train"
Instead of choosing the K's manually we can use tuneGrid and find the best k in the range of 1-30

#### cross validation:
We trained the algorithm on the normalized training set with the 'train' function of the 'caret' library with different options of K (in the range of 1-30). We started with cross validation. 
```{r message=FALSE, warning=FALSE}
knn_train_cv<-train(Class ~ .,train_set_norm, method = "knn",
                       trControl = trainControl(method = "cv", number = 10),
                       tuneGrid = data.frame(k=seq(1,30,2)),
                       metric = "Kappa")
```
We used confusionMatrix to check how well we did.
```{r message=FALSE, warning=FALSE}
knn_predictions_cv<-predict(knn_train_cv, test_set_norm)
knn_cv_mat <- confusionMatrix(knn_predictions_cv,data_test[,63])
```
We again did the same thing just on the dataset that didn't include high correlations.
```{r KNN cross validation stats, message=FALSE, warning=FALSE, echo=FALSE}
knn_cv_stats<- data.frame( 
                           Accuracy=knn_cv_mat[["overall"]][["Accuracy"]],
                           Sensitivity=round(knn_cv_mat[["byClass"]][["Sensitivity"]],3), 
                           Precision=round(knn_cv_mat[["byClass"]][["Precision"]],3), 
                           Specificity=round(knn_cv_mat[["byClass"]][["Specificity"]],3),
                           K=paste0("k=",knn_train_cv[["bestTune"]][["k"]]))
 
knn_train_cv_new<-train(Class ~ .,train_set_norm_new,
                       method = "knn",
                       trControl = trainControl(method = "cv", number = 10),
                       tuneGrid = data.frame(k=seq(1,30,2)),
                       metric = "Kappa")
knn_predictions_cv_new<-predict(knn_train_cv_new, test_set_norm_new)
knn_cv_mat_new <- confusionMatrix(knn_predictions_cv_new,data_test[,63])
knn_cv_new_stats<- data.frame( 
                           Accuracy=knn_cv_mat_new[["overall"]][["Accuracy"]],
                           Sensitivity=round(knn_cv_mat_new[["byClass"]][["Sensitivity"]], 3),
                           Precision=round(knn_cv_mat_new[["byClass"]][["Precision"]],3), 
                           Specificity=round(knn_cv_mat_new[["byClass"]][["Specificity"]],3),
                           K=paste0("k=3"))
knn_cv_stats$General<-round(calc_score(knn_cv_stats),3)
knn_cv_new_stats$General<-round(calc_score(knn_cv_new_stats),3)
print(paste(paste0("CV: the whole dataset's score = ", knn_cv_stats$General ),
            paste0("> the abridged dataset's score = ", knn_cv_new_stats$General)))
```
The sensitivity was higher on the smaller dataset (80%->84%) with one less sick diagnosed as healthy, and the accuracy stayed the same, but the precision (87%->75%) and specificity (88%->72%) both decreased drastically (4 more healthy were diagnosed as sick), so the overall grade was higher for the dataset that didn't include high correlations.
```{r echo=FALSE}
best_knn_stats["CV, selected",]<-knn_cv_stats
```
Compared to the normalized manually chosen data we have an increase in both precision (84%->87%) and the specificity (84%->88%), but a decrease in sensitivity (84%->80%). Because we put so much weight on the sensitivity, even though the precision and specificity were higher (the accuracy stayed 84%), the overall general score was lower.

#### repeated cross validation:
We wanted to see if using repeated cross validation will yield better results.
```{r knn_train_repeatedcv, message=FALSE, warning=FALSE}
knn_train_repeatedcv<-train(Class ~.,train_set_norm,
                       method = "knn",
                       tuneGrid = data.frame(k=seq(1,30,2)),
                       trControl = trainControl(method = "repeatedcv", number = 10, repeats = 10),
                       metric = "Kappa")
```

```{r knn_predictions_rep_cv, message=FALSE, warning=FALSE, echo=FALSE}
knn_predictions_rep_cv<-predict(knn_train_repeatedcv, test_set_norm)
knn_repcv_mat <- confusionMatrix(knn_predictions_rep_cv,data_test[,63],dnn = c('predicted', 'actual'))
knn_df_norm_rep_cv<-data.frame(predictions=c(as.numeric(knn_predictions_rep_cv)),labels = c(data_test[,63]))
knn_repeatedcv_stats<- data.frame(
                           Accuracy=knn_repcv_mat[["overall"]][["Accuracy"]],
                           Sensitivity=round(knn_repcv_mat[["byClass"]] [["Sensitivity"]],3), 
                           Precision=round(knn_repcv_mat[["byClass"]][["Precision"]],3), 
                           Specificity=round(knn_repcv_mat[["byClass"]][["Specificity"]],3),
                           K=paste0("k=", knn_train_repeatedcv[["bestTune"]][["k"]])) 

```


```{r warning=FALSE, message=FALSE, echo=FALSE, results='hide'}
knn_train_repeatedcv_new<-train(Class ~.,train_set_norm_new,
                       method = "knn",
                       tuneGrid = data.frame(k=seq(1,30,2)),
                       trControl = trainControl(method = "repeatedcv", number = 10, repeats = 10),
                       metric = "Kappa")
knn_predictions_rep_cv_new<-predict(knn_train_repeatedcv_new, test_set_norm_new)
knn_repcv_mat_new <- confusionMatrix(knn_predictions_rep_cv_new,data_test[,63],dnn = c('predicted', 'actual'))
knn_df_norm_rep_cv_new<-data.frame(predictions=c(as.numeric(knn_predictions_rep_cv_new)),labels = c(data_test[,63]))
knn_repeatedcv_new_stats <- data.frame(
                           Accuracy=knn_repcv_mat_new[["overall"]][["Accuracy"]],
                           Sensitivity=round(knn_repcv_mat_new[["byClass"]] [["Sensitivity"]],3), 
                           Precision=round(knn_repcv_mat_new[["byClass"]][["Precision"]],3), 
                           Specificity=round(knn_repcv_mat_new[["byClass"]][["Specificity"]],3),
                           K=paste0("k=", knn_train_repeatedcv_new[["bestTune"]][["k"]])) 
knn_repeatedcv_stats$General<-round(calc_score(knn_repeatedcv_stats),3)
knn_repeatedcv_new_stats$General<-round(calc_score(knn_repeatedcv_new_stats),3)

```
```{r echo=FALSE}
print(paste(paste0("repeated CV: the whole dataset's score = ",knn_repeatedcv_stats$General),
            paste0("< and the abridged dataset's score = ",knn_repeatedcv_new_stats$General)))
best_knn_stats["repeated CV",]<-knn_repeatedcv_stats
```
This time there was a preference toward the whole dataset, with all evaluations increasing with the smaller dataset.

```{r echo=FALSE, fig.show='hide', results='hide'}
knitr::kable(best_knn_stats[,-5], caption = " KNN- normalized data")%>%
  kable_paper("striped", full_width = F) %>%
  column_spec(6, bold = T) %>%
  row_spec(1, bold = T, color = "black", background = "yellow")
```


We wanted to see if we get better results if we use scaling on our data instead of normalizing. Between CV and repeated CV, on the smaller dataset we got better results using repeated CV, but they were both not as good as the one from the whole dataset, which there was no difference between the 2, so we will run the algorithm on the scaled dataset using repeated cross validation (and choosing manually, which so far yielded the best results).

## z-score standardization 

### manually

We then tried with z score scaling instead of normalization. Again we started by choosing manually, and tried k=3,9,15,21, though we show here just for k=3
```{r pred_knn_3_z, message=FALSE, warning=FALSE, results='hide'}
pred_knn_3_z <- knn(train = train_set_z[,-63], test = test_set_z[,-63], cl = data_train[,63], k=3)
knn_table_3_z<-CrossTable(x = data_test[,63], y = pred_knn_3_z, prop.chisq=FALSE)[["t"]]
```
```{r knn k choice z, message=FALSE, warning=FALSE, echo=FALSE, results='hide'}
pred_knn_9_z <- knn(train = train_set_z[,-63], test = test_set_z[,-63], cl = data_train[,63], k=9)
knn_table_9_z<-CrossTable(x = data_test[,63], y = pred_knn_9_z, prop.chisq=FALSE)[["t"]]

pred_knn_15_z <- knn(train = train_set_z[,-63], test = test_set_z[,-63], cl = data_train[,63], k=15)
knn_table_15_z<-CrossTable(x = data_test[,63], y = pred_knn_15_z, prop.chisq=FALSE)[["t"]]

pred_knn_21_z <- knn(train = train_set_z[,-63], test = test_set_z[,-63], cl = data_train[,63], k=21)
knn_table_21_z<-CrossTable(x = data_test[,63], y = pred_knn_21_z, prop.chisq=FALSE)[["t"]]

knn_seperate_cv_stats_z <- data.frame(
                           Accuracy=c(accuracy(knn_table_3_z)$.estimate,
                                    accuracy(knn_table_9_z)$.estimate,
                                    accuracy(knn_table_15_z)$.estimate,
                                    accuracy(knn_table_21_z)$.estimate),
                           Sensitivity=c(round(sensitivity(knn_table_3_z),3),
                                       round(sensitivity(knn_table_9_z),3),
                                       round(sensitivity(knn_table_15_z),3),
                                       round(sensitivity(knn_table_21_z),3)), 
                           Precision=c(round(precision(knn_table_3_z),3),
                                       round(precision(knn_table_9_z),3),
                                       round(precision(knn_table_15_z),3),
                                       round(precision(knn_table_21_z),3)), 
                           Specificity=c(round(specificity(knn_table_3_z),3),
                                       round(specificity(knn_table_9_z),3),
                                       round(specificity(knn_table_15_z),3),
                                       round(specificity(knn_table_21_z),3)),
                           K=c("k=3","k=9","k=15","k=21"))
knn_seperate_cv_stats_z$General <- calc_score(knn_seperate_cv_stats_z)
knn_seperate_cv_stats_z_best<-knn_seperate_cv_stats_z[1,]
```
We got the best 'General' result from k=3, with precision of 88% and specificity of 87%, but with sensitivity of 81.5%. The best sensitivity (84%) we got with k=21, but with precision and specificity of 84%.
We wanted again to check if we can get better results with the smaller dataset. We decided to run it only with k=3, since that was the best result according to our calculations.

```{r message=FALSE, warning=FALSE, echo=FALSE, results='hide'}
pred_knn_3_z_new <- knn(train = train_set_z_new[,-21], test = test_set_z_new[,-21], cl = data_train[,63], k=3)
knn_table_3_z_new<-CrossTable(x = data_test[,63], y = pred_knn_3_z_new, prop.chisq=FALSE)[["t"]]
knn_seperate_cv_stats_z_new <- data.frame(Accuracy=accuracy(knn_table_3_z_new)$.estimate,
                           Sensitivity=round(sensitivity(knn_table_3_z_new),3), 
                           Precision=round(precision(knn_table_3_z_new),3), 
                           Specificity=round(specificity(knn_table_3_z_new),3),k="k=3")

knn_seperate_cv_stats_z_best$General<-round(calc_score(knn_seperate_cv_stats_z_best),3)
knn_seperate_cv_stats_z_new$General<-round(calc_score(knn_seperate_cv_stats_z_new),3)
print(paste(paste0("k=3: the whole z-scaled dataset's score = ",knn_seperate_cv_stats_z_best$General),
            paste0("< the abridged dataset's score = ",knn_seperate_cv_stats_z_new$General)))
```

```{r message=FALSE, warning=FALSE}

best_knn_stats[paste(
  "manually", knn_seperate_cv_stats_z_new$k,", scaled, selected"),] <-
  knn_seperate_cv_stats_z_new
```

We saw that the results were much better using the z-score scaling. The best result was from k=3, with accuracy, sensitivity, precision and and specificity all are 92% (out of 25 sick 23 were predicted sick, out of 25 healthy 23 were predicted healthy) 
We then proceeded to check the cress validation using Caret again but on scaled data

### repeated cross validation on scaled data
```{r knn z score train, message=FALSE, warning=FALSE, results='hide'}
knn_train_repeatedcv_z<-train(train_set_z[,-63],data_train[,63],
                       method = "knn",
                       tuneGrid = data.frame(k=seq(1,30,2)),
                       trControl = trainControl(method = "repeatedcv", number = 10, repeats = 10),
                       metric = "Kappa")
```
```{r knn z score, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
knn_predictions_rep_cv_z<-predict(knn_train_repeatedcv_z, test_set_z[,-63])
knn_repcv_mat_z <- confusionMatrix(knn_predictions_rep_cv_z,data_test[,63])

knn_z_table<-CrossTable(x = data_test[,63], y = knn_predictions_rep_cv_z, prop.chisq=FALSE)
df_z<-data.frame(predictions=c(as.numeric(knn_predictions_rep_cv_z)),labels = c(data_test[,63]))
knn_cv_stats_z <- data.frame(
                          Accuracy=knn_repcv_mat_z[["overall"]][["Accuracy"]],
                           Sensitivity=round(knn_repcv_mat_z[["byClass"]][["Sensitivity"]], 3),
                           Precision=round(knn_repcv_mat_z[["byClass"]][["Precision"]],3), 
                           Specificity=round(knn_repcv_mat_z[["byClass"]][["Specificity"]],3),
                           K=paste0("k=", knn_train_repeatedcv_z[["bestTune"]][["k"]])) 
```
```{r message=FALSE, warning=FALSE, echo=FALSE,results='hide'}
knn_train_repeatedcv_z_new<-train(train_set_z_new[,-21],data_train[,63],
                       method = "knn",
                       tuneGrid = data.frame(k=seq(1,30,2)),
                       trControl = trainControl(method = "cv", number = 10, repeats = 10),
                       metric = "Kappa")

knn_predictions_rep_cv_z_new<-predict(knn_train_repeatedcv_z_new, test_set_z_new[,-21])
knn_repcv_mat_z_new <- confusionMatrix(knn_predictions_rep_cv_z_new,data_test[,63])

df_z_new<-data.frame(predictions=c(as.numeric(knn_predictions_rep_cv_z_new)),labels = c(data_test[,63]))
knn_cv_stats_z_new <- data.frame(
                          Accuracy=knn_repcv_mat_z_new[["overall"]][["Accuracy"]],
                           Sensitivity=round(knn_repcv_mat_z_new[["byClass"]][["Sensitivity"]],3), 
                           Precision=round(knn_repcv_mat_z_new[["byClass"]][["Precision"]],3), 
                           Specificity=round(knn_repcv_mat_z_new[["byClass"]][["Specificity"]],3),
                           K=paste0("k=", knn_train_repeatedcv_z_new[["bestTune"]][["k"]])) 
knn_cv_stats_z$General<-round(calc_score(knn_cv_stats_z),3)
knn_cv_stats_z_new$General<-round(calc_score(knn_cv_stats_z_new),3)

```
```{r message=FALSE, warning=FALSE, echo=FALSE}
print(paste(paste0("CV: the whole z-scaled dataset's score using is ",knn_cv_stats_z$General),
            paste0("< the abridged dataset's score = ",knn_cv_stats_z_new$General)))
best_knn_stats["CV, scaled, selected",] = knn_cv_stats_z_new
```

Again we got better results out of the smaller dataset (accuracy from 84%0>86%, sensitivity 80%->84% and precision 87%->87.5%)

## knn conclusions

```{r message=FALSE,warning=FALSE, echo=FALSE, out.width='100%'}
best_knn_stats[,"method"]<-rownames(best_knn_stats)
best_knn_stats_melt<-best_knn_stats %>% 
  select(Accuracy,Sensitivity,Precision,Specificity,General,method) %>%
  pivot_longer(., cols = c(Accuracy,Sensitivity,Precision,Specificity), 
               names_to = "statistics", values_to = "percentage")
plotted_knn<-ggplot(best_knn_stats_melt,aes(x=statistics, y=percentage, fill=method)) +
  geom_bar(position = 'dodge', stat='identity')+ theme(legend.position="right")+ labs(title = "best knn statistics")

knitr::kable(best_knn_stats %>% select(Accuracy,Sensitivity, Precision, Specificity,General), caption = ' best KNN results')%>%
  kable_paper("striped", full_width = F) %>%
  column_spec(6, bold = T) %>%
  row_spec(4, bold = T, color = "black", background = "yellow")

best_knn<-best_knn_stats[best_knn_stats$General==max(best_knn_stats$General),]
best_stats["KNN",]<-cbind(best_knn%>% select(Accuracy,Sensitivity, Precision, Specificity,General), notes=paste(rownames(best_knn)))
```

We did not see any difference whether we used the crossed validation or the repeated cross validation.
We got worse results when we used the smaller dataset on the normalized data, but much better on the scaled data.
Interestingly, the best results were with the manually chosen K's (when K=3). 
using CV and repeated CV the best result was with K=17. We had manually chosen K=15 and K=21, and both were not as good as K=3.
Using KNN algorithm with K=3 on scaled data with only the selected featured gave us the best result yet.




#### confusion matrix of the best KNN model:
```{r message=FALSE, warning=FALSE, echo=FALSE}
grid.table(knn_table_3_z_new)
```

# Decision Tree (DT) algorithm

Decision Trees are a type of Supervised Machine Learning (that is you explain what the input is and what the corresponding output is in the training data) where the data is continuously split according to a certain parameter. The tree can be explained by two entities, namely decision nodes and leaves (cf https://www.xoriant.com/).

```{r message=FALSE, warning=FALSE}
DT_model <- C5.0(data_train[-63], data_train$Class)
```

The preceding text shows some simple facts about the tree, including the function
call that generated it, the number of features (labeled predictors), and examples
(labeled samples) used to grow the tree. Also listed is the tree size of 62, which
indicates that the tree is 62 decisions deep.

Next we looked at the summary of the model. 
```{r message=FALSE, warning=FALSE, results='hide'}
summary(DT_model)
```

The numbers in parentheses indicate the number of examples meeting the criteria for
that decision, and the number incorrectly classified by the decision. 
For instance, on the first line, 56/4 indicates that of the 56 examples reaching the decision, 4 were incorrectly classified as not likely to default. In other words, 4 applicants actually defaulted, in spite of the model's prediction.


```{r message=FALSE, warning=FALSE,results='hide'}
DT_pred <- predict(DT_model, data_test)
DT_pred_tbl<- confusionMatrix(DT_pred,data_test[,63])
```
```{r message=FALSE, warning=FALSE, echo=FALSE}
best_dt_stats <- data.frame( 
                          Accuracy=DT_pred_tbl[["overall"]][["Accuracy"]],
                           Sensitivity=round(DT_pred_tbl[["byClass"]][["Sensitivity"]],3), 
                           Precision=round(DT_pred_tbl[["byClass"]][["Precision"]],3), 
                           Specificity=round(DT_pred_tbl[["byClass"]][["Specificity"]],3)) 
rownames(best_dt_stats)<-c("DT default")

print.data.frame(best_dt_stats)
```

The performance here is somewhat worse than its performance on the
training data, but not unexpected, given that a model's performance is often worse
on unseen data. Also note that there are relatively many mistakes where the model predicted not a default, when in practice the loaner did default. 
Unfortunately, this type of error is a potentially costly mistake, as sick patients could be blind with time. 
We will try to improve the result.

### Adaptive Boosting

To improve our model we will use a C5.0 feature called adaptive boosting. This is a process in which many decision trees are built and the trees vote on the best class for each example. 

The C5.0() function makes it easy to add boosting to our C5.0 decision tree. We
simply need to add an additional trials parameter indicating the number of
separate decision trees to use in the boosted team. The trials parameter sets an
upper limit; the algorithm will stop adding trees if it recognizes that additional 
trials do not seem to be improving the accuracy. 
We started with 10 trials, a number that has become the de facto standard, as research 
suggests that this reduces error rates on test data by about 25 percent:

```{r message=FALSE, warning=FALSE, results='hide'}
DT_boost10 <- C5.0(data_train[-63], data_train$Class , trials = 10)
summary(DT_boost10)
```
```{r message=FALSE, warning=FALSE, echo=FALSE}
DT_boost_pred10 <- predict(DT_boost10, data_test)
DT_boost_pred10_tbl<-confusionMatrix(DT_boost_pred10, data_test[,63])
best_dt_stats["boosting with 10 trials",] <- data.frame(
                          Accuracy=DT_boost_pred10_tbl[["overall"]][["Accuracy"]],
                           Sensitivity=round(DT_boost_pred10_tbl[["byClass"]][["Sensitivity"]],3), 
                           Precision=round(DT_boost_pred10_tbl[["byClass"]][["Precision"]],3), 
                           Specificity=round(DT_boost_pred10_tbl[["byClass"]][["Specificity"]],3)) 
```

```{r message=FALSE, warning=FALSE,echo=FALSE}
print.data.frame(best_dt_stats["boosting with 10 trials",])
```
The classifier got an error rate of
0% percent. This is quite an improvement over the previous training error rate
before adding boosting! However, the model is still not doing well at predicting class of patients, which may be a result of our relatively small training dataset, or it may just be a very difficult problem to solve.

### fine-tune with cost-matrix

Next, we proceeded to fine-tune our algorithm, using a cost matrix. 
The C5.0 algorithm allows us to assign a penalty to different types of errors, in order to discourage a tree from making more costly mistakes. The penalties are designated in a cost matrix, which specifies how much costlier each error is, relative to any other prediction.

We created a default 2x2 matrix, and filled with our cost values. If we consider that the most important is to predict a sick patient as sick, much more that a normal patient as normal,
our penalty values could then be defined as:

```{r message=FALSE, warning=FALSE, echo=FALSE}
matrix_dimensions <- list(c("normal", "glaucoma"), c("normal", "glaucoma"))
names(matrix_dimensions) <- c("predicted", "actual")

error_cost <- matrix(c(0, 6,20, 0), nrow = 2, dimnames = matrix_dimensions)
print.table(error_cost)
```

We trained again to see if the cost matrix made any difference.

```{r message=FALSE, warning=FALSE}
dt_cost <- C5.0(data_train[-63], data_train$Class , costs = error_cost)
```
```{r message=FALSE, warning=FALSE,echo=FALSE, results='hide'}
dt_cost_pred <- predict(dt_cost, data_test) # predict on test data
dt_cost_pred_tbl<-confusionMatrix(dt_cost_pred, data_test[,63],dnn = c('predicted', 'actual'))

```
```{r message=FALSE, warning=FALSE, fig.height=6, fig.cap="Desition Tree", fig.width=16, echo=FALSE}
best_dt_stats["fine tuning with cost matrix",] <- data.frame(
                          Accuracy=dt_cost_pred_tbl[["overall"]][["Accuracy"]],
                           Sensitivity=round(dt_cost_pred_tbl[["byClass"]][["Sensitivity"]],3), 
                           Precision=round(dt_cost_pred_tbl[["byClass"]][["Precision"]],3), 
                           Specificity=round(dt_cost_pred_tbl[["byClass"]][["Specificity"]],3)) 
best_dt_stats$General <- calc_score(best_dt_stats)
plot(dt_cost)

```
 
 This last version makes more mistakes overall, but the types of mistakes are very different. Where the previous models incorrectly classified a small number of class correctly, our weighted model does much better in this regard. This trade resulted in a reduction of false negatives at the sick patient false positives may be acceptable if our cost estimates were accurate.
  
## DT conclusions

```{r message=FALSE, warning=FALSE, echo=FALSE}
knitr::kable(best_dt_stats , caption = ' best  DT stat ')%>%
  kable_paper("striped", full_width = F)  %>%
  column_spec(6, bold = T) %>%
  row_spec(1, bold = T, color = "black", background = "yellow")
```

Our best model was the first, default model, with sensitivity of 84% (like with the cost-matrix) and precision of 91.3% and specificity of 92%.

#### confusion matrix of the best DT model:
```{r message=FALSE, warning=FALSE, echo=FALSE}
grid.table(DT_pred_tbl[["table"]])
best_stats["DT",]<-cbind(best_dt_stats[1,], notes="default")
``` 






# Random forest (RF) algorithm

Random forest is a Supervised Machine Learning Algorithm that is used in Classification and Regression problems. It builds decision trees on different samples and takes their majority vote for classification and average in case of regression.
The Random Forest Algorithm can handle the data set containing continuous variables as in the case of regression and categorical variables as in the case of classification.(cf: https://www.analyticsvidhya.com/blog/2021/06/understanding-random-forest/)

## Random Forest Modeling

The random Forest algorithm uses many decision trees. For classification, which is what
we are focusing on, the final class result is based on the class that was picked from most trees. 

First we define our model with the parsnip package. Random forests have very little mandatory parameters. 
Here we only define the number of trees:

```{r rf_mod, message=FALSE, warning=FALSE}
rf_mod <-  rand_forest(trees = 1000) %>%  set_engine("ranger") %>% set_mode("classification") 
```

We want reproducible results, so we set.seed. Taking a look at the model:

```{r rf_fit, message=FALSE, warning=FALSE, results='hide'}
rf_fit <- rf_mod %>% fit(Class ~ ., data = data_train) # train model
```

We can see our model had 1000 trees like we specified, and several other metrics, such as the prediction error, 13% here.

### Estimating performance

Next we want to see if we can improve our model. We do this by changing some things about it. 
The changes can either be random (guessing) or more precise, depending on performance metrics. In our example, we will use the area under the Receiver Operating Characteristic (ROC) curve (which demonstrates the trade-off between the sensitivity and and specificity), and overall classification accuracy.

Using the yardstick package, let's calculate ROC and Accuracy. Notice that we are still only working with the data_train partition of our data:

```{r RF rf_training_pred, message=FALSE, warning=FALSE}
rf_training_pred <- predict(rf_fit, data_train) %>% 
  bind_cols(predict(rf_fit, data_train, type = "prob")) %>% 
  bind_cols(data_train %>%  select(Class))
```
```{r, message=FALSE, warning=FALSE, echo=FALSE}
rf_training_pred_roc_auc<-rf_training_pred %>%   roc_auc(truth = Class, .pred_glaucoma)# ROC calculation
rf_training_pred_acc<- rf_training_pred %>%  accuracy(truth = Class, .pred_class)# Accuracy calculation
print(paste("training: the roc_auc estimate = ", round( rf_training_pred_roc_auc$.estimate, 3),
      ", the accuracy estimate = ",round( rf_training_pred_acc$.estimate,3)))
```

As we can see, these are very good results. Almost too good, but it is on the set we trained on.
There are several reasons why training set statistics like the ones shown in this section can be unrealistically optimistic:

- Overfitting - Models like random forests, neural networks, and other black-box methods can essentially memorize the training set. Re-predicting that same set should always result in nearly perfect results.

- The training set does not have the capacity to be a good arbiter of performance. It is not an independent piece of information; predicting the training set can only reflect what the model already knows.

so we checked how the model performs on the test data:

```{r RF rf_testing_pred, message=FALSE, warning=FALSE, results='hide'}
rf_testing_pred <- 
  predict(rf_fit, data_test) %>% 
  bind_cols(predict(rf_fit, data_test, type = "prob")) %>% 
  bind_cols(data_test %>% select(Class))
```
```{r, message=FALSE, warning=FALSE, echo=FALSE}
rf_testing_pred_tbl<-confusionMatrix(rf_testing_pred$.pred_class, rf_testing_pred$Class,dnn = c('predicted', 'actual'))
rf_testing_pred_roc<-rf_testing_pred %>%   roc_auc(truth = Class, .pred_glaucoma)# ROC calculation
rf_testing_pred_acc<- rf_testing_pred %>%  accuracy(truth = Class, .pred_class)# Accuracy calculation
print(paste("testing: the roc_auc estimate = ", round( rf_testing_pred_roc$.estimate, 3),
      ", the accuracy estimate = ",round( rf_testing_pred_acc$.estimate,3)))

best_rf_stats <- data.frame(
                          Accuracy=rf_testing_pred_tbl[["overall"]][["Accuracy"]],
                           Sensitivity=round(rf_testing_pred_tbl[["byClass"]][["Sensitivity"]],3), 
                           Precision=round(rf_testing_pred_tbl[["byClass"]][["Precision"]],3), 
                           Specificity=round(rf_testing_pred_tbl[["byClass"]][["Specificity"]],3)) 
rownames(best_rf_stats)<-"RF default"

```
These validation results are lower than the ones we got on the training data, as expected, but are still pretty good.


## Resampling

Resampling methods, such as cross-validation and the bootstrap, are empirical simulation systems. They create a series of data sets similar to the training/testing split discussed previously; a subset of the data is used for creating the model and a different subset is used to measure performance. Resampling is always used with the training set. 

Here we'll use 10-fold cross-validation. This means that we'll create 10 "mini" datasets, or folds. We call the majority part of the folds (9 out of 10 in this case) the "analysis set" and the minority the "assessment set". We then train a model using the analysis set, and test it on the assessment set, effectively repeating the modeling process 10 times. This is how its done:

```{r RF folds, message=FALSE, warning=FALSE, results='hide'}
folds <- vfold_cv(data_train, v = 10) # create the folds
rf_wf <-   workflow() %>%  add_model(rf_mod) %>%  add_formula(Class ~ .)
rf_fit_rs <-   rf_wf %>%   fit_resamples(folds)# add folds to workflow and train model
```
```{r message=FALSE, warning=FALSE, echo=FALSE}
rf_fit_rs_tbl_stats<-collect_metrics(rf_fit_rs)
print(paste("the",rf_fit_rs_tbl_stats[1,1],"estimate =", round(rf_fit_rs_tbl_stats[1,]$mean,3), 
             ", the ",rf_fit_rs_tbl_stats[2,1],"estimate = ",round(rf_fit_rs_tbl_stats[2,]$mean,3)))
```
```{r message=FALSE, warning=FALSE, echo=FALSE,results='hide'}

rf_wf_fit <- rf_wf %>%  last_fit(patient_split) # Set workflow
rf_wf_fit_pred<- rf_wf_fit %>%  collect_predictions()  # collect metrics
rf_wf_fit_tbl<-confusionMatrix(rf_wf_fit_pred$.pred_class,
                                    rf_wf_fit_pred$Class,
                                    dnn = c('predicted', 'actual'))


```

We see these results are lower and look more realistic.

```{r warning=FALSE, message=FALSE, echo=FALSE}
best_rf_stats["10-fold CV",] <- data.frame(
                          Accuracy=rf_wf_fit_tbl[["overall"]][["Accuracy"]],
                           Sensitivity=round(rf_wf_fit_tbl[["byClass"]][["Sensitivity"]],3), 
                           Precision=round(rf_wf_fit_tbl[["byClass"]][["Precision"]],3), 
                           Specificity=round(rf_wf_fit_tbl[["byClass"]][["Specificity"]],3))
best_rf_stats["10-fold CV",]$General<-calc_score(best_rf_stats["10-fold CV",])
```


## Tuning hyperparameters

Another way to improve the performance of our models is by changing the parameters we provide them with. This is also called Tuning. Random Forests, as mentioned above, are not very sensitive to such parameters, but decision trees are. Lets see:

```{r Tuning tune_spec, message=FALSE, warning=FALSE, results='hide'}
# defining the parameters for the decision tree
tune_spec <-   decision_tree(cost_complexity = tune(), # to control the size of the tree
    tree_depth = tune()) %>% set_engine("rpart") %>%   set_mode("classification")
```

Note that in the above code, tune() is still just a placeholder. 
We will fill in values later on.

Next, we will create several smaller datasets, to try different parameters. 
The grid_regular command below will create 5 such datasets for each parameter combination, meaning 25 in total.

```{r Tuning tree_grid, message=FALSE, warning=FALSE, results='hide'}
tree_grid <- grid_regular(cost_complexity(), tree_depth(), levels = 5)
tree_grid %>%   count(tree_depth)
glaucoma_folds <- vfold_cv(data_train) # create the actual cross-validation folds
```
### Model tuning with a grid

```{r Tuning tree_wf,message=FALSE, warning=FALSE, results='hide'}
tree_wf <- workflow() %>%  add_model(tune_spec) %>%  add_formula(Class ~ .)
tree_res <- tree_wf %>%  tune_grid(resamples = glaucoma_folds, grid = tree_grid)
tree_res %>% collect_metrics()
```

Its easier to see how the models did with a graph:

```{r RF plot preformance, message=FALSE, warning=FALSE, echo=FALSE, results='hide', fig.height=2, fig.cap='Model tuning with a grid evaluation'}
tree_res %>%
  collect_metrics() %>%
  mutate(tree_depth = factor(tree_depth)) %>%
  ggplot(aes(cost_complexity, mean, color = tree_depth)) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free", nrow = 1) +
  theme(legend.position = "top")+
  scale_x_log10(labels = scales::label_number()) +
  scale_color_viridis_d(option = "plasma", begin = .9, end = 0)
```

We can see that our tree with a depth of 1, is the worst model according to both metrics and across all candidate values of cost_complexity. Our deepest tree, with a depth of 15, did better.  The show_best() function shows us the top 5 candidate models by default:

```{r message=FALSE, warning=FALSE, echo=FALSE, results='hide'}
print.data.frame(tree_res %>%  show_best("roc_auc"))

```

And finally, let's finalize the workflow with the best tree. 

```{r RF best_tree final_wf, message=FALSE, warning=FALSE, results='hide'}
best_tree <- tree_res %>%  select_best("roc_auc") # Selecting the best tree
final_wf <-   tree_wf %>%   finalize_workflow(best_tree) # Finalize workflow
print(final_wf)
```
```{r message=FALSE, warning=FALSE,echo=FALSE}
print.data.frame(best_tree)
```

So we have our best tree parameters. Lets take a better look:

```{r  Exploring results final_tree, message=FALSE, warning=FALSE,results='hide', fig.height=2}
# Exploring results
final_tree <-   final_wf %>%  fit(data = data_train) 
print(final_tree)

# vip package: another way to look at how the model uses the different features
final_tree %>%   extract_fit_parsnip() %>%   vip()
```

## Random forest conclusions and Finalizing the model


Finally, let’s return to our test data and estimate the model performance we expect to see with new data. We can use the function last_fit() with our finalized model; this function fits the finalized model on the full training data set and evaluates the finalized model on the testing data.

```{r Random forest conclusions, message=FALSE, warning=FALSE,results='hide'}
final_fit <- final_wf %>%  last_fit(patient_split) # Set workflow
final_fit %>%  collect_metrics()# train
final_fit_tbl_stats<-collect_metrics(final_fit)
```
```{r message=FALSE, warning=FALSE, echo=FALSE}
print(paste("the",final_fit_tbl_stats[1,]$.metric,"estimate =", round(final_fit_tbl_stats[1,]$.estimate,3), 
             ", the ",final_fit_tbl_stats[2,]$.metric,"estimate = ",round(final_fit_tbl_stats[2,]$.estimate,3)))
final_fit_pred<- final_fit %>%  collect_predictions()  # collect metrics
final_fit_pred_tbl<-confusionMatrix(final_fit_pred$.pred_class, final_fit_pred$Class,dnn = c('predicted', 'actual'))
best_rf_stats["final_fit",] <- data.frame(
                          Accuracy=final_fit_pred_tbl[["overall"]][["Accuracy"]],
                           Sensitivity=round(final_fit_pred_tbl[["byClass"]][["Sensitivity"]],3), 
                           Precision=round(final_fit_pred_tbl[["byClass"]][["Precision"]],3), 
                           Specificity=round(final_fit_pred_tbl[["byClass"]][["Specificity"]],3)) 
best_rf_stats$General <-calc_score(best_rf_stats)
knitr::kable(best_rf_stats , caption = ' best RF stat results')%>%
  kable_paper("striped", full_width = F)  %>%
  column_spec(6, bold = T) %>%
  row_spec(3, bold = T, color = "black", background = "yellow")

```

#### confusion matrix of the best RF model:
```{r message=FALSE, warning=FALSE, echo=FALSE}
grid.table(final_fit_pred_tbl[["table"]])
best_stats["RF",]<-cbind(best_rf_stats["final_fit",], notes="final fit")

```

```{r echo=FALSE, fig.height=3}
#final_fit_pred %>%  roc_curve(Class, .pred_glaucoma) %>%   autoplot()
```
We observed in the ROC curve that in our true positive prediction we have very few false positive predictions: more than 75% of chances that the model distinguish sick patients to healthy patients.

Random forest classifies patients with more sensitivity than decision tree. This result is not surprising by the fact that random forest build many decision trees and select the best.

# conclusions

To summarize, our dataset is composed of 196 patients:98 sicks and 98 healthy.
There are 63 features corresponding to the blood vessels eye pressure which help to determine if a patient has glaucoma. Each feature has its degree of importance, some of them seem to be more important than others. Keeping only important features (based on threshold determine by varImp), we did not observe any improvement in the patient classification. Moreover, we observed that some features highly correlate. Unfortunately, after deleting highly correlated features, we did not observe a major improvement in the patient classification. Thereby, we decided to keep all 63 features. We did try in the SVM and KNN algorithms also to use the selected features. We saw no improvement in the SVM model, though our best result was with the KNN algorithm using the selected features only.
Before we tested the different algorithms, we defined a weight for each variable allowing to evaluate the predictions: 4 for sensitivity, 2 for precision,  2 for accuracy and 1 for specificity, in order to classify results of the algorithm used. 


```{r general conclusion and extra, echo=FALSE, message=FALSE, warning=FALSE, out.width='100%', fig.cap= "all statistics", fig.width=15, fig.height=5}
pROC_svm_rbf <- roc(df_rbf$labels,df_rbf$predictions)
pROC_knn_3_z_new <- roc(c(test_set_z$Class), c(as.numeric(pred_knn_3_z_new)))
pROC_DT <- roc(c(test_set_z$Class), c(as.numeric(DT_pred)))
pROC_RF_final <- roc(c(test_set_z$Class), c(as.numeric(final_fit_pred$.pred_class)))

roc_results<- ggroc(list(DT=pROC_DT, KNN=pROC_knn_3_z_new,  RF=pROC_RF_final,SVM=pROC_svm_rbf))+labs(title = "ROC: all algotithms")

#print statistics
best_stats_melt<-cbind(best_stats, method=rownames(best_stats)) %>% 
  select(method, Accuracy,Sensitivity,Precision,Specificity, General) %>%
  pivot_longer(., cols = c(Accuracy,Precision,General), 
               names_to = "statistics", values_to = "percentage")

best_stats_plot<-ggplot(best_stats_melt,aes(x = statistics, y = percentage, fill = method)) + geom_bar(position = 'dodge', stat='identity')+ labs(title = "statistics: all algotithms") + theme(legend.position="right")

all_statistics<-arrangeGrob( best_stats_plot, roc_results+ theme(legend.position="none"), nrow =  1, widths=c(2,1))

 t1 <- ttheme_default(core=list(
        fg_params=list(fontface=c("plain","bold",rep("plain", 2), "bold.italic")),
        bg_params = list(fill=c(blues9[1],"yellow",blues9[3:4]),
                         alpha = rep(c(1,0.5), each=5))
        ),
        colhead=list(fg_params=list(col="navyblue", fontface="bold.italic")),
        rowhead=list(fg_params=list(col="navyblue", fontface="bold")), base_size = 10)
grid.arrange(tableGrob(best_stats, theme = t1), all_statistics)
```

The last figure shows all statistics according to algorithms (showing the best result of each algorithm) on the left, and a ROC plot (sensitivity by specificity) on the right. 
We observe that KNN algorithm gives the best sensitivity score (0.92), so the highest rate of true positive cases, which is the most important variable.About the precision, (true predicted glaucoma/predicted glaucoma), type I error, KNN algorithm gives the better score (0.92) followed by DT algorithm (0.913). About the accuracy (true prediction/ all prediction), proportion of correct prediction, KNN gives the best score again (0.92) followed by DT algorithm (0.88). Finally, about the specificity, (true predicted healthy/healthy), KNN and DT algorithm give the same score (0.92). Combining all together, KNN algorithm gives the best predictions followed by decision tree algorithm. SVM and random forest give similar score of predictions, lower than KNN and decision tree.

The upper table is the statistic results of all algorithms (in numbers). We can see that the SVM ans the RF yielded the same result, which was the lowest. Nest we have the DT, which has the same sensitivity, a bit higher accuracy, and much higher precision and specificity. The specificity (92%) was highest in both the DT and the KNN, and in all aspects the highest result we got was when using the KNN algorithm on scaled, selected (without highly correlated data) data. 
