---
title: "Final Project"
author: "Lilach Herzog & Leslie Cohen"
date: "13 9 2022"
output:
  pdf_document:
    fig_caption: yes
  word_document:
    toc: yes
    toc_depth: 2
editor_options:
  chunk_output_Type: inline
---
# preproccessing

## Upload data and libraries

```{r, message=FALSE, warning=FALSE, echo=FALSE}
library(tidymodels) # for the sampling and splitting the data
library(class)

library(corrplot) # for eda


library(tidyverse)    # data manipulation and visualization
library(ggplot2)
library(ggfortify) #autoplot
library(gridExtra) # grid.arrange
library(grid) # textGrob
library(gtable) # gtable_add_rows+gtable_add_grob

library(caret) # confusionMatrix
library(gmodels) # CrossTable

library(caret) # Classification And REgression Training: contains functions to streamline the model training process for complex regression and classification problems.
library(pROC) # roc

library(kernlab)      # SVM: ksvm
library(e1071)        # SVM methodology

library(C50) # DT
library(modeldata)  # for the cells data
library(vip)        # for variable importance plots
library(ranger)

```


```{r message=FALSE, warning=FALSE}

glaucoma <- read.csv("GlaucomaM.csv")# read data
```

We have data from 196 patients, with 63 features. We want to classify our data according to the 'Class' column, which contains the information about whether all the information about a person (represented in a row) belongs to a healthy (normal) or sick (glaucoma) patient, divided neatly in half. Before starting on teaching the algorithm, we converted the 'Class' column to a factor.
```{r, message=FALSE, warning=FALSE, results='hide'}
glaucoma$Class<-as.factor(glaucoma$Class)
```
Since the whole table is already shuffled according to the 'Class' column, we don't need to shuffle the whole table again
In order to get consistent results we initialized a pseudo random number generator. We also made sure to shuffle our data.
```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
set.seed(123)
glaucoma <- glaucoma[sample(nrow(glaucoma)),]
```
## split into training and test sets
### Splitting
We split the whole data set into 80% training set and 20% test set.
```{r splitting, message=FALSE, warning=FALSE}
index <- sample(nrow(glaucoma),round(nrow(glaucoma)*.8),replace = FALSE) #80% of dataset for training
train_set <- glaucoma[index,]
test_set<- glaucoma[-index,]
train_set_labels <-train_set["Class"] #save true "Classes"of the training set
test_set_labels <-test_set["Class"] #save true "Classes"of the test set
```
Since we know that there are about a half of each Class in the data set, we wanted to be sure that the distribution is as we defined (about 1/2 of each Class in both training and test sets should be 'normal' and 1/2 'glaucoma').

```{r, message=FALSE, warning=FALSE, echo=FALSE}
train_div<-prop.table(table(train_set$Class))
testdiv<-prop.table(table(test_set$Class))
print.data.frame(data.frame(set=c("train","test"),glaucoma=c(train_div[1],testdiv[1]), normal=c(train_div[2],testdiv[2])))
```
The training set is what we will use to train the algorithm, to help it learn our data, and then we will run the algorithm on 20% of the data (test set), and predict according to what it learned in the training set, and since it is still our data we have the true class each individual belongs to, so we can evaluate how well the algorithm succeeded in predicting.

We then tried instead of splitting it out own to try the 'initial_split' function, with the buikt in 'training' and 'testing' functions
```{r better splitting, message=FALSE, warning=FALSE, echo=FALSE}
patient_split <- initial_split(glaucoma, strata = Class )
data_train <- training(patient_split) # we used the default parameters of splitting size
data_test <- testing(patient_split)
train_div<-prop.table(table(data_train$Class))
testdiv<-prop.table(table(data_test$Class))
print.data.frame(data.frame(set=c("train","test"),glaucoma=c(train_div[1],testdiv[1]), normal=c(train_div[2],testdiv[2])))

```
instead of 0.5159236/ 0.4840764 partition of glaucoma/ normal in the training set and 0.4358974/0.5641026 in the test set, we got exactly 0.5 in both sets.

Since in most algorithms it is important for the data to be scaled, we decided to start by normalizing it.

### normalization
We decided to normalize the data, thinking that the range of values of each parameter is similar in order to compare the distances of different features with different scales.
```{r normalization, results='hide', message=FALSE, warning=FALSE}
normalize <- function(x) {return ((x - min(x)) / (max(x) - min(x)))}
normalize(c(1, 2, 3, 4, 5)) # test our function 1
normalize(c(10, 20, 30, 40, 50)) # test our function 2
train_set_norm <- as.data.frame(lapply(data_train[,-63], normalize))
train_set_norm$Class <- data_train$Class
test_set_norm <- as.data.frame(lapply(data_test[,-63], normalize))
test_set_norm$Class <- data_test$Class
summary(train_set_norm$ag) # test our dataset
```

### z-score standardization 
We also tried with z score scaling instead of normalization.
```{r z-score standardization, message=FALSE, warning=FALSE, results='hide'}
train_set_z <- as.data.frame(scale(data_train[,-63]))
test_set_z <- as.data.frame(scale(data_test[,-63]))
train_set_z$Class <- data_train$Class
test_set_z$Class <- data_test$Class
```

# EDA analysis

Results obtained with corrplot: Positive correlations are displayed in a
blue scale while negative correlations are displayed in a red scale. we
can observe 8 clusters of features that highly correlate. Results obtained with heatmap: we
can observe one big cluster that could be divided in 2 and 2 others
clusters of features.

We would like to observe if we can  cluster patients healthy or with glaucoma with all the features.
We performed first a PCA on all features and values.

```{r, message=FALSE, warning=FALSE, results='hide', echo=FALSE}

# Preform PCA on our data

gl.pca <- prcomp(glaucoma[,1:62],
                   center = TRUE,
                   scale. = TRUE)
  
# summary of the 
# prcomp object
summary(gl.pca)
gl_pca_plot <- autoplot(gl.pca,
                          data = glaucoma,
                          colour = 'Class')
  

```
```{r, message=FALSE, warning=FALSE, echo=FALSE}
gl_pca_plot
```

In the PCA, we can observe 2 clusters of patients: glaucoma patients and normal
patients. Unfortunately, some of them were not clustered correctly (mixed clouds of blue and red points) in the same area. This mixture could be, probably to the fact that many features correlate, so our next course of action was to check for any correlation between features to determine which feature are correlated by using pearson correlation

```{r, message=FALSE, warning=FALSE, results='hide'}
#calculation of correlation, using pearson correlation
gl.cor = cor(glaucoma[,c(1:62)], method = c("pearson"))

#corrplot(gl.cor)

palette = colorRampPalette(c("green", "white", "red")) (20)
heatmap(x = gl.cor, col = palette, symm = TRUE)

```

```{r, message=FALSE, warning=FALSE, results='hide'}
library("Hmisc")
gl.rcorr = rcorr(as.matrix(glaucoma[,c(1:62)]))
gl.rcorr
gl.coeff = gl.rcorr$r

cor_matrix_rm <- gl.cor                  # Modify correlation matrix
cor_matrix_rm[upper.tri(cor_matrix_rm)] <- 0
diag(cor_matrix_rm) <- 0
cor_matrix_rm

data<-glaucoma[,c(1:62)]
data_new <- data[ , !apply(cor_matrix_rm,    # Remove highly correlated variables
                           2,
                           function(x) any(x > 0.8))]
head(data_new)

colSums(glaucoma[,c(1:62)] != 0)

```
```{r, message=FALSE, warning=FALSE, echo=FALSE}
gl.cor2 = cor(data_new, method = c("pearson"))
#corrplot(gl.cor2)
heatmap(x = gl.cor2, col = palette, symm = TRUE)


```

```{r, message=FALSE, warning=FALSE, echo=FALSE}

########
# Preform PCA on our new data

gl.pca2 <- prcomp(data_new,
                   center = TRUE,
                   scale. = TRUE)#play with options
  
```
```{r, message=FALSE, warning=FALSE, results='hide'}
# summary of the 
# prcomp object
summary(gl.pca2)
```
```{r, message=FALSE, warning=FALSE, echo=FALSE}
gl_pca_plot2 <- autoplot(gl.pca2,
                          data = glaucoma,
                          colour = 'Class')
gl_pca_plot2
#gl_pca_plot
```


# Support Vector Machines (SVM)
Support vectors are the data points that define the position and the margin of the hyperplane. They are called “support” vectors, because these are the representative data points of the classes. If we move one of them, the position and/or the margin will change.
Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that assigns new examples into that same space and predicts which category they belong to based on which side of the gap they fall. The SVM algorithm maps training examples to points in space so as to maximize the width of the gap between the two categories.
The effectiveness of SVM depends a lot on the selection of kernel. We tried using 4 different kernels and compared their results.

## different kernels
### default
```{r SVM default, message=FALSE, warning=FALSE, echo=FALSE}
# linear
Class_classifier_svm <-  ksvm(x=Class ~., data=data_train)
Class_predictions_svm <- predict(Class_classifier_svm, data_test)
svm_linear_norm <- confusionMatrix(Class_predictions_svm,data_test[,63])
pROC_svm <- roc(c(data_test$Class), c(as.numeric(Class_predictions_svm)))
svm_stats_default <- data.frame(method="default", 
                           acc=svm_linear_norm[["overall"]][["Accuracy"]],
                           sens=svm_linear_norm[["byClass"]] [["Sensitivity"]], 
                           prec=round(svm_linear_norm[["byClass"]] [["Precision"]],3), 
                           spec=round(svm_linear_norm[["byClass"]] [["Specificity"]],3))
svm_stats<-svm_stats_default
```

Since it is important when using SVM to work only on numeric and scaled data, we made sure our data was numeric (all columns originally numeric, and the ‘Class’ column we factorizes), randomized (we used the ‘sample’ function beforehand) and scaled. 
The package we used (kernlab) let us scale the data when running the algorithm itself, so we'll start by scaling with the algorithm. We built the model:
```{r linear SVM train, message=FALSE, warning=FALSE, results='hide'}
train_svm_linear<-train(Class ~ ., data_train,
                       method = "svmLinear",
                       trControl = trainControl(method="repeatedcv", number = 10, repeats = 10),
                       metric = "Kappa",  
                       preProcess = c("center","scale"))

svm_predictions_linear<-predict(train_svm_linear, test_set_norm)
Class_classifier_linear <-  ksvm(x=Class ~., data=data_train,kernel = "vanilladot")
```

We can see that the model used 49 support vectors, and that the training error was 0.054795. SVM can be a bit of a black box so we don't know much else.
After building the model on our training set (80% of the data) we ran the model on the test set:
```{r Class_predictions_linear, message=FALSE, warning=FALSE}
Class_predictions_linear <- predict(Class_classifier_linear, data_test[,-63])
```
We used the table() function to compare the predicted Class to the true Class in the testing:
```{r svm_linear confusionMatrix, message=FALSE, warning=FALSE,results='hide'}
table(Class_predictions_linear, data_test$Class)
svm_linear <- confusionMatrix(Class_predictions_linear,data_test[,63])
```


Now we wanted to see how well our classifier performed according to the different ways of measuring success (precision, recall, accuracy ans AUC):

```{r svm_linear_stats, message=FALSE, warning=FALSE, echo=FALSE}
df_linear<-data.frame(predictions=c(as.numeric(Class_predictions_linear)),labels = c(data_test$Class))
pROC_svm_linear <- roc(df_linear$labels,df_linear$predictions)

svm_linear_stats <- data.frame(method="linear", 
                           acc=svm_linear[["overall"]][["Accuracy"]],
                           AUC=pROC_svm_linear[["auc"]],
                           sens=svm_linear[["byClass"]] [["Sensitivity"]], 
                           prec=round(svm_linear[["byClass"]] [["Precision"]],3), 
                           spec=round(svm_linear[["byClass"]] [["Specificity"]],3))

svm_stats["linear",]<-svm_linear_stats
print.data.frame(svm_stats)
```

The accuracy is  0.846 and the AUC is 0.857. These are pretty good results. We want to make them even better, but what's really important for us is to minimize the false negatives- type II error- since we are working with information about patients, a result telling a person that might be sick that he is healthy can be much more problematic than a result telling a healthy person that he might be sick, and our recall is 0.944.


A standard Gaussian Radial basis function (RBF) kernel is the second kernel we tried. We used the ksvm() function here. 

```{r svm_rbf, message=FALSE, warning=FALSE}
# Gaussian Radial basis function (RBF) kernel
Class_classifier_rbf <-  ksvm(x=Class ~., data=data_train,kernel = "rbfdot")
```
```{r svm rbf, message=FALSE, warning=FALSE, echo=FALSE}
Class_predictions_rbf <- predict(Class_classifier_rbf, data_test[,-63])
svm_rbf <- confusionMatrix(Class_predictions_rbf,data_test[,63])


# calculate performance
df_rbf<-data.frame(predictions=c(as.numeric(Class_predictions_rbf)),labels = c(data_test$Class))
pROC_svm_rbf <- roc(df_rbf$labels,df_rbf$predictions)
svm_rbf_stats <- data.frame(method="RBF", 
                           acc=svm_rbf[["overall"]][["Accuracy"]],
                           AUC=pROC_svm_rbf[["auc"]],
                           sens=svm_rbf[["byClass"]] [["Sensitivity"]], 
                           prec=round(svm_rbf[["byClass"]] [["Precision"]],3), 
                           spec=round(svm_rbf[["byClass"]][["Specificity"]],3))
print.data.frame(svm_rbf_stats)
svm_stats["RBF",]=svm_rbf_stats

```

```{r svm_poly, message=FALSE, warning=FALSE}
# Polynomial kernel
Class_classifier_poly <-  ksvm(x=Class ~., data=data_train,kernel = "polydot")
```
```{r SVM Polynomial, message=FALSE, warning=FALSE, echo=FALSE}
Class_predictions_poly <- predict(Class_classifier_poly, data_test[,-63])
svm_poly <- confusionMatrix(Class_predictions_poly,data_test[,63])

# calculate performance
df_poly<-data.frame(predictions=c(as.numeric(Class_predictions_poly)),labels = c(data_test$Class))
pROC_svm_poly <- roc(df_poly$labels,df_poly$predictions)
svm_poly_stats <- data.frame(method="Polynomial", 
                           acc=svm_poly[["overall"]][["Accuracy"]],
                           AUC=pROC_svm_poly[["auc"]],
                           sens=svm_poly[["byClass"]] [["Sensitivity"]], 
                           prec=round(svm_poly[["byClass"]] [["Precision"]],3), 
                           spec=round(svm_poly[["byClass"]][["Specificity"]],3))
print.data.frame(svm_poly_stats)
svm_stats["Polynomial",]=svm_poly_stats 

```

```{r SVM Hyperbolic tangent, message=FALSE, warning=FALSE}
# Hyperbolic tangent kernel
Class_classifier_tan <- ksvm(x=Class ~., data=data_train,kernel = "tanhdot")
```
```{r SVM tan, message=FALSE, warning=FALSE, echo=FALSE, results='hide'}
Class_predictions_tan <- predict(Class_classifier_tan, data_test[,-63])
svm_tan <- confusionMatrix(Class_predictions_tan,data_test[,63])

# calculate performance
df_tan<-data.frame(predictions=c(as.numeric(Class_predictions_tan)),labels = c(data_test$Class))
pROC_svm_tan <- roc(df_tan$labels,df_tan$predictions)
svm_tan_stats <- data.frame(method="Hyperbolic", 
                           acc=svm_tan[["overall"]][["Accuracy"]],
                           AUC=pROC_svm_tan[["auc"]],
                           sens=svm_tan[["byClass"]] [["Sensitivity"]], 
                           prec=round(svm_tan[["byClass"]] [["Precision"]],3), 
                           spec=round(svm_tan[["byClass"]][["Specificity"]],3))
svm_stats["Hyperbolic",]=svm_tan_stats

#print statistics
best_svm_stats_melt<-svm_stats %>% 
  select( method,acc,sens,prec,spec) %>%
  pivot_longer(., cols = c(acc,sens,prec,spec), 
               names_to = "statistics", values_to = "percentage")

plotted_svm<-ggplot(best_svm_stats_melt,aes(x = statistics, y = percentage, fill = method)) + labs(title = "svm statistics")+ geom_bar(width = 0.4,position = 'dodge', stat='identity') + theme(legend.text = element_text(size = 6),legend.position = "bottom", legend.title = element_text(face = "bold", size=6))

#grid.arrange(tableGrob(svm_stats[,-1]), plotted_svm, ncol=2, nrow=1)
```
```{r message=FALSE, warning=FALSE}
print.data.frame(svm_stats["linear",])
```
We can see a slight decrease in accuracy/AUC (from 88% to 86%), sensitivity (from 84% to 80%) and precision (from 91.3% to 90.9%). In both cases the specificity is 92% which is good.
Other kernels might prove even better, or the cost of constraints parameter C could be varied to modify the width of the decision boundary.

```{r svm_best_stats_1, message=FALSE, warning=FALSE, echo=FALSE}
#We saved the best scores
svm_best_stats<-svm_stats["linear",]
rownames(svm_best_stats)<-c("linear")
```

## pre-normalized data
### normalized data
We tried using the data that we normalized and compared the 2.
```{r svm normalized linear, message=FALSE, warning=FALSE, results='hide'}
# linear
Class_classifier_linear_norm <-  ksvm(x=Class ~., data=train_set_norm,kernel = "vanilladot", scaled=FALSE)
Class_predictions_linear_norm <- predict(Class_classifier_linear_norm, test_set_norm)
svm_linear_norm <- confusionMatrix(Class_predictions_linear_norm,data_test[,63])
pROC_svm_linear_norm <- roc(c(data_test$Class), c(as.numeric(Class_predictions_linear_norm)))
svm_linear_norm_stats <- data.frame(method="linear", 
                           acc=svm_linear_norm[["overall"]][["Accuracy"]],
                           AUC=pROC_svm_linear_norm[["auc"]],
                           sens=svm_linear_norm[["byClass"]][["Sensitivity"]], 
                           prec=round(svm_linear_norm[["byClass"]][["Precision"]],3), 
                           spec=round(svm_linear_norm[["byClass"]][["Specificity"]],3))
svm_norm_stats<-svm_linear_norm_stats
rownames(svm_norm_stats)<-c("linear")
```
As before we ran the same kerneks, but show here the process of the linear kernel only (the rest can be seen in our code).
```{r svm normalized data, message=FALSE, warning=FALSE, echo=FALSE, results='hide'}


# rbf
Class_classifier_rbf_norm <-  ksvm(x=Class ~., data=train_set_norm,kernel = "rbfdot", scaled=FALSE)
Class_predictions_rbf_norm <- predict(Class_classifier_rbf_norm, test_set_norm)
svm_rbf_norm <- confusionMatrix(Class_predictions_rbf_norm,data_test[,63])
pROC_svm_rbf_norm <- roc(c(data_test$Class), c(as.numeric(Class_predictions_rbf_norm)))
svm_rbf_norm_stats <- data.frame(method="rbf", 
                           acc=svm_rbf_norm[["overall"]][["Accuracy"]],
                           AUC=pROC_svm_rbf_norm[["auc"]],
                           sens=svm_rbf_norm[["byClass"]] [["Sensitivity"]], 
                           prec=round(svm_rbf_norm[["byClass"]][["Precision"]],3), 
                        spec=round(svm_rbf_norm[["byClass"]][["Specificity"]],3))
svm_norm_stats["rbf",]<-svm_rbf_norm_stats


# polynomial
Class_classifier_poly_norm <-  ksvm(x=Class ~., data=train_set_norm,kernel = "polydot", scaled=FALSE)
Class_predictions_poly_norm <- predict(Class_classifier_poly_norm, test_set_norm)
svm_poly_norm <- confusionMatrix(Class_predictions_poly_norm,data_test[,63])
pROC_svm_poly_norm <- roc(c(data_test$Class), c(as.numeric(Class_predictions_poly_norm)))
svm_poly_norm_stats <- data.frame(method="poly", 
                           acc=svm_poly_norm[["overall"]][["Accuracy"]],
                           AUC=pROC_svm_poly_norm[["auc"]],
                           sens=svm_poly_norm[["byClass"]] [["Sensitivity"]], 
                           prec=round(svm_poly_norm[["byClass"]][["Precision"]],3), 
                           spec=round(svm_poly_norm[["byClass"]][["Specificity"]],3))
svm_norm_stats["poly",]<-svm_poly_norm_stats

# Hyperbolic tangent
Class_classifier_tan_norm <-  ksvm(x=Class ~., data=train_set_norm,kernel = "tanhdot", scaled=FALSE)
Class_predictions_tan_norm <- predict(Class_classifier_tan_norm, test_set_norm)
svm_tan_norm <- confusionMatrix(Class_predictions_tan_norm,data_test[,63])
pROC_svm_tan_norm <- roc(c(data_test$Class), c(as.numeric(Class_predictions_tan_norm)))
svm_tan_norm_stats <- data.frame(method="Hyperbolic", 
                           acc=svm_tan_norm[["overall"]][["Accuracy"]],
                           AUC=pROC_svm_tan_norm[["auc"]],
                           sens=svm_tan_norm[["byClass"]] [["Sensitivity"]], 
                           prec=round(svm_tan_norm[["byClass"]] [["Precision"]],3), 
                           spec=round(svm_tan_norm[["byClass"]][["Specificity"]],3))
svm_norm_stats["tan",]<-svm_tan_norm_stats
```
```{r svm normalized data print, message=FALSE, warning=FALSE, echo=FALSE,results='hide'}
#print statistics
best_svm_norm_stats_melt<-svm_norm_stats %>% 
  select( method,acc,sens,prec,spec) %>%
  pivot_longer(., cols = c(acc,sens,prec,spec), 
               names_to = "statistics", values_to = "percentage")

plotted_svm_norm<-ggplot(best_svm_norm_stats_melt,aes(x = statistics, y = percentage, fill = method)) + geom_bar(position = 'dodge', stat='identity')+ labs(title = "normalized data- svm statistics")+ theme(legend.position="top")

#grid.arrange(plotted_svm_norm, tableGrob(svm_norm_stats[,-1]), nrow = 2, ncol = 1)
```
```{r message=FALSE, warning=FALSE, echo=FALSE}
print.data.frame(svm_norm_stats["linear",])
#Add the best scores of the normalized
svm_best_stats["linear norm",]<-svm_norm_stats["linear",]
```

### z-score scaled
We tried scaling with z-score as well. The rest of the code is exactly as before so we did not print it in the report.
```{r  message=FALSE, warning=FALSE, results='hide'}
# linear
Class_classifier_linear_z <-  ksvm(x=Class ~., data=train_set_z,kernel = "vanilladot", scaled=FALSE)
```
```{r svm scaled data linear, message=FALSE, warning=FALSE, results='hide',echo=FALSE}
Class_predictions_linear_z <- predict(Class_classifier_linear_norm, test_set_z)
svm_linear_z <- confusionMatrix(Class_predictions_linear_z,data_test[,63])
pROC_svm_linear_z <- roc(c(data_test$Class), c(as.numeric(Class_predictions_linear_z)))
svm_linear_z_stats <- data.frame(method="linear", 
                           acc=svm_linear_z[["overall"]][["Accuracy"]],
                           AUC=pROC_svm_linear_z[["auc"]],
                           sens=svm_linear_z[["byClass"]] [["Sensitivity"]], 
                           prec=round(svm_linear_z[["byClass"]][["Precision"]],3), 
                           spec=round(svm_linear_z[["byClass"]][["Specificity"]],3))
```
As before we ran the same kerneks, but show here the process of the linear kernel only (the rest can be seen in our code).
```{r svm scaled data, message=FALSE, warning=FALSE, echo=FALSE, results='hide'}
svm_z_stats<-svm_linear_z_stats
rownames(svm_z_stats)<-c("linear")

# rbf
Class_classifier_rbf_z <-  ksvm(x=Class ~., data=train_set_z,kernel = "rbfdot", scaled=FALSE)
Class_predictions_rbf_z <- predict(Class_classifier_rbf_norm, test_set_z)
svm_rbf_z <- confusionMatrix(Class_predictions_rbf_z,data_test[,63])
pROC_svm_rbf_z <- roc(c(data_test$Class), c(as.numeric(Class_predictions_rbf_z)))
svm_rbf_z_stats <- data.frame(method="rbf", 
                           acc=svm_rbf_z[["overall"]][["Accuracy"]],
                           AUC=pROC_svm_rbf_z[["auc"]],
                           sens=svm_rbf_z[["byClass"]] [["Sensitivity"]], 
                           prec=round(svm_rbf_z[["byClass"]] [["Precision"]],3), 
                           spec=round(svm_rbf_z[["byClass"]] [["Specificity"]],3))
svm_z_stats["rbf",]<-svm_rbf_z_stats


# polynomial
Class_classifier_poly_z <-  ksvm(x=Class ~., data=train_set_z,kernel = "polydot", scaled=FALSE)
Class_predictions_poly_z <- predict(Class_classifier_poly_z, test_set_z)
svm_poly_z <- confusionMatrix(Class_predictions_poly_z,data_test[,63])
pROC_svm_poly_z <- roc(c(data_test$Class), c(as.numeric(Class_predictions_poly_z)))
svm_poly_z_stats <- data.frame(method="poly", 
                           acc=svm_poly_z[["overall"]][["Accuracy"]],
                           AUC=pROC_svm_poly_z[["auc"]],
                           sens=svm_poly_z[["byClass"]] [["Sensitivity"]], 
                           prec=round(svm_poly_z[["byClass"]][["Precision"]],3), 
                           spec=round(svm_poly_z[["byClass"]][["Specificity"]],3))
svm_z_stats["poly",]<-svm_poly_z_stats

# Hyperbolic tangent
Class_classifier_tan_z <-  ksvm(x=Class ~., data=train_set_z,kernel = "tanhdot", scaled=FALSE)
Class_predictions_tan_z <- predict(Class_classifier_tan_z, test_set_z)
svm_tan_z <- confusionMatrix(Class_predictions_tan_z,data_test[,63])
pROC_svm_tan_z <- roc(c(data_test$Class), c(as.numeric(Class_predictions_tan_z)))
svm_tan_z_stats <- data.frame(method="tan", 
                           acc=svm_tan_z[["overall"]][["Accuracy"]],
                           AUC=pROC_svm_tan_z[["auc"]],
                           sens=svm_tan_z[["byClass"]] [["Sensitivity"]], 
                           prec=round(svm_tan_z[["byClass"]] [["Precision"]],3), 
                           spec=round(svm_tan_z[["byClass"]][["Specificity"]],3))
svm_z_stats["tan",]<-svm_tan_z_stats

#print statistics
best_svm_z_stats_melt<-svm_z_stats %>% 
  select( method,acc,sens,prec,spec) %>%
  pivot_longer(., cols = c(acc,sens,prec,spec), 
               names_to = "statistics", values_to = "percentage")

plotted_svm_z<-ggplot(best_svm_z_stats_melt,aes(x = statistics, y = percentage, fill = method)) + geom_bar(position = 'dodge', stat='identity')+ labs(title = "z scaled data- svm statistics")+ theme(legend.position="top")

#grid.arrange( plotted_svm_z, tableGrob(svm_z_stats[,-1]),nrow = 2, ncol = 1)

```
```{r message=FALSE,warning=FALSE}
print.data.frame(svm_z_stats["linear",])
#Add the best scores of the normalized
svm_best_stats["linear z-scaled",]<-svm_z_stats["linear",]
#print best svm_best_stats
print(svm_best_stats)        

```

## SVM conclusions

```{r svm conclusion and extra, echo=FALSE, message=FALSE, warning=FALSE}
 
roc_test<-roc.test(pROC_svm_linear,pROC_svm_rbf)
roc_test$estimate


g2 <- ggroc(list(vanilladot=pROC_svm_linear, rbf=pROC_svm_rbf))+labs(title = "linear Vs. RBF")


# And change axis labels to FPR/FPR
linear_roc <- ggroc(pROC_svm_linear, legacy.axes = TRUE)+ ggtitle("linear ROC curve") +geom_segment(aes(x = 0, xend = 1, y = 0, yend = 1), color="darkgrey", linetype="dashed")
rbf_roc <- ggroc(pROC_svm_rbf, legacy.axes = TRUE)+ ggtitle("rbf ROC curve") +geom_segment(aes(x = 0, xend = 1, y = 0, yend = 1), color="darkgrey", linetype="dashed")

#grid.arrange(linear_roc + xlab("FPR") + ylab("TPR"),g2+theme(legend.position = "top"),rbf_roc + xlab("FPR") + ylab("TPR"), ncol=3) 

```

# K-Nearest Neighbors (KNN) algorithm
The KNN algorithm is a non-parametric supervised learning method, used for classification and regression (we used it for classification). It is based on the assumption that similar objects exist in close proximity (are near to each other). K is a positive integer (a user-defined constant) that represents the number of training samples. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class which is most frequent among the k training samples nearest to that query point.

## normalized data
### choose K's manually
```{r pred_knn_3, message=FALSE, warning=FALSE, results='hide'}
pred_knn_3 <- knn(train = train_set_norm[,-63], test = test_set_norm[,-63], cl = data_train[,63], k=3)
knn_table_3<-CrossTable(x = data_test[,63], y = pred_knn_3, prop.chisq=FALSE)[["t"]]
knn_seperate_cv_stats <- data.frame(k=3, Accuracy=accuracy(knn_table_3)$.estimate,
                           Sensitivity=round(sensitivity(knn_table_3),3), 
                           Precision=round(precision(knn_table_3),3), 
                           Specificity=round(specificity(knn_table_3),3))
```
We chose and tried k=3,9,11,21, but we showed here just for k=3
```{r manual k choice, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}

pred_knn_9 <- knn(train = train_set_norm[,-63], test = test_set_norm[,-63], cl = data_train[,63], k=9)
knn_table_9<-CrossTable(x = data_test[,63], y = pred_knn_9, prop.chisq=FALSE)[["t"]]

pred_knn_11 <- knn(train = train_set_norm[,-63], test = test_set_norm[,-63], cl = data_train[,63], k=11)
knn_table_11<-CrossTable(x = data_test[,63], y = pred_knn_11, prop.chisq=FALSE)[["t"]]

pred_knn_21 <- knn(train = train_set_norm[,-63], test = test_set_norm[,-63], cl = data_train[,63], k=21)
knn_table_21<-CrossTable(x = data_test[,63], y = pred_knn_21, prop.chisq=FALSE)[["t"]]

knn_seperate_cv_stats <- data.frame(
                           Accuracy=c(accuracy(knn_table_3)$.estimate,
                                    accuracy(knn_table_9)$.estimate,
                                    accuracy(knn_table_11)$.estimate,
                                    accuracy(knn_table_21)$.estimate),
                           Sensitivity=c(round(sensitivity(knn_table_3),3),
                                       round(sensitivity(knn_table_9),3),
                                       round(sensitivity(knn_table_11),3),
                                       round(sensitivity(knn_table_21),3)), 
                           Precision=c(round(precision(knn_table_3),3),
                                       round(precision(knn_table_9),3),
                                       round(precision(knn_table_11),3),
                                       round(precision(knn_table_21),3)), 
                           Specificity=c(round(specificity(knn_table_3),3),
                                       round(specificity(knn_table_9),3),
                                       round(specificity(knn_table_11),3),
                                       round(specificity(knn_table_21),3)),
                           K=c("k=3","k=9","k=11","k=21"))
```
```{r message=FALSE, warning=FALSE, echo=FALSE}
grid.arrange(tableGrob(knn_seperate_cv_stats),ncol=1)
best_knn_stats<-knn_seperate_cv_stats[3,]
rownames(best_knn_stats)<-"manually chosen K's"
```

The best results we got was with k=11, with accuracy of 76%, sensitivity of 74.1% and specificy of 78.3% (precision was 80% like k=9 and k=21).

Instead of choosing the K's manually we can use tuneGrid and find the best k in the range of 1-30

### choose K's with Caret's "train"
#### cross validation
We trained the algorithm on the normalized training set with the 'train' function of the 'caret' library with different options of K (in the range of 1-30). We started with cross validation. 
```{r message=FALSE, warning=FALSE}
train_with_knn_cv<-train(Class ~ .,train_set_norm,
                       method = "knn",
                       trControl = trainControl(method = "cv", number = 10),
                       tuneGrid = data.frame(k=seq(1,30,2)),
                       metric = "Kappa")
```
We used confusionMatrix to check how well we did (checked on both the train and the test sets).
```{r message=FALSE, warning=FALSE}
knn_predictions_cv<-predict(train_with_knn_cv, test_set_norm)
knn_cv_mat <- confusionMatrix(knn_predictions_cv,data_test[,63])
```
We then looiked at the statistics
```{r KNN cross validation stats, message=FALSE, warning=FALSE}
best_knn_stats["cross validation",] <- data.frame( 
                           Accuracy=knn_cv_mat[["overall"]][["Accuracy"]],
                           Sensitivity=knn_cv_mat[["byClass"]][["Sensitivity"]], 
                           Precision=round(knn_cv_mat[["byClass"]][["Precision"]],3), 
                           Specificity=round(knn_cv_mat[["byClass"]][["Specificity"]],3),
                           K=paste0("k=",train_with_knn_cv[["bestTune"]][["k"]]))
print.data.frame(best_knn_stats)
```

Compared to the normalized manually chosen data we have an increase in the sensitivity (74.1%->80%), but a significant decrease in the rest (the accuracy went from 76% to 72%, the precision from 80% to 69% and the specificity from 78.3% to 64%).

#### repeated cross validation
We wanted to see if using repeated cross validation will yield better results.
```{r train_with_knn_repeatedcv, message=FALSE, warning=FALSE}
train_with_knn_repeatedcv<-train(Class ~.,train_set_norm,
                       method = "knn",
                       tuneGrid = data.frame(k=seq(1,30,2)),
                       trControl = trainControl(method = "repeatedcv", number = 10, repeats = 10),
                       metric = "Kappa")
```

Again we used confusionMatrix to check how well we did.
```{r knn_predictions_rep_cv, message=FALSE, warning=FALSE, echo=FALSE}
knn_predictions_rep_cv<-predict(train_with_knn_repeatedcv, test_set_norm)
knn_repcv_mat <- confusionMatrix(knn_predictions_rep_cv,data_test[,63])
knn_df_norm_rep_cv<-data.frame(predictions=c(as.numeric(knn_predictions_rep_cv)),labels = c(data_test[,63]))
best_knn_stats["repeatedcv ",] <- data.frame(
                           Accuracy=knn_repcv_mat[["overall"]][["Accuracy"]],
                           Sensitivity=knn_repcv_mat[["byClass"]] [["Sensitivity"]], 
                           Precision=round(knn_repcv_mat[["byClass"]][["Precision"]],3), 
                           Specificity=round(knn_repcv_mat[["byClass"]][["Specificity"]],3),
                           K=paste0("k=", train_with_knn_repeatedcv[["bestTune"]][["k"]])) 
print(knn_repcv_mat[["table"]])
print.data.frame(best_knn_stats["repeatedcv ",])
#grid.arrange(tableGrob(best_knn_stats))
```

We saw no change on most aspects of our statistics, but there was a small decrease in precision from 69% to 68.97%, so we will continue to use cross validation

## z-score standardization 
### manually
We then tried with z score scaling instead of normalization. Again we started by choosing manually, and tried k=3,9,11,21, though we show here just for k=3
```{r pred_knn_3_z, message=FALSE, warning=FALSE, results='hide'}
train_set_z <- as.data.frame(scale(data_train[,-63]))
test_set_z <- as.data.frame(scale(data_test[,-63]))
train_set_z$Class <- data_train$Class
test_set_z$Class <- data_test$Class

pred_knn_3_z <- knn(train = train_set_z[,-63], test = test_set_z[,-63], cl = data_train[,63], k=3)
knn_table_3_z<-CrossTable(x = data_test[,63], y = pred_knn_3_z, prop.chisq=FALSE)[["t"]]
knn_seperate_cv_stats_z <- data.frame(k=3, Accuracy=accuracy(knn_table_3_z)$.estimate,
                           Sensitivity=round(sensitivity(knn_table_3_z),3), 
                           Precision=round(precision(knn_table_3_z),3), 
                           Specificity=round(specificity(knn_table_3_z),3))
```
```{r knn k choice z, message=FALSE, warning=FALSE, echo=FALSE, results='hide'}
pred_knn_9_z <- knn(train = train_set_z[,-63], test = test_set_z[,-63], cl = data_train[,63], k=9)
knn_table_9_z<-CrossTable(x = data_test[,63], y = pred_knn_9_z, prop.chisq=FALSE)[["t"]]

pred_knn_11_z <- knn(train = train_set_z[,-63], test = test_set_z[,-63], cl = data_train[,63], k=11)
knn_table_11_z<-CrossTable(x = data_test[,63], y = pred_knn_11_z, prop.chisq=FALSE)[["t"]]

pred_knn_21_z <- knn(train = train_set_z[,-63], test = test_set_z[,-63], cl = data_train[,63], k=21)
knn_table_21_z<-CrossTable(x = data_test[,63], y = pred_knn_21_z, prop.chisq=FALSE)[["t"]]

knn_seperate_cv_stats_z <- data.frame(
                           Accuracy=c(accuracy(knn_table_3_z)$.estimate,
                                    accuracy(knn_table_9_z)$.estimate,
                                    accuracy(knn_table_11_z)$.estimate,
                                    accuracy(knn_table_21_z)$.estimate),
                           Sensitivity=c(round(sensitivity(knn_table_3_z),3),
                                       round(sensitivity(knn_table_9_z),3),
                                       round(sensitivity(knn_table_11_z),3),
                                       round(sensitivity(knn_table_21_z),3)), 
                           Precision=c(round(precision(knn_table_3_z),3),
                                       round(precision(knn_table_9_z),3),
                                       round(precision(knn_table_11_z),3),
                                       round(precision(knn_table_21_z),3)), 
                           Specificity=c(round(specificity(knn_table_3_z),3),
                                       round(specificity(knn_table_9_z),3),
                                       round(specificity(knn_table_11_z),3),
                                       round(specificity(knn_table_21_z),3)),
                           K=c("k=3","k=9","k=11","k=21"))
```
```{r message=FALSE, warning=FALSE}
print.data.frame(knn_seperate_cv_stats_z)
best_knn_stats["manually chosen K z",] = knn_seperate_cv_stats_z[2,]
```

We saw that the results were much better using the z-score scaling. The best result was from k=9, with accuracy of 82% (rather than 76% or 72% from the normalized data choosing manually or with Caret respectively), sensitivity of 86.4% rather than 74.1% or 80% from the normalized data choosing manually or with Caret respectively) and specificity of 78.6% (rather than 78.3% or 64% from the normalized data choosing manually or with Caret respectively). The 76% precision was higher than the Caret chosen K's (~69%) but lower than the manually chosen K's (80%). All in all we thinl that using z-score to scale did a better job than our normalization. 
We then proceeded to check the cress validation using Caret again but on scaled data

### cross validation on scaled data
```{r knn z score train, message=FALSE, warning=FALSE, results='hide'}
train_with_knn_repeatedcv_z<-train(train_set_z[,-63],data_train[,63],
                       method = "knn",
                       tuneGrid = data.frame(k=seq(1,30,2)),
                       trControl = trainControl(method = "cv", number = 10, repeats = 10),
                       metric = "Kappa")
```
```{r knn z score, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
train_cknn_repcv_mat_z <- confusionMatrix(predict(train_with_knn_repeatedcv_z, train_set_z[,-63]),data_train[,63])
knn_predictions_rep_cv_z<-predict(train_with_knn_repeatedcv_z, test_set_z[,-63])
knn_repcv_mat_z <- confusionMatrix(knn_predictions_rep_cv_z,data_test[,63])

knn_z_table<-CrossTable(x = data_test[,63], y = knn_predictions_rep_cv_z, prop.chisq=FALSE)
df_z<-data.frame(predictions=c(as.numeric(knn_predictions_rep_cv_z)),labels = c(data_test[,63]))
best_knn_stats["cv z score",] <- data.frame(
                          Accuracy=knn_repcv_mat_z[["overall"]][["Accuracy"]],
                           Sensitivity=knn_repcv_mat_z[["byClass"]][["Sensitivity"]], 
                           Precision=round(knn_repcv_mat_z[["byClass"]][["Precision"]],3), 
                           Specificity=round(knn_repcv_mat_z[["byClass"]][["Specificity"]],3),
                           K=paste0("k=", train_with_knn_repeatedcv_z[["bestTune"]][["k"]])) 
```
```{r message=FALSE, warning=FALSE}
print.data.frame(best_knn_stats["cv z score",])
```

Compared to cross validation on normalized data we have increased accuracy (72%->78%), precision (69%->81.8%) and specificity (64%->84%), but decreased sensitivity (80%->72%).
Compared to manually chosen K's on z-scaled data we have increased precision (76%->81.8%) and specificity (78.6%->84%), but decreased accuracy (82%->78%) and sensitivity (86.4%->72%).

## knn conclusions
By looking at the confusion matrices of both the manually and Caret chosen K's 
```{r message=FALSE,warning=FALSE}
best_knn_stats[,"method"]<-rownames(best_knn_stats)
best_knn_stats_melt<-best_knn_stats %>% 
  select( method,Accuracy,Sensitivity,Precision,Specificity) %>%
  pivot_longer(., cols = c(Accuracy,Sensitivity,Precision,Specificity), 
               names_to = "statistics", values_to = "percentage")
plotted_knn<-ggplot(best_knn_stats_melt,aes(x=statistics, y=percentage, fill=method)) +
  geom_bar(position = 'dodge', stat='identity')+ theme(legend.position="top")+ labs(title = "best knn statistics")
#grid.arrange( plotted_knn, tableGrob(best_knn_stats[,-6]),nrow = 2, ncol = 1)
```
```{r message=FALSE,warning=FALSE}
t_knn_9_z <- tableGrob(knn_table_9_z)
gtitle_knn_9_z <- textGrob("manually chosen (k=9)")
gtable_knn_9_z <- gtable_add_rows(t_knn_9_z, heights = grobHeight(gtitle_knn_9_z)+ unit(5,"mm"), pos = 0)
gtable_knn_9_z <- gtable_add_grob(gtable_knn_9_z, gtitle_knn_9_z, 1, 1, 1, ncol(gtable_knn_9_z))

t_cv_z <- tableGrob(knn_z_table[["t"]])
title_cv_z <- textGrob("cross validation (z-score scaled)")
table_cv_z <- gtable_add_rows( t_cv_z,heights = grobHeight(title_cv_z) + unit(5,"mm"), pos = 0)
table_cv_z <- gtable_add_grob( table_cv_z, title_cv_z, 1, 1, 1, ncol(table_cv_z))

#confusion matrices
grid.arrange(gtable_knn_9_z,table_cv_z,ncol=2)
#grid.newpage()
#grid.draw(table_cv_z)
```

We did not see any difference whether we used the crossed validation or the repeated cross validation.
We did see a difference when we used the Z score to scale instead of normalization: we gotincreased accuracy/AUC (from 72% to 84%) and precision (from 67.7% to 90.4%!), but at the cost of the sensitivity/recall (which decreased from 84% to 76%).

# Decision Tree (DT) algorithm


We'll start by applying the model in training data (63th column is the label to be predicted)
```{r message=FALSE, warning=FALSE}
DT_model <- C5.0(data_train[-63], data_train$Class)
```

The preceding text shows some simple facts about the tree, including the function
call that generated it, the number of features (labeled predictors), and examples
(labeled samples) used to grow the tree. Also listed is the tree size of 62, which
indicates that the tree is 62 decisions deep.

Next we looked at the summary of the model. 
```{r message=FALSE, warning=FALSE, results='hide'}
summary(DT_model)
```

The numbers in parentheses indicate the number of examples meeting the criteria for
that decision, and the number incorrectly classified by the decision. 
For instance, on the first line, 56/4 indicates that of the 56 examples reaching the decision, 4 were incorrectly classified as not likely to default. In other words, 4 applicants actually defaulted, in spite of the model's prediction.

As we now know, it is very important to evaluate our model performance:

```{r message=FALSE, warning=FALSE}
# apply model on test data
DT_pred <- predict(DT_model, data_test)
print(confusionMatrix(DT_pred,data_test[,63], dnn = c('actual', 'predicted'))[["table"]])
```

The performance here is somewhat worse than its performance on the
training data, but not unexpected, given that a model's performance is often worse
on unseen data. Also note that there are relatively many mistakes where the model predicted not a default, when in practice the loaner did default. 
Unfortunately, this type of error is a potentially costly mistake, as sick patients could be blind with time. 
We will try to improve the result.

### Adaptive Boosting

To improve our model we will use a C5.0 feature called adaptive boosting. This is a process in which many decision trees are built and the trees vote on the best class for each example. 

The C5.0() function makes it easy to add boosting to our C5.0 decision tree. We
simply need to add an additional trials parameter indicating the number of
separate decision trees to use in the boosted team. The trials parameter sets an
upper limit; the algorithm will stop adding trees if it recognizes that additional 
trials do not seem to be improving the accuracy. 
We'll start with 10 trials, a number that has become the de facto standard, as research 
suggests that this reduces error rates on test data by about 25 percent:

```{r message=FALSE, warning=FALSE, results='hide'}
# boosting with 10 trials (on training)
DT_boost10 <- C5.0(data_train[-63], data_train$Class , trials = 10)
summary(DT_boost10)
```

The classifier got an error rate of
0% percent. This is quite an improvement over the previous training error rate
before adding boosting! However, it remains to be seen whether we see
a similar improvement on the test data. Let's take a look:

```{r message=FALSE, warning=FALSE}
# boosting on test data
DT_boost_pred10 <- predict(DT_boost10, data_test)
print(confusionMatrix(DT_boost_pred10, data_test[,63], dnn = c('actual', 'predicted'))[["table"]])
```

The model is still not doing well at predicting class of patients, which may be a result of our relatively small training dataset, or it may just be a very difficult problem to solve.

Next, lets proceed to fine-tune our algorithm, using a cost matrix. 
The C5.0 algorithm allows us to assign a penalty to different types of errors, in order to discourage a tree from making more costly mistakes. The penalties are designated in a cost matrix, which specifies how much costlier each error is, relative to any other prediction.

First, we'll create a default 2x2 matrix, to later be filled with our cost values:

```{r message=FALSE, warning=FALSE}
matrix_dimensions <- list(c("normal", "glaucoma"), c("normal", "glaucoma"))
names(matrix_dimensions) <- c("predicted", "actual")
```

If we consider that the most important is to predict a sick patient as sick, much more that a normal patient as normal,
our penalty values could then be defined as:

```{r message=FALSE, warning=FALSE}
error_cost <- matrix(c(0, 6,20, 0), nrow = 2, dimnames = matrix_dimensions)
print(error_cost)
```

Now lets train again and see if the cost matrix made any difference:

```{r message=FALSE, warning=FALSE}
dt_cost <- C5.0(data_train[-63], data_train$Class , costs = error_cost)
dt_cost_pred <- predict(dt_cost, data_test) # predict on test data
print(confusionMatrix(dt_cost_pred, data_test[,63],dnn = c('actual', 'predicted'))[["table"]])
```

Compare these results to the boosted model; this version makes more mistakes overall, but the types of mistakes are very different. Where the previous models incorrectly classified a small number of class correctly, our weighted model has does much better in this regard. This trade resulting in a reduction of false negatives at the sick patient false positives may be acceptable if our cost estimates were accurate.


```{r message=FALSE, warning=FALSE}
plot(dt_cost)
```

# Random forest (RF) algorithm
Random forest is a Supervised Machine Learning Algorithm. This algorithm is used in Classification and Regression problems. It builds decision trees on different samples and takes their majority vote for classification and average in case of regression.
The Random Forest Algorithm can handle the data set containing continuous variables as in the case of regression and categorical variables as in the case of classification.(cf: https://www.analyticsvidhya.com/blog/2021/06/understanding-random-forest/)

## Random Forest Modeling

The random Forest algorithm uses many decision trees. For classification, which is what
we are focusing on, the final class result is based on the class that was picked from most trees. 

First we define our model with the parsnip package. Random forests have very little mandatory parameters. 
Here we only define the number of trees:

```{r rf_mod, message=FALSE, warning=FALSE}
# create random forest model
rf_mod <-  rand_forest(trees = 1000) %>%  set_engine("ranger") %>% set_mode("classification") 
```

We want reproducible results, so we set.seed. Taking a look at the model:

```{r rf_fit, message=FALSE, warning=FALSE}
# train model
rf_fit <- rf_mod %>% fit(Class ~ ., data = data_train)
rf_fit
```

We can see our model had 1000 trees like we specified, and several other metrics, such as the prediction error, 13% here.

### Estimating performance

Next we want to see if we can improve our model. We do this by changing some things about it. 
The changes can either be random (guessing) or more precise, depending on performance metrics. In our example, we will use the area under the Receiver Operating Characteristic (ROC) curve (which demonstrates the trade-off between the sensitivity and and specificity), and overall classification accuracy.

Using the yardstick package, let's calculate ROC and Accuracy. Notice that we are still only working with the data_train partition of our data:

```{r RF rf_training_pred, message=FALSE, warning=FALSE}
rf_training_pred <- predict(rf_fit, data_train) %>% 
  bind_cols(predict(rf_fit, data_train, type = "prob")) %>% 
  bind_cols(data_train %>%  select(Class))

rf_training_pred %>%   roc_auc(truth = Class, .pred_glaucoma)# ROC calculation
rf_training_pred %>%  accuracy(truth = Class, .pred_class)# Accuracy calculation
```

As we can see, these are very good results. Almost too good, so we would like to see how the model performs on the test data:

```{r RF rf_testing_pred, message=FALSE, warning=FALSE}
rf_testing_pred <- 
  predict(rf_fit, data_test) %>% 
  bind_cols(predict(rf_fit, data_test, type = "prob")) %>% 
  bind_cols(data_test %>% select(Class))

rf_testing_pred %>%  roc_auc(truth = Class, .pred_glaucoma) # ROC calculation
rf_testing_pred %>%   accuracy(truth = Class, .pred_class) # Accuracy calculation
```

These validation results are lower than the ones we got on the training data.
There are several reasons why training set statistics like the ones shown in this section can be unrealistically optimistic:

- Overfitting - Models like random forests, neural networks, and other black-box methods can essentially memorize the training set. Re-predicting that same set should always result in nearly perfect results.

- The training set does not have the capacity to be a good arbiter of performance. It is not an independent piece of information; predicting the training set can only reflect what the model already knows.

## Resampling

Resampling methods, such as cross-validation and the bootstrap, are empirical simulation systems. They create a series of data sets similar to the training/testing split discussed previously; a subset of the data is used for creating the model and a different subset is used to measure performance. Resampling is always used with the training set. 

Here we'll use 10-fold cross-validation. This means that we'll create 10 "mini" datasets, or folds. We call the majority part of the folds (9 out of 10 in this case) the "analysis set" and the minority the "assessment set". We then train a model using the analysis set, and test it on the assessment set, effectively repeating the modeling process 10 times. This is how its done:

```{r RF folds, message=FALSE, warning=FALSE, results='hide'}
folds <- vfold_cv(data_train, v = 10) # create the folds
folds
# here we use a workflow, which bundles together the model specifications and actual modeling
# set the random forest workflow 
rf_wf <-   workflow() %>%  add_model(rf_mod) %>%  add_formula(Class ~ .)

# add folds to workflow and train model
rf_fit_rs <-   rf_wf %>%   fit_resamples(folds)
rf_fit_rs
```
```{r message=FALSE, warning=FALSE}
print(collect_metrics(rf_fit_rs))
```

We see these results are lower and look more realistic.
Now, looking at the test set for results, we expect and see much more similar results to the above ROC and accuracy:

```{r rf_testing_pred stats, message=FALSE, warning=FALSE}
rf_testing_pred %>%    roc_auc(truth = Class, .pred_glaucoma)
rf_testing_pred %>% accuracy(truth = Class, .pred_class)
```


## Tuning hyperparameters

Another way to improve the performance of our models is by changing the parameters we provide them with. This is also called Tuning. Random Forests, as mentioned above, are not very sensitive to such parameters, but decision trees are. Lets see:

```{r Tuning tune_spec, message=FALSE, warning=FALSE, results='hide'}
# defining the parameters for the decision tree
tune_spec <-   decision_tree(cost_complexity = tune(), # to control the size of the tree
    tree_depth = tune()) %>% set_engine("rpart") %>%   set_mode("classification")
tune_spec
```

Note that in the above code, tune() is still just a placeholder. 
We will fill in values later on.

Next, we will create several smaller datasets, to try different parameters. 
The grid_regular command below will create 5 such datasets for each parameter combination, meaning 25 in total.

```{r Tuning tree_grid, message=FALSE, warning=FALSE, results='hide'}
# create 5 'datasets' with different combinations
tree_grid <- grid_regular(cost_complexity(), 
                          tree_depth(),
                          levels = 5)
tree_grid
tree_grid %>%   count(tree_depth)
glaucoma_folds <- vfold_cv(data_train) # create the actual cross-validation folds
```
### Model tuning with a grid

```{r Tuning tree_wf,message=FALSE, warning=FALSE, results='hide'}
# define the workflow
tree_wf <- workflow() %>%  add_model(tune_spec) %>%  add_formula(Class ~ .)
# update and train workflow with grids
tree_res <- tree_wf %>%  tune_grid(resamples = glaucoma_folds, grid = tree_grid)
tree_res
tree_res %>% collect_metrics()
```
Its easier to see how the models did with a graph:

```{r RF plot preformance, message=FALSE, warning=FALSE, echo=FALSE, results='hide'}
# plot preformance
tree_res %>%
  collect_metrics() %>%
  mutate(tree_depth = factor(tree_depth)) %>%
  ggplot(aes(cost_complexity, mean, color = tree_depth)) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free", nrow = 2) +
  scale_x_log10(labels = scales::label_number()) +
  scale_color_viridis_d(option = "plasma", begin = .9, end = 0)
```

We can see that our tree with a depth of 1, is the worst model according to both metrics and across all candidate values of cost_complexity. Our deepest tree, with a depth of 15, did better.  The show_best() function shows us the top 5 candidate models by default:

```{r message=FALSE, warning=FALSE}
# best tree
tree_res %>%  show_best("roc_auc")
```

And finally, let's finalize the workflow with the best tree. 

```{r RF best_tree final_wf, message=FALSE, warning=FALSE}
best_tree <- tree_res %>%  select_best("roc_auc") # Selecting the best tree
best_tree
# Finalize workflow
final_wf <-   tree_wf %>%   finalize_workflow(best_tree)
final_wf
```
### Exploring results

So we have our best tree parameters. Lets take a better look:

```{r  Exploring results final_tree, message=FALSE, warning=FALSE}
final_tree <-   final_wf %>%  fit(data = data_train) 
final_tree
```

Another way to look at how the model uses the different features is using the vip package:

```{r message=FALSE, warning=FALSE}
#final_tree %>% 
 # pull_workflow_fit() %>% 
  #vip()
final_tree %>%   extract_fit_parsnip() %>%   vip()
```
## Random forest conclusions and Finalizing the model


Finally, let’s return to our test data and estimate the model performance we expect to see with new data. We can use the function last_fit() with our finalized model; this function fits the finalized model on the full training data set and evaluates the finalized model on the testing data.

```{r Random forest conclusions, message=FALSE, warning=FALSE}
final_fit <- final_wf %>%  last_fit(patient_split) # Set workflow
final_fit %>%  collect_metrics()# train
final_fit %>%  collect_predictions() %>%  # collect metrics
  roc_curve(Class, .pred_glaucoma) %>%   autoplot()
args(decision_tree)
```
We can observe in the ROC curve that in our true positive prediction we have very few false positive predictions.

