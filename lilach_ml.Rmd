---
title: "Final Project"
author: "Lilach Herzog & Leslie Cohen"
date: "13 9 2022"
output:
  pdf_document:
    fig_caption: yes
    toc: yes
    toc_depth: 1
  word_document:
    toc: yes
    toc_depth: 2
editor_options:
  chunk_output_Type: inline
fig_width: 5 
fig_height: 3
fig_caption: true
bookdown::pdf_book:
  includes:
    in_header: preamble.tex
---
```{r, message=FALSE, warning=FALSE, echo=FALSE}
knitr::opts_chunk$set(fig.width=5, fig.height=3,collapse = TRUE) 
```
## Upload data and libraries

```{r, message=FALSE, warning=FALSE, echo=FALSE}
library(tidymodels) # for the sampling and splitting the data
library(class)

library("Hmisc")
library(corrplot) # for eda
library(hopkins)

library(tidyverse)    # data manipulation and visualization
library(ggplot2)
library(ggfortify) #autoplot
library(gridExtra) # grid.arrange
library(grid) # textGrob
library(gtable) # gtable_add_rows+gtable_add_grob

library(caret) # confusionMatrix
library(gmodels) # CrossTable

library(caret) # Classification And REgression Training: contains functions to streamline the model training process for complex regression and classification problems.
library(pROC) # roc

library(kernlab)      # SVM: ksvm
library(e1071)        # SVM methodology

library(C50) # DT
library(modeldata)  # for the cells data
library(vip)        # for variable importance plots
library(ranger)

```


```{r message=FALSE, warning=FALSE}

glaucoma <- read.csv("GlaucomaM.csv")# read data
```

We have data from 196 patients, with 63 features.
Since the whole table is already shuffled according to the 'Class' column, we don't need to shuffle the whole table again
In order to get consistent results we initialized a pseudo random number generator. We also made sure to shuffle our data.
```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
set.seed(123)
glaucoma <- glaucoma[sample(nrow(glaucoma)),]
```


# preproccessing

 We want to classify our data according to the 'Class' column, which contains the information about whether all the information about a person (represented in a row) belongs to a healthy (normal) or sick (glaucoma) patient, divided neatly in half. Before starting on teaching the algorithm, we converted the 'Class' column to a factor.
```{r, message=FALSE, warning=FALSE, results='hide'}
glaucoma$Class<-as.factor(glaucoma$Class)
glaucoma_num_class<-as.numeric(glaucoma$Class)
```

## EDA analysis

Results obtained with corrplot: Positive correlations are displayed in a
blue scale while negative correlations are displayed in a red scale. we
can observe 8 clusters of features that highly correlate. Results obtained with heatmap: we
can observe one big cluster that could be divided in 2 and 2 others
clusters of features.

We would like to observe if we can  cluster patients healthy or with glaucoma with all the features.
We performed first a PCA on all features and values.

Preform PCA on our data
```{r, message=FALSE, warning=FALSE, results='hide'}
default_pca <- prcomp(glaucoma[,1:62], center = TRUE,  scale. = TRUE)
summary(default_pca)
default_pca_plot <- autoplot(default_pca, data = glaucoma, colour = 'Class')
fit_pca <- lm(Class ~ .,
            cbind(Class = glaucoma[, "Class"], default_pca$x[, 1:2]) %>% as.data.frame())
```
```{r, message=FALSE, warning=FALSE, echo=FALSE, fig.height=2}
grid.arrange(default_pca_plot +
               labs(title = paste0("hopkins=",round(hopkins(glaucoma[,1:62], m=nrow(default_pca$x)-1),3)),
                    subtitle =paste0("r-sqr= ", round(summary(fit_pca)$adj.r.squared, 3))))
```

In the PCA, we can observe 2 clusters of patients: glaucoma patients and normal
patients. Unfortunately, some of them were not clustered correctly (mixed clouds of blue and red points) in the same area. 

## correlation
This mixture could be, probably to the fact that many features correlate, so our next course of action was to check for any correlation between features to determine which feature are correlated by using pearson correlation


```{r, message=FALSE, warning=FALSE, results='hide', fig.show='hide'}
gl.cor = cor(glaucoma[,c(1:62)], method = c("pearson"))
corrplot(gl.cor)
palette = colorRampPalette(c("green", "white", "red")) (20)
heatmap(x = gl.cor, col = palette, symm = TRUE)
```

```{r, message=FALSE, warning=FALSE, results='hide', fig.show='hide'}
gl.rcorr = rcorr(as.matrix(glaucoma[,c(1:62)]))
gl.rcorr
gl.coeff = gl.rcorr$r

cor_matrix_rm <- gl.cor                  # Modify correlation matrix
cor_matrix_rm[upper.tri(cor_matrix_rm)] <- 0
diag(cor_matrix_rm) <- 0

data<-glaucoma[,c(1:62)]
# Remove highly correlated variables
data_no_corr <- data[ , !apply(cor_matrix_rm,  2,function(x) any(x > 0.8))]
head(data_no_corr)
colSums(glaucoma[,c(1:62)] != 0)

gl.cor2 = cor(data_no_corr, method = c("pearson"))
corrplot(gl.cor2)
heatmap(x = gl.cor2, col = palette, symm = TRUE)
```
Preform PCA on our new data
```{r, message=FALSE, warning=FALSE, results='hide', fig.show='hide'}
corr_pca <- prcomp(data_no_corr, center = TRUE, scale. = TRUE)#play with options
summary(corr_pca)
data_no_corr$Class<-glaucoma$Class
corr_pca_plot <- autoplot(corr_pca, data = data_no_corr, colour = 'Class')
fit_corr_pca  <- lm(Class ~ ., cbind(Class = glaucoma$Class, corr_pca$x[, 1:2])
                    %>% as.data.frame())
```


## importance
We then used the varImp to estimate the variable importance, which is printed and plotted, using the logistic regression method to train.
We will start learning about training and ML algorithms starting next practice.
```{r,  message=FALSE, warning=FALSE, results='hide', fig.show='hide'}
control <- trainControl(method="repeatedcv", number=10, repeats=3)
model <- train(Class~., data=glaucoma, method="multinom", preProcess="scale", trControl=control)
importance <- varImp(model, scale=FALSE) # estimate variable importance
print(importance)
plot(importance)

important_feat<-glaucoma[,rownames(importance$importance)[importance$importance$Overall>0.5]]
important_feat.pca <- prcomp(important_feat, center = TRUE,  scale. = TRUE)
summary(important_feat.pca)
important_feat[,"Class"]<-glaucoma[,"Class"]
important_feat_plot <- autoplot(important_feat.pca, data = important_feat, colour = 'Class')
fit_important <- lm(Class ~ ., cbind(Class = glaucoma$Class, important_feat.pca$x[, 1:2]) %>% as.data.frame())
```
```{r, message=FALSE, warning=FALSE, echo=FALSE, results='hide', fig.show='hide', fig.cap="PCA", fig.width=6, fig.height=2}
grid.arrange(default_pca_plot +
               labs(title = "all data" ,subtitle =
                                 paste(paste0("r-sqr= ", round(summary(fit_pca)$adj.r.squared, 3)),
                                 paste0(";hopkins=",round(hopkins(glaucoma[,-63], m=nrow(default_pca$x)-1),4)))) +
               theme(legend.position = "bottom"),
             corr_pca_plot + labs(title = "remove correlates",subtitle =
                                 paste(paste0("r-sqr= ", round(summary(fit_corr_pca )$adj.r.squared, 3)),
                                 paste0(";hopkins=",round(hopkins(data_no_corr[,-21], m=nrow(corr_pca$x)-1),3)))) +
               theme(legend.position = "bottom"),
             important_feat_plot + labs(title = "keep importatnt features",subtitle =
                                 paste(paste0("r-sqr= ", round(summary(fit_important)$adj.r.squared, 3)),
                                 paste0(";hopkins=",
                                        round(hopkins(important_feat[,-31], m=nrow(important_feat.pca$x)-1),4)))) +
               theme(legend.position = "bottom"),
             ncol=3,nrow=1) 
```
```{r, message=FALSE, warning=FALSE, echo=FALSE}
pca_stats <- data.frame(features = c("All features", "Without correlated features", "Only important features"),
                        r_squared = c(round(summary(fit_pca)$adj.r.squared, 3),
                                      round(summary(fit_corr_pca)$adj.r.squared, 3),
                                      round(summary(fit_important)$adj.r.squared, 3)),
                        hopkins = c(round(hopkins(glaucoma[,-63], m=nrow(default_pca$x)-1),4),
                                    round(hopkins(data_no_corr[,-21], m=nrow(corr_pca$x)-1),4),
                                    round(hopkins(important_feat[,-31], m=nrow(important_feat.pca$x)-1),4)))

print.data.frame(pca_stats)                      
```
It looks like the best result we got from the data without the correlated features. We will sometimes try to run the algorithms on the smaller dataset without the correlated, but seeing as they are all fairly similar, we'll mainly work on the whole dataset.

## split into training and test sets

We split the whole data set into 80% training set and 20% test set.
```{r splitting, message=FALSE, warning=FALSE}
set.seed(123)
index <- sample(nrow(glaucoma),round(nrow(glaucoma)*.8),replace = FALSE) 
train_set <- glaucoma[index,]
test_set<- glaucoma[-index,]
train_set_labels <-train_set["Class"] #save true "Classes"of the training set
test_set_labels <-test_set["Class"] #save true "Classes"of the test set
```
Since we know that there are about a half of each Class in the data set, we wanted to be sure that the distribution is as we defined (about 1/2 of each Class in both training and test sets should be 'normal' and 1/2 'glaucoma').

```{r, message=FALSE, warning=FALSE}
train_div<-prop.table(table(train_set$Class))
testdiv<-prop.table(table(test_set$Class))
print.data.frame(data.frame(set=c("train","test"),glaucoma=c(train_div[1],testdiv[1]),
                            normal=c(train_div[2],testdiv[2])))
```
The training set is what we will use to train the algorithm, to help it learn our data, and then we will run the algorithm on 20% of the data (test set), and predict according to what it learned in the training set, and since it is still our data we have the true class each individual belongs to, so we can evaluate how well the algorithm succeeded in predicting.

We then tried instead of splitting it out own to try the 'initial_split' function, with the buikt in 'training' and 'testing' functions
```{r better splitting, message=FALSE, warning=FALSE}
patient_split <- initial_split(glaucoma, strata = Class )
data_train <- training(patient_split) # we used the default parameters of splitting size
data_test <- testing(patient_split)
train_div<-prop.table(table(data_train$Class))
testdiv<-prop.table(table(data_test$Class))
print.data.frame(data.frame(set=c("train","test"),glaucoma=c(train_div[1],testdiv[1]),
                            normal=c(train_div[2],testdiv[2])))
```
instead of 0.5159236/ 0.4840764 partition of glaucoma/ normal in the training set and 0.4358974/0.5641026 in the test set, we got exactly 0.5 in both sets.

We created a training and test set that include only the desired features.
```{r warning=FALSE, message=FALSE, echo=FALSE, results='hide'}
data_train_new<-data_train[,colnames(data_no_corr)]
data_test_new<-data_test[,colnames(data_no_corr)]
```

Since in most algorithms it is important for the data to be scaled, we decided to start by normalizing it.

#### normalization:
We decided to normalize the data, thinking that the range of values of each parameter is similar in order to compare the distances of different features with different scales.
```{r normalization, results='hide', message=FALSE, warning=FALSE}
normalize <- function(x) {return ((x - min(x)) / (max(x) - min(x)))}
```
```{r results='hide', message=FALSE, warning=FALSE, echo=FALSE}
normalize(c(1, 2, 3, 4, 5)) # test our function 1
normalize(c(10, 20, 30, 40, 50)) # test our function 2
```
```{r results='hide', message=FALSE, warning=FALSE}
train_set_norm <- cbind(as.data.frame(lapply(data_train[,-63], normalize)), Class=data_train$Class)
test_set_norm <- cbind(as.data.frame(lapply(data_test[,-63], normalize)), Class=data_test$Class)
summary(train_set_norm$ag) # test our dataset
```
```{r results='hide', message=FALSE, warning=FALSE, echo=FALSE}
train_set_norm_new<-train_set_norm[,colnames(data_no_corr)]
test_set_norm_new<-test_set_norm[,colnames(data_no_corr)]
```

#### z-score standardization: 
We also tried with z score scaling instead of normalization.
```{r z-score standardization, message=FALSE, warning=FALSE, results='hide'}
train_set_z <- cbind(as.data.frame(lapply(data_train[,-63], scale)), Class=data_train$Class)
test_set_z <- cbind(as.data.frame(lapply(data_test[,-63], scale)), Class=data_test$Class)
```
```{r results='hide', message=FALSE, warning=FALSE, echo=FALSE}
train_set_z_new<-train_set_z[,colnames(data_no_corr)]
test_set_z_new<-test_set_z[,colnames(data_no_corr)]
```

## algorithms evaluation
In order to decide which algorithm was best we calculated the accuracy, sensitivity, precision and specificity. We also wanted one general number that includes all the calculations, so we did:

- Sensitivity (true predicted glaucoma/have glaucoma) is the percentage of true positives -predicted to have glaucoma- out of all the people that actually have glaucoma. In other words- it refers to the test's ability to correctly detect ill patients who do have the condition. Since we don't want to miss a sick person that then won't be treated ( type II error), it is more important that the sensitivity  will be high. We decided to give it a weight of 3.

- Precision (true predicted glaucoma/predicted glaucoma), or positive predictive value (PPV) is the percentage of true positives out of all the predicted positives. We don't want to scare a person that isn't sick for nothing (type I error), but of course (in case of glaucoma) that error is preferable to type II error.  We decided to give it a weight of 2.

- Accuracy (true prediction/all predictions) is the proportion of correct predictions (both true positives and true negatives) among the total number of cases examined. It's a test to generally know how well we predicted, without segregating the positives from the negatives.  We decided to give it a weight of 2.

- Specificity (true predicted healthy/ healthy) relates to the test's ability to correctly reject healthy patients without a condition. It's also important, but less so.  We decided to give it a weight of 1.

```{r calc_score, results='hide', message=FALSE, warning=FALSE}
calc_score <- function(x) {return ( (2*x$Accuracy + 3*x$Sensitivity +
                                       2*x$Precision + x$Specificity) / 8 ) }
```

# SVM (Support Vector Machines)

Support vectors are the data points that define the position and the margin of the hyperplane. They are called “support” vectors, because these are the representative data points of the classes. If we move one of them, the position and/or the margin will change.
Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that assigns new examples into that same space and predicts which category they belong to based on which side of the gap they fall. The SVM algorithm maps training examples to points in space so as to maximize the width of the gap between the two categories.
The effectiveness of SVM depends a lot on the selection of kernel. We tried using 4 different kernels and compared their results.

### default 
```{r SVM default, message=FALSE, warning=FALSE, results='hide'}
Class_classifier_svm <-  ksvm(x=Class ~., data=data_train)
Class_predictions_svm <- predict(Class_classifier_svm, data_test)
svm_linear_norm <- confusionMatrix(Class_predictions_svm,data_test[,63], dnn = c('actual', 'predicted'))
svm_stats_default <- data.frame(
                           Accuracy=svm_linear_norm[["overall"]][["Accuracy"]],
                           Sensitivity=round(svm_linear_norm[["byClass"]] [["Sensitivity"]],3), 
                           Precision=round(svm_linear_norm[["byClass"]] [["Precision"]],3), 
                           Specificity=round(svm_linear_norm[["byClass"]] [["Specificity"]],3))
svm_stats_default$General <- round(calc_score(svm_stats_default),3)

```
We ran the algorithm on the datatset with the selected features
```{r  message=FALSE, warning=FALSE, results='hide', echo=FALSE}
Class_classifier_svm_new <-  ksvm(x=Class ~., data=data_train_new)
Class_predictions_svm_new <- predict(Class_classifier_svm_new, data_test_new)
svm_linear_norm_new <- confusionMatrix(Class_predictions_svm_new,data_test[,63], dnn = c('actual', 'predicted'))
pROC_svm_new <- roc(c(data_test$Class), c(as.numeric(Class_predictions_svm_new)))
svm_stats_default_new <- data.frame( 
                           Accuracy=svm_linear_norm_new[["overall"]][["Accuracy"]],
                           Sensitivity=round(svm_linear_norm_new[["byClass"]] [["Sensitivity"]],3), 
                           Precision=round(svm_linear_norm_new[["byClass"]] [["Precision"]],3), 
                           Specificity=round(svm_linear_norm_new[["byClass"]] [["Specificity"]],3))
svm_stats_default_new$General <- round(calc_score(svm_stats_default_new),3)
```
```{r  message=FALSE, warning=FALSE, echo=FALSE}
print(paste(paste0("the whole normalized dataset's score = ", svm_stats_default$General),
            paste0("> the abridged dataset's score = ", svm_stats_default_new$General)))
svm_stats<-svm_stats_default
rownames(svm_stats)<-"defalut"
```

## different kernels

Since it is important when using SVM to work only on numeric and scaled data, we made sure our data was numeric (all columns originally numeric, and the ‘Class’ column we factorizes), randomized (we used the ‘sample’ function beforehand) and scaled. 
The package we used (kernlab) let us scale the data when running the algorithm itself, so we'll start by scaling with the algorithm. We built the model:
```{r linear SVM train, message=FALSE, warning=FALSE, results='hide'}
train_svm_linear<-train(Class ~ ., data_train,
                       method = "svmLinear",
                       trControl = trainControl(method="repeatedcv", number = 10, repeats = 10),
                       metric = "Kappa",  
                       preProcess = c("center","scale"))

svm_predictions_linear<-predict(train_svm_linear, data_test)
Class_classifier_linear <-  ksvm(x=Class ~., data=data_train,kernel = "vanilladot")
```

We saw that the model used 49 support vectors, and that the training error was 0.054795. SVM can be a bit of a black box so we don't know much else.
After building the model on our training set (80% of the data) we ran the model on the test set:
```{r Class_predictions_linear, message=FALSE, warning=FALSE}
Class_predictions_linear <- predict(Class_classifier_linear, data_test[,-63])
```
We used the table() function to compare the predicted Class to the true Class in the testing:
```{r svm_linear confusionMatrix, message=FALSE, warning=FALSE,results='hide'}
table(Class_predictions_linear, data_test$Class)
svm_linear <- confusionMatrix(Class_predictions_linear,data_test[,63])
```

Now we wanted to see how well our classifier performed according to the different ways of measuring success:

```{r svm_linear_stats, message=FALSE, warning=FALSE, echo=FALSE}
df_linear<-data.frame(predictions=c(as.numeric(Class_predictions_linear)),labels = c(data_test$Class))
pROC_svm_linear <- roc(df_linear$labels,df_linear$predictions)

svm_linear_stats <- data.frame(  
                           Accuracy=svm_linear[["overall"]][["Accuracy"]],
                           #AUC=pROC_svm_linear[["auc"]],
                           Sensitivity=round(svm_linear[["byClass"]] [["Sensitivity"]],3), 
                           Precision=round(svm_linear[["byClass"]] [["Precision"]],3), 
                           Specificity=round(svm_linear[["byClass"]] [["Specificity"]],3))
svm_linear_stats$General <- round(calc_score(svm_linear_stats),3)

```
```{r message=FALSE, warning=FALSE, echo=FALSE,results='hide'}
train_svm_linear_new<-train(Class ~ ., data_train_new,
                       method = "svmLinear",
                       trControl = trainControl(method="repeatedcv", number = 10, repeats = 10),
                       metric = "Kappa",  
                       preProcess = c("center","scale"))

svm_predictions_linear_new<-predict(train_svm_linear_new, data_test_new)
Class_classifier_linear_new <-  ksvm(x=Class ~., data=data_train_new,kernel = "vanilladot")
Class_predictions_linear_new <- predict(Class_classifier_linear_new, data_test_new[,-63])
table(Class_predictions_linear_new, data_test$Class)
svm_linear_new <- confusionMatrix(Class_predictions_linear_new,data_test[,63])
svm_linear_new_stats <- data.frame(  
                           Accuracy=svm_linear_new[["overall"]][["Accuracy"]],
                           Sensitivity=round(svm_linear_new[["byClass"]] [["Sensitivity"]], 3),
                           Precision=round(svm_linear_new[["byClass"]] [["Precision"]],3), 
                           Specificity=round(svm_linear_new[["byClass"]] [["Specificity"]],3))
svm_linear_new_stats$General <- round(calc_score(svm_linear_new_stats),3)
```
```{r message=FALSE, warning=FALSE, echo=FALSE,}
print(paste(paste0("linear: the whole dataset's score = ", svm_linear_stats$General),
            paste0("> the abridged dataset's score = ", svm_linear_new_stats$General)))
svm_stats["linear",]<-svm_linear_stats
```

A standard Gaussian Radial basis function (RBF)) is the second kernel we tried. We used the ksvm() function here. 

```{r svm_rbf, message=FALSE, warning=FALSE}
Class_classifier_rbf <-  ksvm(x=Class ~., data=data_train,kernel = "rbfdot")
```
```{r svm rbf, message=FALSE, warning=FALSE, echo=FALSE}
Class_predictions_rbf <- predict(Class_classifier_rbf, data_test[,-63])
svm_rbf <- confusionMatrix(Class_predictions_rbf,data_test[,63])


# calculate performance
df_rbf<-data.frame(predictions=c(as.numeric(Class_predictions_rbf)),labels = c(data_test$Class))
pROC_svm_rbf <- roc(df_rbf$labels,df_rbf$predictions)
svm_rbf_stats <- data.frame(  
                           Accuracy=svm_rbf[["overall"]][["Accuracy"]],
                           #AUC=pROC_svm_rbf[["auc"]],
                           Sensitivity=round(svm_rbf[["byClass"]] [["Sensitivity"]],3), 
                           Precision=round(svm_rbf[["byClass"]] [["Precision"]],3), 
                           Specificity=round(svm_rbf[["byClass"]][["Specificity"]],3))
svm_rbf_stats$General <- round(calc_score(svm_rbf_stats),3)

Class_classifier_rbf_new <-  ksvm(x=Class ~., data=data_train_new,kernel = "rbfdot")
Class_predictions_rbf_new <- predict(Class_classifier_rbf_new, data_test_new[,-63])
svm_rbf_new <- confusionMatrix(Class_predictions_rbf_new,data_test[,63])
df_rbf_new<-data.frame(predictions=c(as.numeric(Class_predictions_rbf_new)),labels = c(data_test$Class))
pROC_svm_rbf_new <- roc(df_rbf_new$labels,df_rbf_new$predictions)
svm_rbf_stats_new <- data.frame(  
                           Accuracy=svm_rbf_new[["overall"]][["Accuracy"]],
                           #AUC=pROC_svm_rbf_new[["auc"]],
                           Sensitivity=round(svm_rbf_new[["byClass"]] [["Sensitivity"]],3), 
                           Precision=round(svm_rbf_new[["byClass"]] [["Precision"]],3), 
                           Specificity=round(svm_rbf_new[["byClass"]][["Specificity"]],3))
svm_rbf_stats_new$General <- round(calc_score(svm_rbf_stats_new),3)
```
```{r message=FALSE, warning=FALSE, echo=FALSE,}
print(paste(paste0("RBF: the whole dataset's score = ", svm_rbf_stats$General),
            paste0("> the abridged dataset's score = ", svm_rbf_stats_new$General)))
svm_stats["RBF",]=svm_rbf_stats
```
```{r svm_poly, message=FALSE, warning=FALSE}
Class_classifier_poly <-  ksvm(x=Class ~., data=data_train,kernel = "polydot")
```
```{r SVM Polynomial, message=FALSE, warning=FALSE, echo=FALSE}
Class_predictions_poly <- predict(Class_classifier_poly, data_test[,-63])
svm_poly <- confusionMatrix(Class_predictions_poly,data_test[,63])

# calculate performance
df_poly<-data.frame(predictions=c(as.numeric(Class_predictions_poly)),labels = c(data_test$Class))
pROC_svm_poly <- roc(df_poly$labels,df_poly$predictions)
svm_poly_stats <- data.frame(
                           Accuracy=svm_poly[["overall"]][["Accuracy"]],
                           #AUC=pROC_svm_poly[["auc"]],
                           Sensitivity=round(svm_poly[["byClass"]] [["Sensitivity"]],3), 
                           Precision=round(svm_poly[["byClass"]] [["Precision"]],3), 
                           Specificity=round(svm_poly[["byClass"]][["Specificity"]],3))
svm_poly_stats$General<-round(calc_score(svm_poly_stats),3)

Class_classifier_poly_new <-  ksvm(x=Class ~., data=data_train_new,kernel = "polydot")
Class_predictions_poly_new <- predict(Class_classifier_poly_new, data_test_new[,-63])
svm_poly_new <- confusionMatrix(Class_predictions_poly_new,data_test[,63])
df_poly_new<-data.frame(predictions=c(as.numeric(Class_predictions_poly_new)),labels = c(data_test$Class))
pROC_svm_pol_newy <- roc(df_poly_new$labels,df_poly_new$predictions)
svm_poly_stats_new <- data.frame( 
                           Accuracy=svm_poly_new[["overall"]][["Accuracy"]],
                           #AUC=pROC_svm_poly[["auc"]],
                           Sensitivity=round(svm_poly_new[["byClass"]] [["Sensitivity"]], 3),
                           Precision=round(svm_poly_new[["byClass"]] [["Precision"]],3), 
                           Specificity=round(svm_poly_new[["byClass"]][["Specificity"]],3))

svm_poly_stats_new$General <- round(calc_score(svm_poly_stats_new),3)
```
```{r message=FALSE, warning=FALSE, echo=FALSE,}
print(paste(paste0("polynomial: the whole dataset's score = ", svm_poly_stats$General),
            paste0("> the abridged dataset's score = ", svm_poly_stats_new$General)))

svm_stats["Polynomial",]=svm_poly_stats 

```
```{r SVM Hyperbolic tangent, message=FALSE, warning=FALSE}
Class_classifier_tan <- ksvm(x=Class ~., data=data_train,kernel = "tanhdot")
```
```{r SVM tan, message=FALSE, warning=FALSE, echo=FALSE, results='hide'}
Class_predictions_tan <- predict(Class_classifier_tan, data_test[,-63])
svm_tan <- confusionMatrix(Class_predictions_tan,data_test[,63])

# calculate performance
df_tan<-data.frame(predictions=c(as.numeric(Class_predictions_tan)),labels = c(data_test$Class))
pROC_svm_tan <- roc(df_tan$labels,df_tan$predictions)
svm_tan_stats <- data.frame(Accuracy=svm_tan[["overall"]][["Accuracy"]],
                           #AUC=pROC_svm_tan[["auc"]],
                           Sensitivity=round(svm_tan[["byClass"]] [["Sensitivity"]], 3),
                           Precision=round(svm_tan[["byClass"]] [["Precision"]],3), 
                           Specificity=round(svm_tan[["byClass"]][["Specificity"]],3))

#new data hyperbolic
Class_classifier_tan_new <- ksvm(x=Class ~., data=data_train_new,kernel = "tanhdot")
Class_predictions_tan_new <- predict(Class_classifier_tan_new, data_test_new[,-63])
svm_tan_new <- confusionMatrix(Class_predictions_tan_new,data_test[,63])
svm_tan_stats_new <- data.frame(Accuracy=svm_tan_new[["overall"]][["Accuracy"]],
                           #AUC=pROC_svm_tan[["auc"]],
                           Sensitivity=round(svm_tan_new[["byClass"]] [["Sensitivity"]], 3),
                           Precision=round(svm_tan_new[["byClass"]] [["Precision"]],3), 
                           Specificity=round(svm_tan_new[["byClass"]][["Specificity"]],3))
svm_tan_stats$General <-round(calc_score(svm_tan_stats),3)
svm_tan_stats_new$General <-round(calc_score(svm_tan_stats_new),3)


svm_stats["Hyperbolic",]=svm_tan_stats_new

#print statistics
best_svm_stats_melt<-svm_stats %>% 
  select(Accuracy,Sensitivity,Precision,Specificity) %>%
  pivot_longer(., cols = c(Accuracy,Sensitivity,Precision,Specificity), 
               names_to = "statistics", values_to = "percentage")

plotted_svm<-ggplot(best_svm_stats_melt,aes(x = statistics, y = percentage, fill = method)) + labs(title = "svm statistics")+ geom_bar(width = 0.4,position = 'dodge', stat='identity') + theme(legend.text = element_text(size = 6),legend.position = "bottom", legend.title = element_text(face = "bold", size=6))

#grid.arrange(tableGrob(svm_stats[,-1]), plotted_svm, ncol=2, nrow=1)
```
```{r message=FALSE, warning=FALSE, echo=FALSE, results='hide', fig.show='hide'}
print(paste(paste0("hyperbolic: the whole dataset's score = ", svm_tan_stats$General),
            paste0("< the abridged dataset's score = ", svm_tan_stats_new$General)))
```
The results for the SVM with different kernels on automatically scaled data are:
```{r fig.width=7,echo=FALSE}
print.data.frame(svm_stats[2:3,])
```
```{r message=FALSE, warning=FALSE, echo=FALSE, results='hide', fig.show='hide'}
t_svm_stats <- tableGrob(svm_stats[2:3,])
gtitle_svm_stats <- textGrob("SVM statistics")
gtable_svm_stats <- gtable_add_rows(t_svm_stats, heights = grobHeight(gtitle_svm_stats)+ unit(5,"mm"), pos = 0)
gtable_svm_stats <- gtable_add_grob(gtable_svm_stats, gtitle_svm_stats, 1, 1, 1, ncol(gtable_svm_stats))
grid.arrange(gtable_svm_stats,ncol=1)
```
We got the same results when running the RBF as the default, and that is the best result.
The linear, polynomial and hyperbolic kernels also yielded the same results, with a slight decrease in accuracy (from 86% to 82%), sensitivity (from 84% to 76%) and precision (from 87.5% to 86.4%). In both cases the specificity is 88% which is good.
Other kernels might prove even better, or the cost of constraints parameter C could be varied to modify the width of the decision boundary.

```{r svm_best_stats_1, message=FALSE, warning=FALSE, echo=FALSE}
#We saved the best scores
svm_best_stats<-svm_stats["RBF",]
rownames(svm_best_stats)<-c("RBF")
```

## pre-scaled data
### normalized data
We tried using the data that we normalized and compared the 2. While using the 'ksvm' function we chose "scaled=FALSE"
```{r svm normalized linear, message=FALSE, warning=FALSE, results='hide'}
Class_classifier_linear_norm <-  ksvm(x=Class ~., data=train_set_norm,
                                      kernel = "vanilladot", scaled=FALSE)
```
```{r message=FALSE, warning=FALSE, results='hide', echo=FALSE}
Class_predictions_linear_norm <- predict(Class_classifier_linear_norm, test_set_norm)
svm_linear_norm <- confusionMatrix(Class_predictions_linear_norm,data_test[,63])
pROC_svm_linear_norm <- roc(c(data_test$Class), c(as.numeric(Class_predictions_linear_norm)))
svm_linear_norm_stats <- data.frame( 
                           Accuracy=svm_linear_norm[["overall"]][["Accuracy"]],
                           #AUC=pROC_svm_linear_norm[["auc"]],
                           Sensitivity=round(svm_linear_norm[["byClass"]][["Sensitivity"]], 3),
                           Precision=round(svm_linear_norm[["byClass"]][["Precision"]],3), 
                           Specificity=round(svm_linear_norm[["byClass"]][["Specificity"]],3))

Class_classifier_linear_norm_new <-  ksvm(x=Class ~., data=train_set_norm_new,
                                          kernel = "vanilladot", scaled=FALSE)
Class_predictions_linear_norm_new <- predict(Class_classifier_linear_norm_new, test_set_norm_new)
svm_linear_norm_new <- confusionMatrix(Class_predictions_linear_norm_new,data_test[,63])
pROC_svm_linear_norm_new <- roc(c(data_test$Class), c(as.numeric(Class_predictions_linear_norm_new)))
svm_linear_norm_stats_new <- data.frame( 
                           Accuracy=svm_linear_norm_new[["overall"]][["Accuracy"]],
                           #AUC=pROC_svm_linear_norm[["auc"]],
                           Sensitivity=round(svm_linear_norm_new[["byClass"]][["Sensitivity"]], 3),
                           Precision=round(svm_linear_norm_new[["byClass"]][["Precision"]],3), 
                           Specificity=round(svm_linear_norm_new[["byClass"]][["Specificity"]],3))

svm_linear_norm_stats$General <- round(calc_score(svm_linear_norm_stats),3)
svm_linear_norm_stats_new$General <- round(calc_score(svm_linear_norm_stats_new),3)
```
As before we ran the same kernels (on both the whole dataset and the selected part), but show here the process of the linear kernel only (the rest can be seen in our code).
```{r  message=FALSE, warning=FALSE, echo=FALSE}
print(paste(paste0("linear: the whole normalized dataset's score = ", svm_linear_norm_stats$General),
            paste0("> the abridged dataset's score = ", svm_linear_norm_stats_new$General)))
svm_norm_stats<-svm_linear_norm_stats
rownames(svm_norm_stats)<-"linear"
```
```{r svm normalized data, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}


# rbf
Class_classifier_rbf_norm <-  ksvm(x=Class ~., data=train_set_norm,kernel = "rbfdot", scaled=FALSE)
Class_predictions_rbf_norm <- predict(Class_classifier_rbf_norm, test_set_norm)
svm_rbf_norm <- confusionMatrix(Class_predictions_rbf_norm,data_test[,63])
pROC_svm_rbf_norm <- roc(c(data_test$Class), c(as.numeric(Class_predictions_rbf_norm)))
svm_rbf_norm_stats <- data.frame( 
                           Accuracy=svm_rbf_norm[["overall"]][["Accuracy"]],
                           #AUC=pROC_svm_rbf_norm[["auc"]],
                           Sensitivity=round(svm_rbf_norm[["byClass"]] [["Sensitivity"]], 3),
                           Precision=round(svm_rbf_norm[["byClass"]][["Precision"]],3), 
                        Specificity=round(svm_rbf_norm[["byClass"]][["Specificity"]],3))
svm_rbf_norm_stats$General<-calc_score(svm_rbf_norm_stats)

Class_classifier_rbf_norm_new <-  ksvm(x=Class ~., data=train_set_norm_new,kernel = "rbfdot", scaled=FALSE)
Class_predictions_rbf_norm_new <- predict(Class_classifier_rbf_norm_new, test_set_norm_new)
svm_rbf_norm_new <- confusionMatrix(Class_predictions_rbf_norm_new,data_test[,63])
pROC_svm_rbf_norm_new <- roc(c(data_test$Class), c(as.numeric(Class_predictions_rbf_norm_new)))
svm_rbf_norm_stats_new <- data.frame(
                           Accuracy=svm_rbf_norm_new[["overall"]][["Accuracy"]],
                           #AUC=pROC_svm_rbf_norm[["auc"]],
                           Sensitivity=round(svm_rbf_norm_new[["byClass"]] [["Sensitivity"]],3), 
                           Precision=round(svm_rbf_norm_new[["byClass"]][["Precision"]],3), 
                        Specificity=round(svm_rbf_norm_new[["byClass"]][["Specificity"]],3))

svm_rbf_norm_stats_new$General <- round(calc_score(svm_rbf_norm_stats_new),3)
```
```{r  message=FALSE, warning=FALSE, echo=FALSE}
print(paste(paste0("RBF: the whole normalized dataset's score = ", svm_rbf_norm_stats$General),
            paste0("> the abridged dataset's score = ", svm_rbf_norm_stats_new$General)))
svm_norm_stats["rbf",]<-svm_rbf_norm_stats
```
```{r  message=FALSE, warning=FALSE, echo=FALSE,results='hide'}
# polynomial
Class_classifier_poly_norm <-  ksvm(x=Class ~., data=train_set_norm,kernel = "polydot", scaled=FALSE)
Class_predictions_poly_norm <- predict(Class_classifier_poly_norm, test_set_norm)
svm_poly_norm <- confusionMatrix(Class_predictions_poly_norm,data_test[,63])
pROC_svm_poly_norm <- roc(c(data_test$Class), c(as.numeric(Class_predictions_poly_norm)))
svm_poly_norm_stats <- data.frame(
                           Accuracy=svm_poly_norm[["overall"]][["Accuracy"]],
                           #AUC=pROC_svm_poly_norm[["auc"]],
                           Sensitivity=round(svm_poly_norm[["byClass"]] [["Sensitivity"]],3), 
                           Precision=round(svm_poly_norm[["byClass"]][["Precision"]],3), 
                           Specificity=round(svm_poly_norm[["byClass"]][["Specificity"]],3))
svm_poly_norm_stats$General<-calc_score(svm_poly_norm_stats)

Class_classifier_poly_norm_new <-  ksvm(x=Class ~., data=train_set_norm_new,kernel = "polydot", scaled=FALSE)
Class_predictions_poly_norm_new <- predict(Class_classifier_poly_norm_new, test_set_norm_new)
svm_poly_norm_new <- confusionMatrix(Class_predictions_poly_norm_new,data_test[,63])
pROC_svm_poly_norm_new <- roc(c(data_test$Class), c(as.numeric(Class_predictions_poly_norm_new)))
svm_poly_norm_stats_new <- data.frame(
                           Accuracy=svm_poly_norm_new[["overall"]][["Accuracy"]],
                           #AUC=pROC_svm_poly_norm[["auc"]],
                           Sensitivity=round(svm_poly_norm_new[["byClass"]] [["Sensitivity"]],3), 
                           Precision=round(svm_poly_norm_new[["byClass"]][["Precision"]],3), 
                           Specificity=round(svm_poly_norm_new[["byClass"]][["Specificity"]],3))
svm_poly_norm_stats_new$General <-round(calc_score(svm_poly_norm_stats_new),3)
```
```{r  message=FALSE, warning=FALSE, echo=FALSE}
print(paste(paste0("polynomial: the whole normalized dataset's score is", svm_poly_norm_stats$General),
            paste0("> the abridged dataset's score = ", svm_poly_norm_stats_new$General)))
svm_norm_stats["poly",]<-svm_poly_norm_stats
```
```{r  message=FALSE, warning=FALSE, echo=FALSE, results='hide'}
# Hyperbolic tangent
Class_classifier_tan_norm <-  ksvm(x=Class ~., data=train_set_norm,kernel = "tanhdot", scaled=FALSE)
Class_predictions_tan_norm <- predict(Class_classifier_tan_norm, test_set_norm)
svm_tan_norm <- confusionMatrix(Class_predictions_tan_norm,data_test[,63])
pROC_svm_tan_norm <- roc(c(data_test$Class), c(as.numeric(Class_predictions_tan_norm)))
svm_tan_norm_stats <- data.frame(
                           Accuracy=svm_tan_norm[["overall"]][["Accuracy"]],
                           #AUC=pROC_svm_tan_norm[["auc"]],
                           Sensitivity=round(svm_tan_norm[["byClass"]] [["Sensitivity"]],3), 
                           Precision=round(svm_tan_norm[["byClass"]] [["Precision"]],3), 
                           Specificity=round(svm_tan_norm[["byClass"]][["Specificity"]],3))
svm_tan_norm_stats$General <- round(calc_score(svm_tan_norm_stats),3)

Class_classifier_tan_norm_new <-  ksvm(x=Class ~., data=train_set_norm_new,kernel = "tanhdot", scaled=FALSE)
Class_predictions_tan_norm_new <- predict(Class_classifier_tan_norm_new, test_set_norm_new)
svm_tan_norm_new <- confusionMatrix(Class_predictions_tan_norm_new,data_test[,63])
pROC_svm_tan_norm_new <- roc(c(data_test$Class), c(as.numeric(Class_predictions_tan_norm_new)))
svm_tan_norm_stats_new <- data.frame(
                           Accuracy=svm_tan_norm_new[["overall"]][["Accuracy"]],
                           #AUC=pROC_svm_tan_norm[["auc"]],
                           Sensitivity=round(svm_tan_norm_new[["byClass"]] [["Sensitivity"]], 3),
                           Precision=round(svm_tan_norm_new[["byClass"]] [["Precision"]],3), 
                           Specificity=round(svm_tan_norm_new[["byClass"]][["Specificity"]],3))

svm_tan_norm_stats_new$General <- round(calc_score(svm_tan_norm_stats_new),3)


```


```{r svm normalized data print, message=FALSE, warning=FALSE, echo=FALSE,results='hide'}
#print statistics
best_svm_norm_stats_melt<-svm_norm_stats %>% 
  select(Accuracy,Sensitivity,Precision,Specificity) %>%
  pivot_longer(., cols = c(Accuracy,Sensitivity,Precision,Specificity), 
               names_to = "statistics", values_to = "percentage")

plotted_svm_norm<-ggplot(best_svm_norm_stats_melt,aes(x = statistics, y = percentage, fill = method)) + geom_bar(position = 'dodge', stat='identity')+ labs(title = "normalized data- svm statistics")+ theme(legend.position="top")

#grid.arrange(plotted_svm_norm, tableGrob(svm_norm_stats[,-1]), nrow = 2, ncol = 1)
```
```{r message=FALSE, warning=FALSE, echo=FALSE, fig.height=3, fig.width=7}
print(paste(paste0("hyperbolic: the whole normalized dataset's score = ", svm_tan_norm_stats$General),
            paste0("< the abridged dataset's score = ", svm_tan_norm_stats_new$General)))
svm_norm_stats["tan, no corr",]<-svm_tan_norm_stats_new
```
The results for the SVM with different kernels on our normalized data are:
```{r fig.width=7,echo=FALSE}
print.data.frame(rbind(svm_norm_stats[-3,], "tan, all data"=svm_tan_norm_stats))
```
```{r message=FALSE, warning=FALSE, echo=FALSE, results='hide',fig.show='hide'}
t_svm_stats_norm <- tableGrob(svm_norm_stats[-3,])
gtitle_svm_stats_norm <- textGrob("SVM norm statistics")
gtable_svm_stats_norm <- gtable_add_rows(t_svm_stats_norm, heights = grobHeight(gtitle_svm_stats_norm)+ unit(5,"mm"), pos = 0)
gtable_svm_stats_norm <- gtable_add_grob(gtable_svm_stats_norm, gtitle_svm_stats_norm, 1, 1, 1, ncol(gtable_svm_stats_norm))
grid.arrange(gtable_svm_stats_norm,ncol=1)

svm_best_stats["RBF norm",]<-svm_norm_stats["rbf",]
```
Using the linear, RBF and Polynomial kernels we managed to got to specificity and precision of 1: all those predicted healthy were actually healthy. But it came at the price of sensitivity: the linear and polynomial yielded a sensitivity of 64%, and the RBF of 68%. 

Interestingly, the hyperbolic kernel on the whole dataset yielded the worst results by far, with sensitivity and precision of 0! the same kernel on the smaller dataset yielded a bit higher results, but still very bad ones (our genereal score jumped from 24% to 60.2%, but the other kernels are at 82% or 84%)

### z-score scaled
We tried scaling with z-score as well. The rest of the code is exactly as before so we did not print it in the report.
```{r  message=FALSE, warning=FALSE, results='hide'}
Class_classifier_linear_z <-  ksvm(x=Class ~., data=train_set_z,kernel = "vanilladot", scaled=FALSE)
```
```{r svm scaled data linear, message=FALSE, warning=FALSE, results='hide',echo=FALSE}
Class_predictions_linear_z <- predict(Class_classifier_linear_z, test_set_z)
svm_linear_z <- confusionMatrix(Class_predictions_linear_z,data_test[,63])
pROC_svm_linear_z <- roc(c(data_test$Class), c(as.numeric(Class_predictions_linear_z)))
svm_linear_z_stats <- data.frame(
                           Accuracy=svm_linear_z[["overall"]][["Accuracy"]],
                           #AUC=pROC_svm_linear_z[["auc"]],
                           Sensitivity=round(svm_linear_z[["byClass"]] [["Sensitivity"]], 3),
                           Precision=round(svm_linear_z[["byClass"]][["Precision"]],3), 
                           Specificity=round(svm_linear_z[["byClass"]][["Specificity"]],3))

Class_classifier_linear_z_new <-  ksvm(x=Class ~., data=train_set_z_new,kernel = "vanilladot", scaled=FALSE)
Class_predictions_linear_z_new <- predict(Class_classifier_linear_z_new, test_set_z_new)
svm_linear_z_new <- confusionMatrix(Class_predictions_linear_z,data_test[,63])
pROC_svm_linear_z_new <- roc(c(data_test$Class), c(as.numeric(Class_predictions_linear_z_new)))
svm_linear_z_new_stats <- data.frame(
                           Accuracy=svm_linear_z_new[["overall"]][["Accuracy"]],
                           #AUC=pROC_svm_linear_z_new[["auc"]],
                           Sensitivity=round(svm_linear_z_new[["byClass"]] [["Sensitivity"]], 3),
                           Precision=round(round(svm_linear_z_new[["byClass"]][["Precision"]],3), 3),
                           Specificity=round(svm_linear_z_new[["byClass"]][["Specificity"]],3)) 
svm_linear_z_stats$General <- round(calc_score(svm_linear_z_stats),3)
svm_linear_z_new_stats$General <- round(calc_score(svm_linear_z_new_stats),3)

print(paste(paste0("the whole z-scaled dataset's score (linear) is ", svm_linear_z_stats$General),
            paste0("; the abridged dataset's score = ", svm_linear_z_new_stats$General)))
```
As before we ran the same kernels, but show here the process of the linear kernel only (the rest can be seen in our code).
```{r svm scaled data, message=FALSE, warning=FALSE, echo=FALSE, results='hide'}
svm_z_stats<-svm_linear_z_stats
rownames(svm_z_stats)<-c("linear")

# rbf
Class_classifier_rbf_z <-  ksvm(x=Class ~., data=train_set_z,kernel = "rbfdot", scaled=FALSE)
Class_predictions_rbf_z <- predict(Class_classifier_rbf_z, test_set_z)
svm_rbf_z <- confusionMatrix(Class_predictions_rbf_z,data_test[,63])
pROC_svm_rbf_z <- roc(c(data_test$Class), c(as.numeric(Class_predictions_rbf_z)))
svm_rbf_z_stats <- data.frame( 
                           Accuracy=svm_rbf_z[["overall"]][["Accuracy"]],
                           #AUC=pROC_svm_rbf_z[["auc"]],
                           Sensitivity=round(svm_rbf_z[["byClass"]] [["Sensitivity"]],3), 
                           Precision=round(svm_rbf_z[["byClass"]] [["Precision"]],3), 
                           Specificity=round(svm_rbf_z[["byClass"]] [["Specificity"]],3)) 

Class_classifier_rbf_z_new <-  ksvm(x=Class ~., data=train_set_z_new,kernel = "rbfdot", scaled=FALSE)
Class_predictions_rbf_z_new <- predict(Class_classifier_rbf_z_new, test_set_z_new)
svm_rbf_z_new <- confusionMatrix(Class_predictions_rbf_z_new,data_test[,63])
pROC_svm_rbf_z_new <- roc(c(data_test$Class), c(as.numeric(Class_predictions_rbf_z_new)))
svm_rbf_z_stats_new <- data.frame(
                           Accuracy=svm_rbf_z_new[["overall"]][["Accuracy"]],
                           #AUC=pROC_svm_rbf_z[["auc"]],
                           Sensitivity=round(svm_rbf_z_new[["byClass"]] [["Sensitivity"]],3), 
                           Precision=round(svm_rbf_z_new[["byClass"]] [["Precision"]],3), 
                           Specificity=round(svm_rbf_z_new[["byClass"]] [["Specificity"]],3)) 

svm_rbf_z_stats$General <- round(calc_score(svm_rbf_z_stats),3)
svm_rbf_z_stats_new$General <- round(calc_score(svm_rbf_z_stats_new),3)

print(paste(paste0("the whole z-scaled dataset's score (RBF) is ", svm_rbf_z_stats$General),
            paste0("; the abridged dataset's score = ", svm_rbf_z_stats_new$General)))

svm_z_stats["rbf",]<-svm_rbf_z_stats


# polynomial
Class_classifier_poly_z <-  ksvm(x=Class ~., data=train_set_z,kernel = "polydot", scaled=FALSE)
Class_predictions_poly_z <- predict(Class_classifier_poly_z, test_set_z)
svm_poly_z <- confusionMatrix(Class_predictions_poly_z,data_test[,63])
pROC_svm_poly_z <- roc(c(data_test$Class), c(as.numeric(Class_predictions_poly_z)))
svm_poly_z_stats <- data.frame(  
                           Accuracy=svm_poly_z[["overall"]][["Accuracy"]],
                           #AUC=pROC_svm_poly_z[["auc"]],
                           Sensitivity=round(svm_poly_z[["byClass"]] [["Sensitivity"]], 3),
                           Precision=round(svm_poly_z[["byClass"]][["Precision"]],3), 
                           Specificity=round(svm_poly_z[["byClass"]][["Specificity"]],3)) 

Class_classifier_poly_z_new <-  ksvm(x=Class ~., data=train_set_z_new,kernel = "polydot", scaled=FALSE)
Class_predictions_poly_z_new <- predict(Class_classifier_poly_z_new, test_set_z_new)
svm_poly_z_new <- confusionMatrix(Class_predictions_poly_z_new,data_test[,63])
pROC_svm_poly_z_new <- roc(c(data_test$Class), c(as.numeric(Class_predictions_poly_z_new)))
svm_poly_z_stats_new <- data.frame(  
                           Accuracy=svm_poly_z_new[["overall"]][["Accuracy"]],
                           #AUC=pROC_svm_poly_z[["auc"]],
                           Sensitivity=round(svm_poly_z_new[["byClass"]] [["Sensitivity"]], 3),
                           Precision=round(svm_poly_z_new[["byClass"]][["Precision"]],3), 
                           Specificity=round(svm_poly_z_new[["byClass"]][["Specificity"]],3)) 
svm_poly_z_stats$General <- round(calc_score(svm_poly_z_stats),3)
svm_poly_z_stats_new$General <- round(calc_score(svm_poly_z_stats_new),3)

print(paste(paste0("the whole z-scaled dataset's score (polinomial) is ", svm_poly_z_stats$General),
            paste0("; the abridged dataset's score = ", svm_poly_z_stats_new$General)))
svm_z_stats["poly",]<-svm_poly_z_stats

# Hyperbolic tangent
Class_classifier_tan_z <-  ksvm(x=Class ~., data=train_set_z,kernel = "tanhdot", scaled=FALSE)
Class_predictions_tan_z <- predict(Class_classifier_tan_z, test_set_z)
svm_tan_z <- confusionMatrix(Class_predictions_tan_z,data_test[,63])
pROC_svm_tan_z <- roc(c(data_test$Class), c(as.numeric(Class_predictions_tan_z)))
svm_tan_z_stats <- data.frame(  
                           Accuracy=svm_tan_z[["overall"]][["Accuracy"]],
                           #AUC=pROC_svm_tan_z[["auc"]],
                           Sensitivity=round(svm_tan_z[["byClass"]] [["Sensitivity"]], 3),
                           Precision=round(svm_tan_z[["byClass"]] [["Precision"]],3), 
                           Specificity=round(svm_tan_z[["byClass"]][["Specificity"]],3)) 

Class_classifier_tan_z_new <-  ksvm(x=Class ~., data=train_set_z_new,kernel = "tanhdot", scaled=FALSE)
Class_predictions_tan_z_new <- predict(Class_classifier_tan_z_new, test_set_z_new)
svm_tan_z_new <- confusionMatrix(Class_predictions_tan_z_new,data_test[,63])
pROC_svm_tan_z_new <- roc(c(data_test$Class), c(as.numeric(Class_predictions_tan_z_new)))
svm_tan_z_stats_new <- data.frame(  
                           Accuracy=svm_tan_z_new[["overall"]][["Accuracy"]],
                           #AUC=pROC_svm_tan_z[["auc"]],
                           Sensitivity=round(svm_tan_z_new[["byClass"]] [["Sensitivity"]], 3),
                           Precision=round(svm_tan_z_new[["byClass"]] [["Precision"]],3), 
                           Specificity=round(svm_tan_z_new[["byClass"]][["Specificity"]],3)) 
svm_tan_z_stats$General <- calc_score(svm_tan_z_stats)
svm_tan_z_stats_new$General <- calc_score(svm_tan_z_stats_new)


print(paste(paste0("the whole z-scaled dataset's score (hyperbolic) is ", svm_tan_z_stats$General),
            paste0("; the abridged dataset's score = ", svm_tan_z_stats_new$General)))
svm_z_stats["tan, no corr",]<-svm_tan_z_stats_new
```
The results for the SVM with different kernels on z-scaled data are:
```{r fig.width=7,echo=FALSE}
print.data.frame(svm_z_stats)
svm_best_stats["RBF z-scaled",]<-svm_z_stats["rbf",]
```
```{r warning=FALSE, message=FALSE, echo=FALSE, results='hide'}
#print statistics
best_svm_z_stats_melt<-svm_z_stats %>% 
  select(Accuracy,Sensitivity,Precision,Specificity) %>%
  pivot_longer(., cols = c(Accuracy,Sensitivity,Precision,Specificity), 
               names_to = "statistics", values_to = "percentage")

plotted_svm_z<-ggplot(best_svm_z_stats_melt,aes(x = statistics, y = percentage, fill = method)) + geom_bar(position = 'dodge', stat='identity')+ labs(title = "z scaled data- svm statistics")+ theme(legend.position="top")

#grid.arrange( plotted_svm_z, tableGrob(svm_z_stats[,-1]),nrow = 2, ncol = 1)

#t_svm_stats_z <- tableGrob(svm_z_stats[-4,])
#gtitle_svm_stats_z <- textGrob("SVM z-scaled statistics")
#gtable_svm_stats_z <- gtable_add_rows(t_svm_stats_z, heights = grobHeight(gtitle_svm_stats_z)+ unit(5,"mm"), pos = 0)
#gtable_svm_stats_z <- gtable_add_grob(gtable_svm_stats_z, gtitle_svm_stats_z, 1, 1, 1, ncol(gtable_svm_stats_z))
#grid.arrange(gtable_svm_stats_z,ncol=1)

```


## SVM conclusions

```{r fig.width=7,echo=FALSE}
print.data.frame(svm_best_stats)
```
We saw that out of all the kernels, the linear one gave the best result, which is what it looked like when we looked at the PCA graphs. We also saw that scaling with our normalization function yielded worse results, and our z-score scaling yielded the same result. 
```{r message=FALSE, warning=FALSE, echo=FALSE, fig.height=3, fig.width=7, results='hide', fig.show='hide'}

#Add the best scores of the normalized

t_svm_stats_best <- tableGrob(svm_best_stats)
gtitle_svm_stats_best <- textGrob("SVM best statistics")
gtable_svm_stats_best <- gtable_add_rows(t_svm_stats_best, heights = grobHeight(gtitle_svm_stats_best)+ unit(5,"mm"), pos = 0)
gtable_svm_stats_best <- gtable_add_grob(gtable_svm_stats_best, gtitle_svm_stats_best, 1, 1, 1, ncol(gtable_svm_stats_best))

t_svm_rbf_z <- tableGrob(svm_rbf_z[["table"]])
gtitle_svm_rbf_z <- textGrob("RBF confusion matrix")
gtable_svm_rbf_z <- gtable_add_rows(t_svm_rbf_z, heights = grobHeight(gtitle_svm_rbf_z)+ unit(5,"mm"), pos = 0)
gtable_svm_rbf_z <- gtable_add_grob(gtable_svm_rbf_z, gtitle_svm_rbf_z, 1, 1, 1, ncol(gtable_svm_rbf_z))

grid.arrange(gtable_svm_stats_best, gtable_svm_rbf_z ,nrow=2)
```
We saved the SVM results of our normalized data with a linear kernel to compare with the rest of the algorithms.
```{r svm save best, message=FALSE, warning=FALSE}
best_stats<-cbind(svm_best_stats["RBF",], notes="RBF")
rownames(best_stats)<-"SVM"
```

# K-Nearest Neighbors (KNN) algorithm
The KNN algorithm is a non-parametric supervised learning method, used for classification and regression (we used it for classification). It is based on the assumption that similar objects exist in close proximity (are near to each other). K is a positive integer (a user-defined constant) that represents the number of training samples. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class which is most frequent among the k training samples nearest to that query point.

## normalized data
### choose K's manually
```{r pred_knn_3, message=FALSE, warning=FALSE, results='hide'}
pred_knn_3 <- knn(train = train_set_norm[,-63], test = test_set_norm[,-63], cl = data_train[,63], k=3)
knn_table_3<-CrossTable(x = data_test[,63], y = pred_knn_3, prop.chisq=FALSE)[["t"]]
knn_seperate_cv_stats <- data.frame(k=3, Accuracy=accuracy(knn_table_3)$.estimate,
                           Sensitivity=round(sensitivity(knn_table_3),3), 
                           Precision=round(precision(knn_table_3),3), 
                           Specificity=round(specificity(knn_table_3),3))
```
We chose and tried k=3,9,11,21, but we showed here just for k=3
```{r manual k choice, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}

pred_knn_9 <- knn(train = train_set_norm[,-63], test = test_set_norm[,-63], cl = data_train[,63], k=9)
knn_table_9<-CrossTable(x = data_test[,63], y = pred_knn_9, prop.chisq=FALSE)[["t"]]

pred_knn_11 <- knn(train = train_set_norm[,-63], test = test_set_norm[,-63], cl = data_train[,63], k=11)
knn_table_11<-CrossTable(x = data_test[,63], y = pred_knn_11, prop.chisq=FALSE)[["t"]]

pred_knn_21 <- knn(train = train_set_norm[,-63], test = test_set_norm[,-63], cl = data_train[,63], k=21)
knn_table_21<-CrossTable(x = data_test[,63], y = pred_knn_21, prop.chisq=FALSE)[["t"]]

knn_seperate_cv_stats <- data.frame(
                           Accuracy=c(accuracy(knn_table_3)$.estimate,
                                    accuracy(knn_table_9)$.estimate,
                                    accuracy(knn_table_11)$.estimate,
                                    accuracy(knn_table_21)$.estimate),
                           Sensitivity=c(round(sensitivity(knn_table_3),3),
                                       round(sensitivity(knn_table_9),3),
                                       round(sensitivity(knn_table_11),3),
                                       round(sensitivity(knn_table_21),3)), 
                           Precision=c(round(precision(knn_table_3),3),
                                       round(precision(knn_table_9),3),
                                       round(precision(knn_table_11),3),
                                       round(precision(knn_table_21),3)), 
                           Specificity=c(round(specificity(knn_table_3),3),
                                       round(specificity(knn_table_9),3),
                                       round(specificity(knn_table_11),3),
                                       round(specificity(knn_table_21),3)),
                           other=c("k=3","k=9","k=11","k=21"))
knn_seperate_cv_stats$General <- round(calc_score(knn_seperate_cv_stats),3)
knn_seperate_cv_stats
best_knn_seperate_cv_stats<-knn_seperate_cv_stats[3,]
```
The best results we got was with k=11, with accuracy of 76%, sensitivity of 74.1% and specificy of 78.3% (precision was 80% like k=9 and k=21). We again used cv with k=11, but this time on the selected features only.
```{r message=FALSE, warning=FALSE, echo=FALSE,results='hide'}
pred_knn_11_new <- knn(train = train_set_norm_new[,-21], test = test_set_norm_new[,-21], cl = data_train[,63], k=11)
knn_table_11_new<-CrossTable(x = data_test[,63], y = pred_knn_11, prop.chisq=FALSE)[["t"]]
knn_seperate_cv_stats_new <- data.frame(Accuracy=accuracy(knn_table_11_new)$.estimate,
                           Sensitivity=round(sensitivity(knn_table_11_new),3), 
                           Precision=round(precision(knn_table_11_new),3), 
                           Specificity=round(specificity(knn_table_11_new),3),
                           "k=11")
knn_seperate_cv_stats_new$General <- round(calc_score(knn_seperate_cv_stats_new),3)
```
```{r message=FALSE, warning=FALSE, echo=FALSE, fig.width=6, fig.height=3}
print(paste(paste0("the whole dataset's score (with k=11) is ", best_knn_seperate_cv_stats$General),
            paste0("; the abridged dataset's score is ", knn_seperate_cv_stats$General)))
best_knn_stats<-best_knn_seperate_cv_stats
rownames(best_knn_stats)<-c("manually chosen K's")
```
There was no difference between the two.

Instead of choosing the K's manually we can use tuneGrid and find the best k in the range of 1-30

### choose K's with Caret's "train"
#### cross validation:
We trained the algorithm on the normalized training set with the 'train' function of the 'caret' library with different options of K (in the range of 1-30). We started with cross validation. 
```{r message=FALSE, warning=FALSE}
knn_train_cv<-train(Class ~ .,train_set_norm, method = "knn",
                       trControl = trainControl(method = "cv", number = 10),
                       tuneGrid = data.frame(k=seq(1,30,2)),
                       metric = "Kappa")
```
We used confusionMatrix to check how well we did (checked on both the train and the test sets).
```{r message=FALSE, warning=FALSE}
knn_predictions_cv<-predict(knn_train_cv, test_set_norm)
knn_cv_mat <- confusionMatrix(knn_predictions_cv,data_test[,63])
```

```{r KNN cross validation stats, message=FALSE, warning=FALSE, echo=FALSE}
knn_cv_stats<- data.frame( 
                           Accuracy=knn_cv_mat[["overall"]][["Accuracy"]],
                           Sensitivity=round(knn_cv_mat[["byClass"]][["Sensitivity"]],3), 
                           Precision=round(knn_cv_mat[["byClass"]][["Precision"]],3), 
                           Specificity=round(knn_cv_mat[["byClass"]][["Specificity"]],3),
                           K=paste0("k=",knn_train_cv[["bestTune"]][["k"]]))
 
knn_train_cv_new<-train(Class ~ .,train_set_norm_new,
                       method = "knn",
                       trControl = trainControl(method = "cv", number = 10),
                       tuneGrid = data.frame(k=seq(1,30,2)),
                       metric = "Kappa")
knn_predictions_cv_new<-predict(knn_train_cv_new, test_set_norm_new)
knn_cv_mat_new <- confusionMatrix(knn_predictions_cv_new,data_test[,63])
knn_cv_new_stats<- data.frame( 
                           Accuracy=knn_cv_mat_new[["overall"]][["Accuracy"]],
                           Sensitivity=round(knn_cv_mat_new[["byClass"]][["Sensitivity"]], 3),
                           Precision=round(knn_cv_mat_new[["byClass"]][["Precision"]],3), 
                           Specificity=round(knn_cv_mat_new[["byClass"]][["Specificity"]],3),
                           K=paste0("k=3"))
knn_cv_stats$General<-round(calc_score(knn_cv_stats),3)
knn_cv_new_stats$General<-round(calc_score(knn_cv_new_stats),3)
print(paste(paste0("the whole dataset's score using cross validation is ", knn_cv_stats$General ),
            paste0("; the abridged dataset's score is ", knn_cv_new_stats$General)))
```
We then looiked at the statistics
```{r echo=FALSE}
print.data.frame(knn_cv_new_stats)
best_knn_stats["cross validation, no corr",]<-knn_cv_new_stats
```
We again did the same thing just on the dataset that didn't include high correlations. The overall grade was higher for the dataset that didn't include high correlations.
Compared to the normalized manually chosen data we have an increase in the sensitivity (74.1%->80%), but a significant decrease in the rest (the accuracy went from 76% to 72%, the precision from 80% to 69% and the specificity from 78.3% to 64%).


#### repeated cross validation:
We wanted to see if using repeated cross validation will yield better results.
```{r knn_train_repeatedcv, message=FALSE, warning=FALSE}
knn_train_repeatedcv<-train(Class ~.,train_set_norm,
                       method = "knn",
                       tuneGrid = data.frame(k=seq(1,30,2)),
                       trControl = trainControl(method = "repeatedcv", number = 10, repeats = 10),
                       metric = "Kappa")
```

```{r knn_predictions_rep_cv, message=FALSE, warning=FALSE, echo=FALSE}
knn_predictions_rep_cv<-predict(knn_train_repeatedcv, test_set_norm)
knn_repcv_mat <- confusionMatrix(knn_predictions_rep_cv,data_test[,63],dnn = c('predicted', 'actual'))
knn_df_norm_rep_cv<-data.frame(predictions=c(as.numeric(knn_predictions_rep_cv)),labels = c(data_test[,63]))
knn_repeatedcv_stats<- data.frame(
                           Accuracy=knn_repcv_mat[["overall"]][["Accuracy"]],
                           Sensitivity=round(knn_repcv_mat[["byClass"]] [["Sensitivity"]],3), 
                           Precision=round(knn_repcv_mat[["byClass"]][["Precision"]],3), 
                           Specificity=round(knn_repcv_mat[["byClass"]][["Specificity"]],3),
                           K=paste0("k=", knn_train_repeatedcv[["bestTune"]][["k"]])) 
#knn_repcv_mat[["table"]]
#grid.arrange(tableGrob(knn_repcv_mat[["table"]]))
#grid.arrange(tableGrob(best_knn_stats))
```


```{r warning=FALSE, message=FALSE, echo=FALSE, results='hide'}
knn_train_repeatedcv_new<-train(Class ~.,train_set_norm_new,
                       method = "knn",
                       tuneGrid = data.frame(k=seq(1,30,2)),
                       trControl = trainControl(method = "repeatedcv", number = 10, repeats = 10),
                       metric = "Kappa")
knn_predictions_rep_cv_new<-predict(knn_train_repeatedcv_new, test_set_norm_new)
knn_repcv_mat_new <- confusionMatrix(knn_predictions_rep_cv_new,data_test[,63],dnn = c('predicted', 'actual'))
knn_df_norm_rep_cv_new<-data.frame(predictions=c(as.numeric(knn_predictions_rep_cv_new)),labels = c(data_test[,63]))
knn_repeatedcv_new_stats <- data.frame(
                           Accuracy=knn_repcv_mat_new[["overall"]][["Accuracy"]],
                           Sensitivity=round(knn_repcv_mat_new[["byClass"]] [["Sensitivity"]],3), 
                           Precision=round(knn_repcv_mat_new[["byClass"]][["Precision"]],3), 
                           Specificity=round(knn_repcv_mat_new[["byClass"]][["Specificity"]],3),
                           K=paste0("k=", knn_train_repeatedcv_new[["bestTune"]][["k"]])) 
knn_repeatedcv_stats$General<-round(calc_score(knn_repeatedcv_stats),3)
knn_repeatedcv_new_stats$General<-round(calc_score(knn_repeatedcv_new_stats),3)

```
```{r echo=FALSE}
print(paste(paste0("the whole dataset's score using repeated CV is ",knn_repeatedcv_stats$General),
            paste0(", and without highly correlated features it is ",knn_repeatedcv_new_stats$General)))
best_knn_stats["repeated cross validation",]<-knn_repeatedcv_stats
```
This time there was no difference between the 2 datasets.

We saw no change on most aspects of our statistics, but there was a small decrease in precision from 69% to 68.97%, so we will continue to use cross validation

## z-score standardization 

### manually

We then tried with z score scaling instead of normalization. Again we started by choosing manually, and tried k=3,9,11,21, though we show here just for k=3
```{r pred_knn_3_z, message=FALSE, warning=FALSE, results='hide'}
pred_knn_3_z <- knn(train = train_set_z[,-63], test = test_set_z[,-63], cl = data_train[,63], k=3)
knn_table_3_z<-CrossTable(x = data_test[,63], y = pred_knn_3_z, prop.chisq=FALSE)[["t"]]
```
```{r knn k choice z, message=FALSE, warning=FALSE, echo=FALSE, results='hide'}
pred_knn_9_z <- knn(train = train_set_z[,-63], test = test_set_z[,-63], cl = data_train[,63], k=9)
knn_table_9_z<-CrossTable(x = data_test[,63], y = pred_knn_9_z, prop.chisq=FALSE)[["t"]]

pred_knn_11_z <- knn(train = train_set_z[,-63], test = test_set_z[,-63], cl = data_train[,63], k=11)
knn_table_11_z<-CrossTable(x = data_test[,63], y = pred_knn_11_z, prop.chisq=FALSE)[["t"]]

pred_knn_21_z <- knn(train = train_set_z[,-63], test = test_set_z[,-63], cl = data_train[,63], k=21)
knn_table_21_z<-CrossTable(x = data_test[,63], y = pred_knn_21_z, prop.chisq=FALSE)[["t"]]

knn_seperate_cv_stats_z <- data.frame(
                           Accuracy=c(accuracy(knn_table_3_z)$.estimate,
                                    accuracy(knn_table_9_z)$.estimate,
                                    accuracy(knn_table_11_z)$.estimate,
                                    accuracy(knn_table_21_z)$.estimate),
                           Sensitivity=c(round(sensitivity(knn_table_3_z),3),
                                       round(sensitivity(knn_table_9_z),3),
                                       round(sensitivity(knn_table_11_z),3),
                                       round(sensitivity(knn_table_21_z),3)), 
                           Precision=c(round(precision(knn_table_3_z),3),
                                       round(precision(knn_table_9_z),3),
                                       round(precision(knn_table_11_z),3),
                                       round(precision(knn_table_21_z),3)), 
                           Specificity=c(round(specificity(knn_table_3_z),3),
                                       round(specificity(knn_table_9_z),3),
                                       round(specificity(knn_table_11_z),3),
                                       round(specificity(knn_table_21_z),3)),
                           K=c("k=3","k=9","k=11","k=21"))
print.data.frame(knn_seperate_cv_stats_z)
knn_seperate_cv_stats_z_best<-knn_seperate_cv_stats_z[2,]
```
We got the best result from k=9. We wanted again to check if we can get better results with the smaller dataset. We decided to run it only with k=9, since that was the best result.

```{r message=FALSE, warning=FALSE, echo=FALSE, results='hide'}
pred_knn_9_z_new <- knn(train = train_set_z_new[,-21], test = test_set_z_new[,-21], cl = data_train[,63], k=9)
knn_table_9_z_new<-CrossTable(x = data_test[,63], y = pred_knn_9_z_new, prop.chisq=FALSE)[["t"]]
knn_seperate_cv_stats_z_new <- data.frame(Accuracy=accuracy(knn_table_9_z_new)$.estimate,
                           Sensitivity=round(sensitivity(knn_table_9_z_new),9), 
                           Precision=round(precision(knn_table_9_z_new),9), 
                           Specificity=round(specificity(knn_table_9_z_new),9),k="k=9")

knn_seperate_cv_stats_z_best$General<-round(calc_score(knn_seperate_cv_stats_z_best),3)
knn_seperate_cv_stats_z_new$General<-round(calc_score(knn_seperate_cv_stats_z_new),3)
print(paste(paste0("the whole z-scaled dataset's score with k=9 is ",knn_seperate_cv_stats_z_best$General),
            paste0("; the abridged dataset's score is ",knn_seperate_cv_stats_z_new$General)))
```
We indeed got (slightly) better results out of the smaller dataset, so that is what we saved.
```{r message=FALSE, warning=FALSE}
print.data.frame(knn_seperate_cv_stats_z_new)
best_knn_stats["manually chosen, scaled, no corr",] = knn_seperate_cv_stats_z_new
```

We saw that the results were much better using the z-score scaling. The best result was from k=9, with accuracy of 82% (rather than 76% or 72% from the normalized data choosing manually or with Caret respectively), sensitivity of 86.4% rather than 74.1% or 80% from the normalized data choosing manually or with Caret respectively) and specificity of 78.6% (rather than 78.3% or 64% from the normalized data choosing manually or with Caret respectively). The 76% precision was higher than the Caret chosen K's (~69%) but lower than the manually chosen K's (80%). All in all we thinl that using z-score to scale did a better job than our normalization. 
We then proceeded to check the cress validation using Caret again but on scaled data

### cross validation on scaled data
```{r knn z score train, message=FALSE, warning=FALSE, results='hide'}
knn_train_repeatedcv_z<-train(train_set_z[,-63],data_train[,63],
                       method = "knn",
                       tuneGrid = data.frame(k=seq(1,30,2)),
                       trControl = trainControl(method = "cv", number = 10, repeats = 10),
                       metric = "Kappa")
```
```{r knn z score, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
knn_predictions_rep_cv_z<-predict(knn_train_repeatedcv_z, test_set_z[,-63])
knn_repcv_mat_z <- confusionMatrix(knn_predictions_rep_cv_z,data_test[,63])

knn_z_table<-CrossTable(x = data_test[,63], y = knn_predictions_rep_cv_z, prop.chisq=FALSE)
df_z<-data.frame(predictions=c(as.numeric(knn_predictions_rep_cv_z)),labels = c(data_test[,63]))
knn_cv_stats_z <- data.frame(
                          Accuracy=knn_repcv_mat_z[["overall"]][["Accuracy"]],
                           Sensitivity=round(knn_repcv_mat_z[["byClass"]][["Sensitivity"]], 3),
                           Precision=round(knn_repcv_mat_z[["byClass"]][["Precision"]],3), 
                           Specificity=round(knn_repcv_mat_z[["byClass"]][["Specificity"]],3),
                           K=paste0("k=", knn_train_repeatedcv_z[["bestTune"]][["k"]])) 
```
```{r message=FALSE, warning=FALSE, echo=FALSE,results='hide'}
knn_train_repeatedcv_z_new<-train(train_set_z_new[,-21],data_train[,63],
                       method = "knn",
                       tuneGrid = data.frame(k=seq(1,30,2)),
                       trControl = trainControl(method = "cv", number = 10, repeats = 10),
                       metric = "Kappa")

knn_predictions_rep_cv_z_new<-predict(knn_train_repeatedcv_z_new, test_set_z_new[,-21])
knn_repcv_mat_z_new <- confusionMatrix(knn_predictions_rep_cv_z_new,data_test[,63])

#knn_z_table1<-CrossTable(x = data_test[,63], y = knn_predictions_rep_cv_z_new, prop.chisq=FALSE)
df_z_new<-data.frame(predictions=c(as.numeric(knn_predictions_rep_cv_z_new)),labels = c(data_test[,63]))
knn_cv_stats_z_new <- data.frame(
                          Accuracy=knn_repcv_mat_z_new[["overall"]][["Accuracy"]],
                           Sensitivity=round(knn_repcv_mat_z_new[["byClass"]][["Sensitivity"]],3), 
                           Precision=round(knn_repcv_mat_z_new[["byClass"]][["Precision"]],3), 
                           Specificity=round(knn_repcv_mat_z_new[["byClass"]][["Specificity"]],3),
                           K=paste0("k=", knn_train_repeatedcv_z_new[["bestTune"]][["k"]])) 
knn_cv_stats_z$General<-round(calc_score(knn_cv_stats_z),3)
knn_cv_stats_z_new$General<-round(calc_score(knn_cv_stats_z_new),3)

```
Again we got better results out of the smaller dataset.

```{r message=FALSE, warning=FALSE, echo=FALSE}
print(paste(paste0("the whole z-scaled dataset's score using CV is ",knn_cv_stats$General),
            paste0("; the abridged dataset's score is ",knn_cv_stats_z_new$General)))
best_knn_stats["cross validation, scaled, no corr",] = knn_cv_stats_z_new
print.data.frame(knn_cv_stats_z_new)
```

Compared to cross validation on normalized data we have increased accuracy (72%->78%), precision (69%->81.8%) and specificity (64%->84%), but decreased sensitivity (80%->72%).
Compared to manually chosen K's on z-scaled data we have increased precision (76%->81.8%) and specificity (78.6%->84%), but decreased accuracy (82%->78%) and sensitivity (86.4%->72%).

## knn conclusions
By looking at the confusion matrices of both the manually and Caret chosen K's 
```{r message=FALSE,warning=FALSE, echo=FALSE}
best_knn_stats[,"method"]<-rownames(best_knn_stats)
best_knn_stats_melt<-best_knn_stats %>% 
  select(Accuracy,Sensitivity,Precision,Specificity) %>%
  pivot_longer(., cols = c(Accuracy,Sensitivity,Precision,Specificity), 
               names_to = "statistics", values_to = "percentage")
plotted_knn<-ggplot(best_knn_stats_melt,aes(x=statistics, y=percentage, fill=method)) +
  geom_bar(position = 'dodge', stat='identity')+ theme(legend.position="top")+ labs(title = "best knn statistics")
#grid.arrange( plotted_knn, tableGrob(best_knn_stats[,-6]),nrow = 2, ncol = 1)
```
```{r message=FALSE,warning=FALSE, echo=FALSE}
print.data.frame(best_knn_stats)
t_knn_9_z <- tableGrob(knn_table_9_z)
gtitle_knn_9_z <- textGrob("manually chosen (k=9)")
gtable_knn_9_z <- gtable_add_rows(t_knn_9_z, heights = grobHeight(gtitle_knn_9_z)+ unit(5,"mm"), pos = 0)
gtable_knn_9_z <- gtable_add_grob(gtable_knn_9_z, gtitle_knn_9_z, 1, 1, 1, ncol(gtable_knn_9_z))

t_cv_z <- tableGrob(knn_z_table[["t"]])
title_cv_z <- textGrob("cross validation (z-score scaled)")
table_cv_z <- gtable_add_rows( t_cv_z,heights = grobHeight(title_cv_z) + unit(5,"mm"), pos = 0)
table_cv_z <- gtable_add_grob( table_cv_z, title_cv_z, 1, 1, 1, ncol(table_cv_z))

#confusion matrices
grid.arrange(gtable_knn_9_z,table_cv_z,ncol=2)
#grid.newpage()
#grid.draw(table_cv_z)

best_stats["KNN",]<-cbind(best_knn_stats["linear norm",], notes="linear norm")
```

We did not see any difference whether we used the crossed validation or the repeated cross validation.
We did see a difference when we used the Z score to scale instead of normalization: we gotincreased accuracy/AUC (from 72% to 84%) and precision (from 67.7% to 90.4%!), but at the cost of the sensitivity/recall (which decreased from 84% to 76%).
The manually chosen (k=9) had one less false negative 

# Decision Tree (DT) algorithm


```{r message=FALSE, warning=FALSE}
DT_model <- C5.0(data_train[-63], data_train$Class)
```

The preceding text shows some simple facts about the tree, including the function
call that generated it, the number of features (labeled predictors), and examples
(labeled samples) used to grow the tree. Also listed is the tree size of 62, which
indicates that the tree is 62 decisions deep.

Next we looked at the summary of the model. 
```{r message=FALSE, warning=FALSE, results='hide'}
summary(DT_model)
```

The numbers in parentheses indicate the number of examples meeting the criteria for
that decision, and the number incorrectly classified by the decision. 
For instance, on the first line, 56/4 indicates that of the 56 examples reaching the decision, 4 were incorrectly classified as not likely to default. In other words, 4 applicants actually defaulted, in spite of the model's prediction.

As we now know, it is very important to evaluate our model performance:

```{r message=FALSE, warning=FALSE}
DT_pred <- predict(DT_model, data_test)
DT_pred_tbl<- confusionMatrix(DT_pred,data_test[,63])
print(DT_pred_tbl[["table"]])
best_dt_stats <- data.frame( 
                          Accuracy=DT_pred_tbl[["overall"]][["Accuracy"]],
                           Sensitivity=round(DT_pred_tbl[["byClass"]][["Sensitivity"]],3), 
                           Precision=round(DT_pred_tbl[["byClass"]][["Precision"]],3), 
                           Specificity=round(DT_pred_tbl[["byClass"]][["Specificity"]],3)) 
rownames(best_dt_stats)<-c("DT default")
```

The performance here is somewhat worse than its performance on the
training data, but not unexpected, given that a model's performance is often worse
on unseen data. Also note that there are relatively many mistakes where the model predicted not a default, when in practice the loaner did default. 
Unfortunately, this type of error is a potentially costly mistake, as sick patients could be blind with time. 
We will try to improve the result.

### Adaptive Boosting

To improve our model we will use a C5.0 feature called adaptive boosting. This is a process in which many decision trees are built and the trees vote on the best class for each example. 

The C5.0() function makes it easy to add boosting to our C5.0 decision tree. We
simply need to add an additional trials parameter indicating the number of
separate decision trees to use in the boosted team. The trials parameter sets an
upper limit; the algorithm will stop adding trees if it recognizes that additional 
trials do not seem to be improving the accuracy. 
We'll start with 10 trials, a number that has become the de facto standard, as research 
suggests that this reduces error rates on test data by about 25 percent:

```{r message=FALSE, warning=FALSE, results='hide'}
DT_boost10 <- C5.0(data_train[-63], data_train$Class , trials = 10)
summary(DT_boost10)
```

The classifier got an error rate of
0% percent. This is quite an improvement over the previous training error rate
before adding boosting! However, it remains to be seen whether we see
a similar improvement on the test data. Let's take a look:

```{r message=FALSE, warning=FALSE}
DT_boost_pred10 <- predict(DT_boost10, data_test)
DT_boost_pred10_tbl<-confusionMatrix(DT_boost_pred10, data_test[,63])
print(DT_boost_pred10_tbl[["table"]])
best_dt_stats["boosting with 10 trials",] <- data.frame(
                          Accuracy=DT_boost_pred10_tbl[["overall"]][["Accuracy"]],
                           Sensitivity=round(DT_boost_pred10_tbl[["byClass"]][["Sensitivity"]],3), 
                           Precision=round(DT_boost_pred10_tbl[["byClass"]][["Precision"]],3), 
                           Specificity=round(DT_boost_pred10_tbl[["byClass"]][["Specificity"]],3)) 
```

The model is still not doing well at predicting class of patients, which may be a result of our relatively small training dataset, or it may just be a very difficult problem to solve.

Next, lets proceed to fine-tune our algorithm, using a cost matrix. 
The C5.0 algorithm allows us to assign a penalty to different types of errors, in order to discourage a tree from making more costly mistakes. The penalties are designated in a cost matrix, which specifies how much costlier each error is, relative to any other prediction.

First, we'll create a default 2x2 matrix, to later be filled with our cost values:

```{r message=FALSE, warning=FALSE}
matrix_dimensions <- list(c("normal", "glaucoma"), c("normal", "glaucoma"))
names(matrix_dimensions) <- c("predicted", "actual")
```

If we consider that the most important is to predict a sick patient as sick, much more that a normal patient as normal,
our penalty values could then be defined as:

```{r message=FALSE, warning=FALSE}
error_cost <- matrix(c(0, 6,20, 0), nrow = 2, dimnames = matrix_dimensions)
print(error_cost)
```

Now lets train again and see if the cost matrix made any difference:

```{r message=FALSE, warning=FALSE}
dt_cost <- C5.0(data_train[-63], data_train$Class , costs = error_cost)
dt_cost_pred <- predict(dt_cost, data_test) # predict on test data
dt_cost_pred_tbl<-confusionMatrix(dt_cost_pred, data_test[,63],dnn = c('predicted', 'actual'))
print(dt_cost_pred_tbl[["table"]])
```
```{r message=FALSE, warning=FALSE, echo=FALSE}
best_dt_stats["fine tuning with cost matrix",] <- data.frame(
                          Accuracy=dt_cost_pred_tbl[["overall"]][["Accuracy"]],
                           Sensitivity=round(dt_cost_pred_tbl[["byClass"]][["Sensitivity"]],3), 
                           Precision=round(dt_cost_pred_tbl[["byClass"]][["Precision"]],3), 
                           Specificity=round(dt_cost_pred_tbl[["byClass"]][["Specificity"]],3)) 
```
```{r message=FALSE, warning=FALSE}
best_dt_stats$General <- calc_score(best_dt_stats)
print.data.frame(best_dt_stats)
best_stats["DT",]<-cbind(best_dt_stats["default",], notes="default")
```

Compare these results to the boosted model; this version makes more mistakes overall, but the types of mistakes are very different. Where the previous models incorrectly classified a small number of class correctly, our weighted model has does much better in this regard. This trade resulting in a reduction of false negatives at the sick patient false positives may be acceptable if our cost estimates were accurate.


```{r message=FALSE, warning=FALSE, fig.height=3, fig.cap="Desition Tree", fig.width=6}
plot(dt_cost)
```

# Random forest (RF) algorithm
Random forest is a Supervised Machine Learning Algorithm. This algorithm is used in Classification and Regression problems. It builds decision trees on different samples and takes their majority vote for classification and average in case of regression.
The Random Forest Algorithm can handle the data set containing continuous variables as in the case of regression and categorical variables as in the case of classification.(cf: https://www.analyticsvidhya.com/blog/2021/06/understanding-random-forest/)

## Random Forest Modeling

The random Forest algorithm uses many decision trees. For classification, which is what
we are focusing on, the final class result is based on the class that was picked from most trees. 

First we define our model with the parsnip package. Random forests have very little mandatory parameters. 
Here we only define the number of trees:

```{r rf_mod, message=FALSE, warning=FALSE}
rf_mod <-  rand_forest(trees = 1000) %>%  set_engine("ranger") %>% set_mode("classification") 
```

We want reproducible results, so we set.seed. Taking a look at the model:

```{r rf_fit, message=FALSE, warning=FALSE, results='hide'}
rf_fit <- rf_mod %>% fit(Class ~ ., data = data_train) # train model
```

We can see our model had 1000 trees like we specified, and several other metrics, such as the prediction error, 13% here.

### Estimating performance

Next we want to see if we can improve our model. We do this by changing some things about it. 
The changes can either be random (guessing) or more precise, depending on performance metrics. In our example, we will use the area under the Receiver Operating Characteristic (ROC) curve (which demonstrates the trade-off between the sensitivity and and specificity), and overall classification accuracy.

Using the yardstick package, let's calculate ROC and Accuracy. Notice that we are still only working with the data_train partition of our data:

```{r RF rf_training_pred, message=FALSE, warning=FALSE}
rf_training_pred <- predict(rf_fit, data_train) %>% 
  bind_cols(predict(rf_fit, data_train, type = "prob")) %>% 
  bind_cols(data_train %>%  select(Class))
```
```{r, message=FALSE, warning=FALSE, echo=FALSE}
rf_training_pred_roc_auc<-rf_training_pred %>%   roc_auc(truth = Class, .pred_glaucoma)# ROC calculation
rf_training_pred_acc<- rf_training_pred %>%  accuracy(truth = Class, .pred_class)# Accuracy calculation

print(paste0("the roc_auc estimate is: ", rf_training_pred_roc_auc$.estimate))
print(paste0("the accuracy estimate is: ", rf_training_pred_acc$.estimate))
```

As we can see, these are very good results. Almost too good, so we would like to see how the model performs on the test data:

```{r RF rf_testing_pred, message=FALSE, warning=FALSE, results='hide'}
rf_testing_pred <- 
  predict(rf_fit, data_test) %>% 
  bind_cols(predict(rf_fit, data_test, type = "prob")) %>% 
  bind_cols(data_test %>% select(Class))
rf_testing_pred_tbl<-confusionMatrix(rf_testing_pred$.pred_class, rf_testing_pred$Class,dnn = c('predicted', 'actual'))
```
```{r, message=FALSE, warning=FALSE, echo=FALSE}
print(rf_testing_pred_tbl[["table"]])
best_rf_stats <- data.frame(
                          Accuracy=rf_testing_pred_tbl[["overall"]][["Accuracy"]],
                           Sensitivity=round(rf_testing_pred_tbl[["byClass"]][["Sensitivity"]],3), 
                           Precision=round(rf_testing_pred_tbl[["byClass"]][["Precision"]],3), 
                           Specificity=round(rf_testing_pred_tbl[["byClass"]][["Specificity"]],3)) 
rownames(best_rf_stats)<-"RF default"
#rf_testing_pred_roc_auc<- rf_testing_pred %>%  roc_auc(truth = Class, .pred_glaucoma) # ROC calculation
#rf_testing_pred_acc<-rf_testing_pred  %>%  accuracy(truth = Class, .pred_class) # Accuracy calculation

#print(paste0("the roc_auc estimate is: ", rf_testing_pred_roc_auc$.estimate))
#print(paste0("the accuracy estimate is: ", rf_testing_pred_acc$.estimate))
```
These validation results are lower than the ones we got on the training data.
There are several reasons why training set statistics like the ones shown in this section can be unrealistically optimistic:

- Overfitting - Models like random forests, neural networks, and other black-box methods can essentially memorize the training set. Re-predicting that same set should always result in nearly perfect results.

- The training set does not have the capacity to be a good arbiter of performance. It is not an independent piece of information; predicting the training set can only reflect what the model already knows.

## Resampling

Resampling methods, such as cross-validation and the bootstrap, are empirical simulation systems. They create a series of data sets similar to the training/testing split discussed previously; a subset of the data is used for creating the model and a different subset is used to measure performance. Resampling is always used with the training set. 

Here we'll use 10-fold cross-validation. This means that we'll create 10 "mini" datasets, or folds. We call the majority part of the folds (9 out of 10 in this case) the "analysis set" and the minority the "assessment set". We then train a model using the analysis set, and test it on the assessment set, effectively repeating the modeling process 10 times. This is how its done:

```{r RF folds, message=FALSE, warning=FALSE, results='hide'}
folds <- vfold_cv(data_train, v = 10) # create the folds
rf_wf <-   workflow() %>%  add_model(rf_mod) %>%  add_formula(Class ~ .)

rf_fit_rs <-   rf_wf %>%   fit_resamples(folds)# add folds to workflow and train model
```
```{r message=FALSE, warning=FALSE}
print.data.frame(collect_metrics(rf_fit_rs))
```

We see these results are lower and look more realistic.
Now, looking at the test set for results, we expect and see much more similar results to the above ROC and accuracy:

```{r rf_testing_pred stats, message=FALSE, warning=FALSE, results='hide', echo=FALSE}
#rf_testing_pred_roc_auc<- rf_testing_pred %>%    roc_auc(truth = Class, .pred_glaucoma)
#rf_testing_pred_acc<-rf_testing_pred %>% accuracy(truth = Class, .pred_class)
rf_testing_pred_tbl2<-confusionMatrix(rf_testing_pred$.pred_class, rf_testing_pred$Class,dnn = c('predicted', 'actual'))
best_rf_stats["10-fold cross-validation",] <- data.frame(
                          Accuracy=rf_testing_pred_tbl2[["overall"]][["Accuracy"]],
                           Sensitivity=round(rf_testing_pred_tbl2[["byClass"]][["Sensitivity"]],3), 
                           Precision=round(rf_testing_pred_tbl2[["byClass"]][["Precision"]],3), 
                           Specificity=round(rf_testing_pred_tbl2[["byClass"]][["Specificity"]],3)) 

#print(paste0("the roc_auc estimate is: ", rf_testing_pred_roc_auc$.estimate))
#print(paste0("the accuracy estimate is: ", rf_testing_pred_acc$.estimate))
```
```{r warning=FALSE, message=FALSE, echo=FALSE, results='hide'}
print.data.frame(best_rf_stats)
```


## Tuning hyperparameters

Another way to improve the performance of our models is by changing the parameters we provide them with. This is also called Tuning. Random Forests, as mentioned above, are not very sensitive to such parameters, but decision trees are. Lets see:

```{r Tuning tune_spec, message=FALSE, warning=FALSE, results='hide'}
# defining the parameters for the decision tree
tune_spec <-   decision_tree(cost_complexity = tune(), # to control the size of the tree
    tree_depth = tune()) %>% set_engine("rpart") %>%   set_mode("classification")
```

Note that in the above code, tune() is still just a placeholder. 
We will fill in values later on.

Next, we will create several smaller datasets, to try different parameters. 
The grid_regular command below will create 5 such datasets for each parameter combination, meaning 25 in total.

```{r Tuning tree_grid, message=FALSE, warning=FALSE, results='hide'}
tree_grid <- grid_regular(cost_complexity(), tree_depth(), levels = 5)
tree_grid %>%   count(tree_depth)
glaucoma_folds <- vfold_cv(data_train) # create the actual cross-validation folds
```
### Model tuning with a grid

```{r Tuning tree_wf,message=FALSE, warning=FALSE, results='hide'}
tree_wf <- workflow() %>%  add_model(tune_spec) %>%  add_formula(Class ~ .)
tree_res <- tree_wf %>%  tune_grid(resamples = glaucoma_folds, grid = tree_grid)
tree_res %>% collect_metrics()
```
Its easier to see how the models did with a graph:

```{r RF plot preformance, message=FALSE, warning=FALSE, echo=FALSE, results='hide'}
tree_res %>%
  collect_metrics() %>%
  mutate(tree_depth = factor(tree_depth)) %>%
  ggplot(aes(cost_complexity, mean, color = tree_depth)) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free", nrow = 2) +
  scale_x_log10(labels = scales::label_number()) +
  scale_color_viridis_d(option = "plasma", begin = .9, end = 0)
```

We can see that our tree with a depth of 1, is the worst model according to both metrics and across all candidate values of cost_complexity. Our deepest tree, with a depth of 15, did better.  The show_best() function shows us the top 5 candidate models by default:

```{r message=FALSE, warning=FALSE}
print.data.frame(tree_res %>%  show_best("roc_auc")) # best tree
```

And finally, let's finalize the workflow with the best tree. 

```{r RF best_tree final_wf, message=FALSE, warning=FALSE, results='hide'}
best_tree <- tree_res %>%  select_best("roc_auc") # Selecting the best tree
final_wf <-   tree_wf %>%   finalize_workflow(best_tree) # Finalize workflow
print(final_wf)
```
```{r message=FALSE, warning=FALSE,echo=FALSE}
print.data.frame(best_tree)
```
### Exploring results

So we have our best tree parameters. Lets take a better look:

```{r  Exploring results final_tree, message=FALSE, warning=FALSE, results='hide'}
final_tree <-   final_wf %>%  fit(data = data_train) 
print(final_tree)
```

Another way to look at how the model uses the different features is using the vip package:

```{r message=FALSE, warning=FALSE,results='hide', fig.show='hide'}
final_tree %>%   extract_fit_parsnip() %>%   vip()
```

```{r message=FALSE, warning=FALSE}
best_rf_stats$General <- round(calc_score(best_rf_stats),3)
```
## Random forest conclusions and Finalizing the model


Finally, let’s return to our test data and estimate the model performance we expect to see with new data. We can use the function last_fit() with our finalized model; this function fits the finalized model on the full training data set and evaluates the finalized model on the testing data.

```{r Random forest conclusions, message=FALSE, warning=FALSE, results='hide', fig.show='hide'}
final_fit <- final_wf %>%  last_fit(patient_split) # Set workflow
final_fit %>%  collect_metrics()# train
final_fit %>%  collect_predictions() %>%  # collect metrics
  roc_curve(Class, .pred_glaucoma) %>%   autoplot()
args(decision_tree)

best_stats["RF",]<-cbind(best_rf_stats["RF default",], notes="RF default")
```
We can observe in the ROC curve that in our true positive prediction we have very few false positive predictions.

# conclusions
```{r svm conclusion and extra, echo=FALSE, message=FALSE, warning=FALSE}
print.data.frame(best_stats)
roc_test<-roc.test(pROC_svm_linear,pROC_svm_rbf)
roc_test$estimate


g2 <- ggroc(list(vanilladot=pROC_svm_linear, rbf=pROC_svm_rbf))+labs(title = "linear Vs. RBF")


# And change axis labels to FPR/FPR
linear_roc <- ggroc(pROC_svm_linear, legacy.axes = TRUE)+ ggtitle("linear ROC curve") +geom_segment(aes(x = 0, xend = 1, y = 0, yend = 1), color="darkgrey", linetype="dashed")
rbf_roc <- ggroc(pROC_svm_rbf, legacy.axes = TRUE)+ ggtitle("rbf ROC curve") +geom_segment(aes(x = 0, xend = 1, y = 0, yend = 1), color="darkgrey", linetype="dashed")

#grid.arrange(linear_roc + xlab("FPR") + ylab("TPR"),g2+theme(legend.position = "top"),rbf_roc + xlab("FPR") + ylab("TPR"), ncol=3) 

```
