---
editor_options:
  markdown:
    wrap: 72
  chunk_output_type: inline
output:
  word_document: 
    fig_width: 10
  html_document:
    self_contained: no
    keep_md: yes
  pdf_document: default
---

------------------------------------------------------------------------

title: "Final Project"

title: "Final Projrct"

author: "Lilach Herzog & Leslie Cohen" date: "13 5 2022" output:
word_document: toc: yes toc_depth: 4 pdf_document: toc_depth: 1
fig_caption: yes number_sections: yes fig_height: 3 editor_options:
chunk_output_Class: inline ---

Our whole project and code can be found on github at
<https://github.com/LeslieLebon/assignment-2.git>

Our objective in this project was to analyze the "GlaucomaM" dataset
(where the 'Class' of the seed is the column used for labeling and
classifying) in order to be able to look at a person's data and
according to that predict with a certain probability whether or not he
might get sick with glaucoma.

First, we need to load libraries and our dataset.

# preproccessing

## Upload data and libraries

```{r, message=FALSE, warning=FALSE}
library(gmodels)
library(C50)
library(class) 
library(tidymodels) # for the rsample package, along with the rest of tidymodels
library(ROCR)
library(reshape2)
#library(ggpmisc)
library(ggplot2)
library(gridExtra)
library(pROC)
library(ggfortify)
library(corrplot)
library(tidyverse)    # data manipulation and visualization
library(kernlab)      # SVM methodology
library(e1071)        # SVM methodology
library(ISLR)         # contains example data set "Khan"
library(RColorBrewer) # customized coloring of plot
library(dplyr)
library("Hmisc")

glaucoma <- read.csv("GlaucomaM.csv")# read data
```

## look at the data and initial preproccessing
We took a quick look at the data as a whole, and specifically the 'Class' column.
In order to further understand the data and be able to work with it we
also used the "table" function. this function builds a contingency table
of the counts at each combination of factor levels- we used it for the
'Class'.

```{r, message=FALSE, warning=FALSE, results='hide'}
head(glaucoma)
str(glaucoma)
colSums(glaucoma==0)
glaucoma%>%select((everything()))%>%summarise_all(funs(sum(is.na(.)))) # check NA's
print(summary(glaucoma$Class))
```

We have 196 patients (rows) and 63 features (columns). We looked at the data
and saw that there are no NA's we need to take care of, and all columns
are numeric, so there is no need to work on that. We can also observe that we have few 0s (between 0 and 48(feature vast)). We decided to keep all features for the moment.

We will classify our data according to the' 'Class' column.Indeed, the class telling us if the data from the corresponding row, is associated to a sick or healthy patient.
To this end, we took a quick look specifically the 'Class' column.
In order to further understand the data and be able to work with it we
also used the "table" function. this function builds a contingency table
of the counts at each combination of factor levels- we used it for the
'Class'.

```{r, message=FALSE, warning=FALSE, results='hide'}
print(summary(glaucoma$Class))
table(glaucoma$Class) # quick look specifically at the 'Class' column
glaucoma$Class<-as.factor(glaucoma$Class)
```

There are two classes to classify: ' normal'
and ' glaucoma' , divided neatly
in half. Before starting on teaching the algorithm, we converted the
'Class' column to a factor, as that is what is required by the C50
package.

Since the whole table is already shuffled according to the 'Class'
column, we don't need to shuffle the whole table again

```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
set.seed(123)#to initialize a pseudo random number generator. 

glaucoma <- glaucoma[sample(nrow(glaucoma)),]
```

# EDA analysis

We will do an EDA analysis to select the correct features which will help us to classify patients.
For that, we performed and plotted a PCA with all features without any filter to observe if we need to delete some features.

```{r, message=FALSE, warning=FALSE, results='hide'}

# Preform PCA on our data

gl.pca <- prcomp(glaucoma[,1:62],
                   center = TRUE,
                   scale. = TRUE)
  
# summary of the 
# prcomp object
summary(gl.pca)
gl_pca_plot <- autoplot(gl.pca,
                          data = glaucoma,
                          colour = 'Class')
  

```
```{r, message=FALSE, warning=FALSE}
gl_pca_plot
```

In the PCA, we can observe 2 clusters of patients: glaucoma patients and normal
patients. Unfortunately, some of them were not clustered correctly (mixed clouds of blue and red points) in the same area. This mixture could be, probably to the fact that many features correlate, so our next course of action was to check for any correlation between features to determine which feature are correlated by using pearson correlation.

```{r, message=FALSE, warning=FALSE, results='hide'}
#calculation of correlation, using pearson correlation
gl.cor = cor(glaucoma[,c(1:62)], method = c("pearson"))

corrplot(gl.cor)

palette = colorRampPalette(c("green", "white", "red")) (20)
heatmap(x = gl.cor, col = palette, symm = TRUE)

```
In the first correlation heatmap (Results obtained with corrplot) positive correlations are displayed in a
blue scale while negative correlations are displayed in a red scale. we
can observe 8 clusters of features that highly correlate. In the second correlation heatmap (Pearson correlation and hierarchical clustering): we
can observe one big cluster of features that have a high correlation. This cluster could be divided in 2 and 2 others
clusters of features.
We will try to remove features which have a high correlation score and plot a PCA to observe if these deletion could improve the patient class clustering.
```{r, message=FALSE, warning=FALSE, results='hide'}
#This generates one table of correlation coefficients (the correlation matrix) and another table of the p-values. By default, the correlations and p-values #are stored in an object of class type rcorr. To extract the values from this object into a useable data structure, you can use the following syntax:
gl.rcorr = rcorr(as.matrix(glaucoma[,c(1:62)]))
gl.rcorr
gl.coeff = gl.rcorr$r

cor_matrix_rm <- gl.cor                  # Modify correlation matrix
cor_matrix_rm[upper.tri(cor_matrix_rm)] <- 0
diag(cor_matrix_rm) <- 0
cor_matrix_rm

data<-glaucoma[,c(1:62)]
data_new <- data[ , !apply(cor_matrix_rm,    # Remove highly correlated variables
                           2,
                           function(x) any(x > 0.8))]
head(data_new)

```
```{r, message=FALSE, warning=FALSE}
gl.cor2 = cor(data_new, method = c("pearson"))
corrplot(gl.cor2)
heatmap(x = gl.cor2, col = palette, symm = TRUE)
colSums(glaucoma[,c(1:62)] != 0)


```
As we can observe in the different heatmap, with the filter applied, there is less features which correlate highly.

```{r, message=FALSE, warning=FALSE}

########
# Preform PCA on our new data

gl.pca2 <- prcomp(data_new,
                   center = TRUE,
                   scale. = TRUE)#play with options
  
```
```{r, message=FALSE, warning=FALSE, results='hide'}
# summary of the 
# prcomp object
summary(gl.pca2)
```
```{r, message=FALSE, warning=FALSE}
gl_pca_plot2 <- autoplot(gl.pca2,
                          data = glaucoma,
                          colour = 'Class')
  




gl_pca_plot2
gl_pca_plot

```
Unfortunately, we cannot observe a high improvement in the patient class clustering. Indeed, in the new PCA we observe again this mixed cluster of blue and red points.
Taking all together, we decided to keep all the features from the dataset.


Before starting on teaching the algorithm, we converted the 'Class'
column to a factor, as that is what is required by the C50 package.

```{r, message=FALSE, warning=FALSE, results='hide'}
glaucoma$Class<-as.factor(glaucoma$Class)
```

Since the whole table is sorted according to the 'Class' column, and we
want to work randomly, we will shuffle the whole table

```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
glaucoma <- glaucoma[sample(nrow(glaucoma)),]
str(glaucoma) #look at the data again to make sure it's shuffled
```

## split into training and test sets

### Splitting

We split the whole data set into 80% training set and 20% test set.
Since we know that there are about a half of each Class in the data set,
we wanted to be sure that the distribution is as we defined (about 1/2
of each Class in of both training and test sets) The training set is
what we will use to train the algorithm, to help it learn our data, and
then we will run the algorithm on 20% of the data (test set), and
predict according to what it learned in the training set, and since it
is still our data we have the true class each individual belongs to, so
we can evaluate how well the algorithm succeeded in predicting.

```{r, message=FALSE, warning=FALSE}
length_of_training_set <-round(0.8*(nrow(glaucoma)),0) 
train_set <- glaucoma[1:length_of_training_set,] #80% of dataset for training
test_set <- glaucoma[(length_of_training_set+1):nrow(glaucoma), ] #20% of dataset for testing
train_set_labels <-train_set["Class"] #save true "Classes" of the training set
test_set_labels <-test_set["Class"] #save true "Classes" of the test set
prop.table(table(train_set$Class))
```

We decided to use SVM, random forest, KNN and ??? algorithms to classify our data.


# SVM

Here we'll use SVM to develop a model similar to those used at the core
of the Optical Character Recognition (OCR) software often bundled with
desktop document scanners. The purpose of such software is to process
paper-based documents by converting printed or handwritten text into an
electronic form to be saved in a database. Of course, this is a
difficult problem due to the many variants in handwritten style and
printed fonts. Even so, software users expect perfection, as errors or
typos can result in embarrassing or costly mistakes in a business
environment.

According to the documentation provided by Frey and Slate, when the
glyphs are scanned into the computer, they are converted into pixels and
16 statistical attributes are recorded.

The attributes measure such characteristics as the horizontal and
vertical dimensions of the glyph, the proportion of black (versus white)
pixels, and the average horizontal and vertical position of the pixels.
Presumably, differences in the concentration of black pixels across
various areas of the box should provide a way to differentiate among the
26 Classs of the alphabet.

As always, we start by reading and looking at the data.

One important thing to note when using SVM is that it only works on
numeric data, and that each feature must be scaled. The data here is
already numeric, and the package we use will let us scale the data when
running the algorithm itself. So we are good to go!

The data is already randomized, so we can easily create our training and
test sets:

And to building the actual model:

```{r message=FALSE, warning=FALSE}
# create and train model
Class_classifier_linear <-  ksvm(x=Class~., data=train_set,kernel = "vanilladot")
Class_classifier_linear
```

As we mentioned, SVM can be a bit of a black box. We know the model uses
61 support vectors, and that the training error is 0.076433 but we don't
know anything else.

Lets run the model on the test set, and see how well our model does.

```{r message=FALSE, warning=FALSE}
# apply model on test set
Class_predictions_linear <- predict(Class_classifier_linear, test_set)
table(Class_predictions_linear, test_set$Class)
svm_table_linear<-CrossTable(test_set$Class, Class_predictions_linear,
prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
dnn = c('actual Type', 'predicted Type'))
svm_linear_tbl<-svm_table_linear[["t"]]
grid.arrange(tableGrob(svm_linear_tbl),ncol=1,nrow=1)
```

To examine how well our classifier performed, we need to compare the
predicted Class to the true Class in the testing dataset, so we used the
table() function.


To avoid having to calculate an accuracy score from this large confusion
matrix, we will use the following command, which returns a vector of
TRUE or FALSE values, indicating whether the model's predicted Class
agrees with (that is, matches) the actual Class in the test dataset. We
will then use it to calculate the percentages.

```{r message=FALSE, warning=FALSE}

df_linear<-data.frame(predictions=c(as.numeric(Class_predictions_linear)),labels = c(test_set$Class))
pROC_svm_linear <- roc(df_linear$labels,df_linear$predictions,
            smoothed = TRUE,
            ci=TRUE, ci.alpha=0.9, stratified=FALSE,
            plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
            print.auc=TRUE, show.thres=TRUE)
sens.ci <- ci.se(pROC_svm_linear)
plot(sens.ci, Class="shape", col="lightblue", title("linear kernel"))
plot(sens.ci, Class="bars")
svm_linear_stats <- data.frame(kernel="linear",precision=precision(svm_linear_tbl)$.estimate, recall= recall(svm_linear_tbl)$.estimate, accuracy=accuracy(svm_linear_tbl)$.estimate, AUC=pROC_svm_linear$au)
grid.arrange(tableGrob(svm_linear_stats),ncol=1,nrow=1)
agreement_linear <- Class_predictions_linear == test_set$Class # calculate true/false values
prop.table(table(agreement_linear)) # true/false preformance percentage
```

The accuracy is  0.846 and the AUC is 0.857. These are pretty good results. We want to make them even better, but what's really important for us is to minimize the false negatives- type II error- since we are working with information about patients, a result telling a person that might be sick that he is healthy can be much more problematic than a result telling a healthy person that he might be sick, and our recall is 0.944.
We still want to see, maybe a different kernel will do a better job?

### Improving SVM Model

There are many kernel functions for SVM, but a standard Gaussian Radial
basis function (RBF) kernel is a good place to start. We'll use the
ksvm() function here. Note that since this is a large dataset, running
times can start to add up:

```{r message=FALSE, warning=FALSE}
# train svm model with RBF kernel function
Class_classifier_rbf <- ksvm(Class ~ ., data = train_set, kernel = "rbfdot")
Class_predictions_rbf <- predict(Class_classifier_rbf, test_set) # predict on test set
svm_table_rbf<-CrossTable(test_set$Class, Class_predictions_rbf,
prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
dnn = c('actual Type', 'predicted Type'))
svm_rbf_tbl<-svm_table_rbf[["t"]]
```

And compare it to the previous model:

```{r message=FALSE, warning=FALSE}
# calculate performance

df_rbf<-data.frame(predictions=c(as.numeric(Class_predictions_rbf)),labels = c(test_set$Class))
pROC_svm_rbf <- roc(df_rbf$labels,df_rbf$predictions,
            smoothed = TRUE,
            ci=TRUE, ci.alpha=0.9, stratified=FALSE,
            plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
            print.auc=TRUE, show.thres=TRUE)
sens.ci <- ci.se(pROC_svm_rbf)
plot(sens.ci, Class="shape", col="lightblue", title("RBF"))
plot(sens.ci, Class="bars")
svm_rbf_stats <- data.frame(kernel="rbf",precision=precision(svm_rbf_tbl)$.estimate, recall= recall(svm_rbf_tbl)$.estimate, accuracy=accuracy(svm_rbf_tbl)$.estimate, AUC=pROC_svm_rbf$auc)
```

We can see a substantial increase in accuracy, from 84.6% to 92.3% and
AUC of 93.2%. Other kernels might prove even better, or the cost of
constraints parameter C could be varied to modify the width of the
decision boundary.

Note that even at this point, we are not really sure how SVM classifies
the characters (unlike a decision tree lets say), but it seems to be negligible, indeed, SVM is good for cases when it does not.

```{r message=FALSE, warning=FALSE}
# compare performance
roc_test<-roc.test(pROC_svm_linear,pROC_svm_rbf)
roc_test$estimate

svm_linear_stats[nrow(svm_linear_stats)+1,]=svm_rbf_stats
grid.arrange(tableGrob(svm_linear_stats),ncol=1,nrow=1)

g2 <- ggroc(list(vanilladot=pROC_svm_linear, rbf=pROC_svm_rbf))+labs(title = "linear Vs. RBF")


# And change axis labels to FPR/FPR
linear_roc <- ggroc(pROC_svm_linear, legacy.axes = TRUE)+ ggtitle("linear ROC curve") +geom_segment(aes(x = 0, xend = 1, y = 0, yend = 1), color="darkgrey", linetype="dashed")
rbf_roc <- ggroc(pROC_svm_rbf, legacy.axes = TRUE)+ ggtitle("rbf ROC curve") +geom_segment(aes(x = 0, xend = 1, y = 0, yend = 1), color="darkgrey", linetype="dashed")
grid.arrange(linear_roc + xlab("FPR") + ylab("TPR"),g2+theme(legend.position = "top")+ labs(color="kernel:"),rbf_roc + xlab("FPR") + ylab("TPR"), ncol=3) 

#for future work
roc.list <- roc(Class~., data=glaucoma)
per_feature <- ggroc(roc.list, linetype=2)
per_feature+ ggtitle("per feature")
```
```{r message=TRUE, warning=TRUE}

# with additional aesthetics:
ggroc(pROC_svm_rbf, legacy.axes = TRUE)+geom_segment(aes(x = 0, xend = 1, y = 0, yend = 1), color="darkgrey", linetype="dashed")


    

# Multiple curves:


# This is equivalent to using roc.formula:
roc.list <- roc(Class~., data=glaucoma)
g.list <- ggroc(roc.list)
# You can change the aesthetics as you normally would with ggplot2:
g.list + scale_colour_brewer(palette="RdGy")+ ggtitle("1")

# with additional aesthetics:

g4 <- ggroc(roc.list, aes="linetype", color="red")
g4+ ggtitle("4")
# changing multiple aesthetics:
g5 <- ggroc(roc.list, aes=c("linetype", "color"))
g5+ ggtitle("5")


# To have all the curves of the same color, use aes="group":
g.group <- ggroc(roc.list, aes="group")
g.group+ ggtitle("7")

```
# Example 2 - SVM compared with MMC and SVC

(This example was taken from <https://afit-r.github.io/svm>)

In cases where the data is linearly separable (can be easily separated
with a single line), SVM is not required. We can simply use a Maximal
Margin Classifier (MMC) to separate the data. When the data is not
linearly separable, we can use a support vector classifier (SVC)
(sub-method of SVM). Lets see how the three methods compare:

## MMC

```{r}
set.seed(123)
# Construct sample data set - completely separated
x <- matrix(rnorm(20*2), ncol = 2)
y <- c(rep(-1,10), rep(1,10))
x[y==1,] <- x[y==1,] + 3/2
dat <- data.frame(x=x, y=as.factor(y))

####Lilach, in glaucoma data frame, there is no x.2 and x.1 column...
# Plot data
ggplot(data = glaucoma, aes(x = x.2, y = x.1, color = y, shape = y)) + 
  geom_point(size = 2) +
  scale_color_manual(values=c("#000000", "#FF0000")) +
  theme(legend.position = "none")
```

We can see that the data can be divided with a single line, but we can
still draw an infinite number of such lines. MMC helps us find the best
one. First, we use the e1071 package:

```{r}
# Fit Support Vector Machine model to data set
svmfit <- svm(y~., data = dat, kernel = "linear", scale = FALSE)
# Plot Results
plot(svmfit, dat)
```

The points marked as X were used to calculate the line, while the points
marked with 0 were ignored.

The same line can be drawn using the kernlab package:

```{r}
# fit model and produce plot
kernfit <- ksvm(x, y, Class = "C-svc", kernel = 'vanilladot')
plot(kernfit, data = glaucoma)
```

## SVC - Support Vector Classifiers

What about when the cases can't be easily separated?

```{r}
set.seed(457)
# Construct sample data set - not completely separated
x <- matrix(rnorm(20*2), ncol = 2)
y <- c(rep(-1,10), rep(1,10))
x[y==1,] <- x[y==1,] + 1
dat <- data.frame(x=x, y=as.factor(y))

# Plot data set
ggplot(data = dat, aes(x = x.2, y = x.1, color = y, shape = y)) + 
  geom_point(size = 2) +
  scale_color_manual(values=c("#000000", "#FF0000")) +
  theme(legend.position = "none")
```

Whether the data is separable or not, the svm() command syntax is the
same. In the case of data that is not linearly separable, however, the
cost = argument takes on real importance. This quantifies the penalty
associated with having an observation on the wrong side of the
classification boundary. We can plot the fit in the same way as the
completely separable case. We first use e1071:

```{r}
# Fit Support Vector Machine model to data set
svmfit <- svm(y~., data = dat, kernel = "linear", cost = 10)
# Plot Results
plot(svmfit, dat)
```

Now we use the kernlab package, but also change the cost from 10 to 100.

```{r}
# Fit Support Vector Machine model to data set
kernfit <- ksvm(x,y, Class = "C-svc", kernel = 'vanilladot', C = 100)
# Plot results

plot(kernfit, data = x)
#I got an error:
#Error in .local(x, ...) : 
#Only plots of classification ksvm objects supported
```

The higher the value of cost, the more the model tries to avoid a
mis-clasification. But how can we decide what is the "best" cost for our
data? Instead of specifying a cost up front, we can use the tune()
function from e1071 to test various costs and identify which value
produces the best fitting model:

```{r}
# find optimal cost of mis-classification
tune.out <- tune(svm, y~., data = dat, kernel = "linear",
                 ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
# extract the best model
(bestmod <- tune.out$best.model)
```

With the optimal cost calculated, we can construct a table of predicted
classes against true classes using the predict() command as follows:

```{r}
ypred <- predict(bestmod, dat)
(misclass <- table(predict = ypred, truth = dat$y))
```

Using this support vector classifier did an ok job, but the data we
generated was small.

## (finally) SVM

The above mentioned SVC is a specific case of SVM, which is a more
robust, generalized method. The options for classification structures
using the svm() command from the e1071 package are linear, polynomial,
radial, and sigmoid. To demonstrate a nonlinear classification boundary,
we will construct a new data set:

```{r}
# construct larger random data set
x <- matrix(rnorm(200*2), ncol = 2)
x[1:100,] <- x[1:100,] + 2.5
x[101:150,] <- x[101:150,] - 2.5
y <- c(rep(1,150), rep(2,50))
dat <- data.frame(x=x,y=as.factor(y))

# Plot data
ggplot(data = dat, aes(x = x.2, y = x.1, color = y, shape = y)) + 
  geom_point(size = 2) +
  scale_color_manual(values=c("#000000", "#FF0000")) +
  theme(legend.position = "none")
```

Notice that the data is not linearly separable, and furthermore, isn't
all clustered together in a single group. There are two sections of
class 1 observations with a cluster of class 2 observations in between.
To demonstrate the power of SVMs, we'll take 100 random observations
from the set and use them to construct our boundary. We set kernel =
"radial" based on the shape of our data and plot the results.

```{r}
# set pseudorandom number generator
set.seed(123)
# sample training data and fit model
train <- base::sample(200,100, replace = FALSE)
svmfit <- svm(y~., data = dat[train,], kernel = "radial", gamma = 1, cost = 1)
# plot classifier
plot(svmfit, dat)
```

The same procedure can be run using the kernlab package, which has far
more kernel options than the corresponding function in e1071. In
addition to the four choices in e1071, this package allows use of a
hyperbolic tangent, Laplacian, Bessel, Spline, String, or ANOVA RBF
kernel. To fit this data, we set the cost to be the same as it was
before, 1.

```{r}
# Fit radial-based SVM in kernlab
kernfit <- ksvm(x[train,],y[train], Class = "C-svc", kernel = 'rbfdot', C = 1, scaled = c())
# Plot training data
plot(kernfit, data = x[train,])
# i got an error again:
#Error in .local(x, ...) : 
#Only plots of classification ksvm objects supported
```

Again, using tune to find the best cost for our data:

```{r}
# tune model to find optimal cost, gamma values
tune.out <- tune(svm, y~., data = dat[train,], kernel = "radial",
                 ranges = list(cost = c(0.1,1,10,100,1000)))
# show best model
tune.out$best.model
```

```{r}
# validate model performance
(valid <- table(true = dat[-train,"y"], pred = predict(tune.out$best.model,
                                             newx = dat[-train,])))
```

# SVM on data with more than 2 classes

```{r}
# construct data set
x <- rbind(x, matrix(rnorm(50*2), ncol = 2))
y <- c(y, rep(0,50))
x[y==0,2] <- x[y==0,2] + 2.5
dat <- data.frame(x=x, y=as.factor(y))
# plot data set
ggplot(data = dat, aes(x = x.2, y = x.1, color = y, shape = y)) + 
  geom_point(size = 2) +
  scale_color_manual(values=c("#000000","#FF0000","#00BA00")) +
  theme(legend.position = "none")
```

The commands don't change for the e1071 package. We specify a cost and
tuning parameter γ and fit a support vector machine. The results and
interpretation are similar to two-class classification.

```{r}
# fit model
svmfit <- svm(y~., data = dat, kernel = "radial", cost = 10, gamma = 1)
# plot results
plot(svmfit, dat)
```

We can check to see how well our model fit the data by using the
predict() command, as follows:

```{r}
# construct table
ypred <- predict(svmfit, dat)
(misclass <- table(predict = ypred, truth = dat$y))
```

The kernlab package, on the other hand, can fit more than 2 classes, but
cannot plot the results. To visualize the results of the ksvm function,
we take the steps listed below to create a grid of points, predict the
value of each point, and plot the results:

```{r}
# fit and plot
kernfit <- ksvm(as.matrix(dat[,2:1]),dat$y, Class = "C-svc", kernel = 'rbfdot', 
                C = 100, scaled = c())

# Create a fine grid of the feature space
x.1 <- seq(from = min(dat$x.1), to = max(dat$x.1), length = 100)
x.2 <- seq(from = min(dat$x.2), to = max(dat$x.2), length = 100)
x.grid <- expand.grid(x.2, x.1)

# Get class predictions over grid
pred <- predict(kernfit, newdata = x.grid)

# Plot the results
cols <- brewer.pal(3, "Set1")
plot(x.grid, pch = 19, col = adjustcolor(cols[pred], alpha.f = 0.05))

classes <- matrix(pred, nrow = 100, ncol = 100)
contour(x = x.2, y = x.1, z = classes, levels = 1:3, labels = "", add = TRUE)

points(dat[, 2:1], pch = 19, col = cols[predict(kernfit)])
```

To conclude, SVM is a robust classification method that can be used on
easy or difficult datasets to separate data with 2 or more classes.
Because it's a 'black box', it's very useful as long as you don't need
to know what is happening behind the scenes.

# KNN algorithm

In the KNN Algorithm, k is a user-defined constant, and an unlabeled
vector (a query or test point) is classified by assigning the label
which is most frequent among the k training samples nearest to that
query point.

## run the KNN algorithm

And finally, we are ready to run our algorithm. We'll start with K of 3,
the the number of Classes we have and also an odd number, reducing the
change of a tie vote.

```{r, message=FALSE, warning=FALSE,results='hide'}
pred_knn <- knn.cv(train = train_set, cl = train_set_labels, k=21)
# I got an error:
#Error in knn.cv(train = train_set, cl = train_set_labels, k = 21) : 
#'train' and 'class' have different lengths
#'
knn_table<-CrossTable(x = test_set_labels, y = pred_knn, prop.chisq=FALSE)
acc_knn21 <- sum(diag(knn_table[["t"]])) / sum(knn_table[["t"]])
df<-data.frame(predictions=c(as.numeric(pred_knn)),labels = c(test_set_labels))
pROC_knn21 <- roc(df$labels,df$predictions,
            smoothed = TRUE,
            ci=TRUE, ci.alpha=0.9, stratified=FALSE,
            plot=FALSE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
            print.auc=TRUE, show.thres=TRUE)
sens.ci <- ci.se(pROC_knn21)
```

Since the knn function returns a factor vector of the predicted values,
we'll compare that vector with the true labels we saved in advance.
We'll do the comparison with the CrossTable function, from the gmodels
package loaded:

```{r, message=FALSE, warning=FALSE,results='hide'}
knn_table<-CrossTable(x = test_set_labels, y = pred_knn, prop.chisq=FALSE)
```

```{r, message=FALSE, warning=FALSE,echo=FALSE}
knn_table[["t"]]
```

We got an accuracy of 100%! We managed to classify our test set
perfectly, now we want to see how other parameters will affect our
perfect score

### normalization

We tried normalization, thinking that the range of values of each
parameter is similar in order to compare the distances of different
features with different scales.

```{r, results='hide', message=FALSE, warning=FALSE}
normalize <- function(x) {return ((x - min(x)) / (max(x) - min(x)))}
normalize(c(1, 2, 3, 4, 5)) # test our function 1
normalize(c(10, 20, 30, 40, 50)) # test our function 2
train_set_norm <- as.data.frame(lapply(train_set[1:7], normalize))
test_set_norm <- as.data.frame(lapply(test_set[1:7], normalize))
summary(train_set_norm$Area) # test our dataset
```

Everything looked ok so we went on the the prediction and evaluation of
accuracy

```{r, message=FALSE, warning=FALSE,results='hide'}
pred_knn_norm <- knn(train = train_set_norm, test = test_set_norm, cl = train_set_labels, k=3)
#i got an error:
#Error in knn(train = train_set_norm, test = test_set_norm, cl = train_set_labels,  : 
#  'train' and 'class' have different lengths

knn_norm_table<-CrossTable(x = test_set_labels, y = pred_knn_norm, prop.chisq=FALSE)
acc_knn3_norm <- sum(diag(knn_norm_table[["t"]])) / sum(knn_norm_table[["t"]])
df_norm<-data.frame(predictions=c(as.numeric(pred_knn_norm)),labels = c(test_set_labels))
pROC_norm <- roc(df_norm$labels,df_norm$predictions,
            smoothed = TRUE,
            ci=TRUE, ci.alpha=0.9, stratified=FALSE,
            plot=FALSE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
            print.auc=TRUE, show.thres=TRUE)
sens.ci <- ci.se(pROC_norm)
```

```{r, message=FALSE, warning=FALSE, echo=FALSE}
knn_norm_table[["t"]]
```

It looks like the normalization actually resulted in decreased accuracy,
we plummeted from 1 to 0.925, as well as AUC that decreased from 1 to
0.967... In the above table we see that all 17 seeds(100%) of Class 1
were predicted correctly, but it has a few false positives (3 seeds that
were wrongly predicted as Class one: one seed was actually Class 2 and 2
were Class 3). on the other hand- Classes two and three which had no
false positives (all seeds that were predicted as two or three were
actually two or 3 respectively) and only a few false negatives (14/15
Class 2 seeds (93.3%) and 6/8 Class 3 seeds (75%) were identified
correctly). Let's check z-score \### z-score standardization We also
tried z-score standardization instead of normalization:

```{r, message=FALSE, warning=FALSE, results='hide'}
train_set_z <- as.data.frame(scale(train_set[1:7]))
test_set_z <- as.data.frame(scale(test_set[1:7]))
pred_z <- knn(train = train_set_z, test = test_set_z, cl = train_set_labels, k=3)
#i got an error:
#Error in knn(train = train_set_z, test = test_set_z, cl = train_set_labels,  : 
 # 'train' and 'class' have different lengths

knn_z_table<-CrossTable(x = test_set_labels, y = pred_z, prop.chisq=FALSE)
acc_knn3_z <- sum(diag(knn_z_table[["t"]])) / sum(knn_z_table[["t"]])
df_z<-data.frame(predictions=c(as.numeric(pred_z)),labels = c(test_set_labels))
pROC_z <- roc(df_z$labels,df_z$predictions,
            smoothed = TRUE,
            ci=TRUE, ci.alpha=0.9, stratified=FALSE,
            plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
            print.auc=TRUE, show.thres=TRUE)
sens.ci <- ci.se(pROC_z)
plot(sens.ci, Class="shape", col="lightblue")
plot(sens.ci, Class="bars")
```

```{r, message=FALSE, warning=FALSE, echo=FALSE}
knn_z_table[["t"]]
```

We have the same decreased accuracy of 92.5% as with thhe normalized,
but the AUC plummeted from 0.967 to 0.853. We can also see that with the
normalization the 2 and 3 Classes had 100% TP and Class_1 had 100% TN,
whereas with the z-score only Class_2 has 100% TP and Class_3 100% TN,
whereas Class_1 now has 2/15 (11.8%) FN and 1/16 (6.7%) FP, so even
though in both cases we had only 3 mistakes, in the normalization we had
a problem just with the over-predictions of 1 while the rest was
perfect, but with the z-score we have more problems..

### different Ks

We tried with different K's (3, 9 and 15)

```{r echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
pred_k9 <- knn(train = train_set, test = test_set, cl = train_set_labels, k=9)
#i got same rror again
#Error in knn(train = train_set, test = test_set, cl = train_set_labels,  : 
 # 'train' and 'class' have different lengths

knn9_table<-CrossTable(x = test_set_labels, y = pred_k9, prop.chisq=FALSE)
acc_knn9 <- sum(diag(knn9_table[["t"]])) / sum(knn9_table[["t"]])
df_knn9<-data.frame(predictions=c(as.numeric(pred_k9)),labels = c(test_set_labels))
pROC_knn9 <- roc(df_knn9$labels,df_knn9$predictions,
            smoothed = TRUE,reuse.auc=TRUE,
            ci=TRUE, ci.alpha=0.9, stratified=FALSE,
            plot=FALSE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
            print.auc=TRUE, show.thres=TRUE)
sens.ci <- ci.se(pROC_knn9)

pred_k15 <- knn(train = train_set, test = test_set, cl = train_set_labels, k=15)
knn15_table<-CrossTable(x = test_set_labels, y = pred_k15, prop.chisq=FALSE)
acc_knn15 <- sum(diag(knn15_table[["t"]])) / sum(knn15_table[["t"]])
df_knn15<-data.frame(predictions=c(as.numeric(pred_k15)),labels = c(test_set_labels))
pROCdf_knn15 <- roc(df_knn15$labels,df_knn15$predictions,
            smoothed = TRUE,reuse.auc=TRUE,
            ci=TRUE, ci.alpha=0.9, stratified=FALSE,
            plot=FALSE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
            print.auc=TRUE, show.thres=TRUE)
sens.ci <- ci.se(pROCdf_knn15)

pred_k21 <- knn(train = train_set, test = test_set, cl = train_set_labels, k=21)
knn21_table<-CrossTable(x = test_set_labels, y = pred_k21, prop.chisq=FALSE)
acc_knn21 <- sum(diag(knn21_table[["t"]])) / sum(knn21_table[["t"]])
df_knn21<-data.frame(predictions=c(as.numeric(pred_k21)),labels = c(test_set_labels))
pROCdf_knn21 <- roc(df_knn21$labels,df_knn21$predictions,
            smoothed = TRUE,reuse.auc=TRUE,
            ci=TRUE, ci.alpha=0.9, stratified=FALSE,
            plot=FALSE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
            print.auc=TRUE, show.thres=TRUE)
sens.ci <- ci.se(pROCdf_knn21)
```

```{r, message=FALSE, warning=FALSE}
df_acc_auc<-data_frame(knn=c(3,9,15,21,"3+norm","3+z"), accuracy=c(acc_knn3,acc_knn9,acc_knn15,acc_knn21,acc_knn3_norm,acc_knn3_z),
                       AUC=c(pROC_knn3[["auc"]],pROC_knn9[["auc"]],pROCdf_knn15[["auc"]],
                             pROCdf_knn21[["auc"]], pROC_norm[["auc"]], pROC_z[["auc"]]))
df_acc_auc_melt<-melt(df_acc_auc)
plotted<-ggplot(data = df_acc_auc_melt, aes(knn,value, fill=variable)) +
  geom_col(position ='dodge')
table_norm<-tableGrob(df_acc_auc)
grid.arrange(plotted+theme(legend.position = "top"), table_norm,ncol=2,nrow=1, widths=c(2,1))
```

We tried several more times with different K's ( 9 , 15 and 21) and it
looks like the results are pretty similar with AUC of 0.853 for all 3
options.

With K = 9 we got one FP for Class_2 (it was actually Class_1) and an
accuracy of 97.5%. With K = 15 we got in addition to the FP in Class two
another FP in Class 1 (was actually Class 2), and the accuracy was 95%.
With K = 21 we don't have any FP in Class two, but 2 FP in Class 1 (was
actually Class 2), and the accuracy was also 95% (both 15 and 21 KNNs
had 2 mistakes).

### feature selection

The DT algorithm uses the feature that provides the most Information
Gain in every split. Using that logic, the feature that is used the
least provides the least information gain, and if it doesn't add much
information it might disturb as noise. Thus, since we know that the
accuracy of the k-NN algorithm can be severely degraded by the presence
of noisy or irrelevant features, we tried the KNN algorithm once more,
with K=15 (it might be easier to improve k=15 with 95% than k=9 with
97.5%) and without the "Kernel.Length" column (which attributed the
least to the building of the DT).

```{r, message=FALSE, warning=FALSE, results='hide'}
feat_selec<- knn(train = train_set[-4], test = test_set[-4], cl = train_set_labels, k=15)
knn9_no4_table<-CrossTable(x = test_set_labels, y = feat_selec, prop.chisq=FALSE)
acc_knn9_no4 <- sum(diag(knn9_no4_table[["t"]])) / sum(knn9_no4_table[["t"]])
```

There was no change on accuracy. We tried again without Compactness and
again without both Compactness and Kernel.Length, and we still got to
97.5% accuracy

```{r, message=FALSE, warning=FALSE, echo=FALSE, results='hide'}
#no Compactness
feat_sel_train <- train_set[-3]
feat_sel_test <- test_set[-3]
feat_sel_pred <- knn(train = train_set_z, test = test_set_z, cl = train_set_labels, k=15)
knn9_no3_table<-CrossTable(x = test_set_labels, y = feat_sel_pred, prop.chisq=FALSE)
acc_knn9_no3 <- sum(diag(knn9_no3_table[["t"]])) / sum(knn9_no3_table[["t"]])

#no Compactness and no Kernel.Length
feat_sel_train <- feat_sel_train[-3]
feat_sel_test <- feat_sel_test[-3]
feat_sel_pred <- knn(train = train_set_z, test = test_set_z, cl = train_set_labels, k=21)
knn9_no3_4_table<-CrossTable(x = test_set_labels, y = feat_sel_pred, prop.chisq=FALSE)
acc_knn9_no3_no4 <- sum(diag(knn9_no3_4_table[["t"]])) / sum(knn9_no3_4_table[["t"]])
```

## knn conclusions

```{r, message=FALSE, warning=FALSE}
knn_acuuracy <- data.frame(num_of_neighbors=c(3,9,15,21,"15+selected"),
                       acuuracy_val=c(acc_knn3,acc_knn9,acc_knn15,acc_knn21, acc_knn9_no4))
acuuracy_melted <- melt(knn_acuuracy)
ggplot(data = knn_acuuracy, aes(num_of_neighbors,acuuracy_val , fill=acuuracy_val)) + geom_col()
```

We managed to classify our data using the KNN algorithm, with k=3 (not
normalized or standardized) we got 100% accuracy (and AUC of 1 of
course). Our results show that- in general- the higher the k is the less
accurate the algorithm is. We tried using K=9 and without the
"Kernel.Length" (or "Compactness") column, but that didn't help the
accuracy at all.


## Random forest

We begin by loading the required libraries:

```{r setup}
library(tidymodels) # for the rsample package, along with the rest of tidymodels
library(modeldata)  # for the cells data
library(vip)        # for variable importance plots
```

We have data for 196 patients, with 63 features. The main outcome variable of interest for us here is called class, which is a factor.Each row is a patient, classified as "glaucoma"  or "normal" . If we can predict these labels accurately, the larger data set can be improved by filtering out the cells most likely to be poorly segmented.


An important aspect when performing prediction is the class balance, lets take a look:

```{r}
glaucoma %>% 
  count(Class) %>% 
  mutate(prop = n/sum(n))
```

We have 50% 'glaucoma' patients and 50% 'normal' patients. We can make the splits with the same proportions as the original data (50/50).

## Data Splitting

To split data, we are using the strata argument, which conducts a stratified split. This ensures that, despite the imbalance we noticed in our class variable, our training and test data sets will keep roughly the same proportions of sick andnormal patients as in the original data.

```{r}
set.seed(123)
patient_split <- initial_split(glaucoma, 
                            strata = Class)

glaucoma_train <- training(patient_split)
glaucoma_test  <- testing(patient_split)

nrow(glaucoma_train)
nrow(glaucoma_train)/nrow(glaucoma)

# training set proportions by class
glaucoma_train %>% 
  count(Class) %>% 
  mutate(prop = n/sum(n))

# test set proportions by class
glaucoma_test %>% 
  count(Class) %>% 
  mutate(prop = n/sum(n))
```

## Random Forest Modeling

The random Forest algorithm uses many decision trees. For classification, which is what
we are focusing on, the final class result is based on the class that was picked from most trees. 

First we define our model with the parsnip package. Random forests have very little mandatory parameters. 
Here we only define the number of trees:

```{r}
# create random forest model
rf_mod <- 
  rand_forest(trees = 1000) %>% 
  set_engine("ranger") %>%   # engine - method of estimation the model will use
  set_mode("classification")
```

We want reproducible results, so we set.seed. Taking a look at the model:

```{r}
# train model
library(ranger)
set.seed(234)
rf_fit <- 
  rf_mod %>% 
  fit(Class ~ ., data = glaucoma_train)
rf_fit
```

We can see our model had 1000 trees like we specified, and several other metrics, such as the prediction error, 13% here.

## Estimating performance

Next we want to see if we can improve our model. We do this by changing some things about it. 
The changes can either be random (guessing) or more precise, depending on performance metrics. In our example, we will use the area under the Receiver Operating Characteristic (ROC) curve (which demonstrates the trade-off between the sensitivity and and specificity), and overall classification accuracy.

Using the yardstick package, let's calculate ROC and Accuracy. Notice that we are still only working with the glaucoma_train partition of our data:

```{r}
rf_training_pred <- 
  predict(rf_fit, glaucoma_train) %>% 
  bind_cols(predict(rf_fit, glaucoma_train, type = "prob")) %>% 
  # Add the true outcome data back in
  bind_cols(glaucoma_train %>% 
              select(Class))

# ROC calculation
rf_training_pred %>%                # training set predictions
  roc_auc(truth = Class, .pred_glaucoma)

# Accuracy calculation
rf_training_pred %>%                # training set predictions
  accuracy(truth = Class, .pred_class)


```

As we can see, these are very good results. Almost too good, so we would like to see how the model performs on the test data:

```{r}
rf_testing_pred <- 
  predict(rf_fit, glaucoma_test) %>% 
  bind_cols(predict(rf_fit, glaucoma_test, type = "prob")) %>% 
  bind_cols(glaucoma_test %>% select(Class))

# ROC calculation
rf_testing_pred %>%                   # test set predictions
  roc_auc(truth = Class, .pred_glaucoma)

# Accuracy calculation
rf_testing_pred %>%                   # test set predictions
  accuracy(truth = Class, .pred_class)
```

These validation results are lower than the ones we got on the training data.
There are several reasons why training set statistics like the ones shown in this section can be unrealistically optimistic:

- Overfitting - Models like random forests, neural networks, and other black-box methods can essentially memorize the training set. Re-predicting that same set should always result in nearly perfect results.

- The training set does not have the capacity to be a good arbiter of performance. It is not an independent piece of information; predicting the training set can only reflect what the model already knows.

## Resampling

Resampling methods, such as cross-validation and the bootstrap, are empirical simulation systems. They create a series of data sets similar to the training/testing split discussed previously; a subset of the data is used for creating the model and a different subset is used to measure performance. Resampling is always used with the training set. 

Here we'll use 10-fold cross-validation. This means that we'll create 10 "mini" datasets, or folds. We call the majority part of the folds (9 out of 10 in this case) the "analysis set" and the minority the "assessment set". We then train a model using the analysis set, and test it on the assessment set, effectively repeating the modeling process 10 times. This is how its done:

```{r}
# create the folds
set.seed(345)
folds <- vfold_cv(glaucoma_train, v = 10)
folds

# here we use a workflow, which bundles together the model specifications and actual modeling
# set the random forest workflow 
rf_wf <- 
  workflow() %>%
  add_model(rf_mod) %>%
  add_formula(Class ~ .)

# add folds to workflow and train model
set.seed(456)
rf_fit_rs <- 
  rf_wf %>% 
  fit_resamples(folds)

rf_fit_rs

collect_metrics(rf_fit_rs)
```

We see these results are lower and look more realistic.
Now, looking at the test set for results, we expect and see much more similar results to the above ROC and accuracy:

```{r}
# ROC calculation on test predictions
rf_testing_pred %>%             
  roc_auc(truth = Class, .pred_glaucoma)

# Accuracy calculation on test predictions
rf_testing_pred %>%           
  accuracy(truth = Class, .pred_class)
```


## Tuning hyperparameters

Another way to improve the performance of our models is by changing the parameters we provide them with. This is also called Tuning. Random Forests, as mentioned above, are not very sensitive to such parameters, but decision trees are. Lets see:

```{r}
# defining the parameters for the decision tree
tune_spec <- 
  decision_tree(
    cost_complexity = tune(), # to control the size of the tree
    tree_depth = tune()
  ) %>% 
  set_engine("rpart") %>% 
  set_mode("classification")

tune_spec
```

Note that in the above code, tune() is still just a placeholder. 
We will fill in values later on.

Next, we will create several smaller datasets, to try different parameters. 
The grid_regular command below will create 5 such datasets for each parameter combination, meaning 25 in total.

```{r}
# create 5 'datasets' with different combinations
tree_grid <- grid_regular(cost_complexity(), 
                          tree_depth(),
                          levels = 5)

tree_grid

tree_grid %>% 
  count(tree_depth)

set.seed(234)
glaucoma_folds <- vfold_cv(glaucoma_train) # create the actual cross-validation folds
```
## Model tuning with a grid

```{r}
set.seed(345)
# define the workflow
tree_wf <- workflow() %>%
  add_model(tune_spec) %>%
  add_formula(Class ~ .)

# update and train workflow with grids
tree_res <- 
  tree_wf %>% 
  tune_grid(
    resamples = glaucoma_folds,
    grid = tree_grid
    )

tree_res

tree_res %>% 
  collect_metrics()
```
Its easier to see how the models did with a graph:

```{r}
# plot preformance
tree_res %>%
  collect_metrics() %>%
  mutate(tree_depth = factor(tree_depth)) %>%
  ggplot(aes(cost_complexity, mean, color = tree_depth)) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free", nrow = 2) +
  scale_x_log10(labels = scales::label_number()) +
  scale_color_viridis_d(option = "plasma", begin = .9, end = 0)
```

We can see that our tree with a depth of 1, is the worst model according to both metrics and across all candidate values of cost_complexity. Our deepest tree, with a depth of 15, did better.  The show_best() function shows us the top 5 candidate models by default:

```{r}
# best tree
tree_res %>%
  show_best("roc_auc")
```

And finally, let's finalize the workflow with the best tree. 

```{r}
best_tree <- tree_res %>%
  select_best("roc_auc") # Selecting the best tree

best_tree

# Finalize workflow
final_wf <- 
  tree_wf %>% 
  finalize_workflow(best_tree)

final_wf
```
## Exploring results

So we have our best tree parameters. Lets take a better look:

```{r}
final_tree <- 
  final_wf %>%
  fit(data = glaucoma_train) 

final_tree
```

Another way to look at how the model uses the different features is using the vip package:

```{r}
#final_tree %>% 
 # pull_workflow_fit() %>% 
  #vip()

final_tree %>% 
  extract_fit_parsnip() %>% 
  vip()
```

## Finalizing the model

Finally, let’s return to our test data and estimate the model performance we expect to see with new data. We can use the function last_fit() with our finalized model; this function fits the finalized model on the full training data set and evaluates the finalized model on the testing data.

```{r}
# Set workflow
final_fit <- 
  final_wf %>%
  last_fit(patient_split) 

# train
final_fit %>%
  collect_metrics()

# collect metrics
final_fit %>%
  collect_predictions() %>% 
  roc_curve(Class, .pred_glaucoma) %>% 
  autoplot()

args(decision_tree)
```
We can observe in the ROC curve that in our true positive prediction we have very few false positive predictions.

# We need another algorithm
# Comparison between the 3/4 algorithm

>>> Lilach, i cannot compare between the different algo because KNN doesnt work
>>> could you please correct the error of test and train set please
>>> could you please correct the error in the SVM about kernfit ?
>>>>>>>>>>>>>>>>>Thanks

